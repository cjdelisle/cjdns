# version 20090331
# Peter Schwabe & Neil Costigan
# Public domain.

# qhasm: vec128 retp

# qhasm: vec128 skp

# qhasm: vec128 xp

# qhasm: input retp

# qhasm: input skp

# qhasm: input xp

# qhasm: vec128 sk

# qhasm: vec128 x1_03

# qhasm: vec128 x1_47

# qhasm: vec128 x1_811

# qhasm: vec128 x1_1215

# qhasm: vec128 x1_1619

# qhasm: vec128 x2_03

# qhasm: vec128 x2_47

# qhasm: vec128 x2_811

# qhasm: vec128 x2_1215

# qhasm: vec128 x2_1619

# qhasm: vec128 z2_03

# qhasm: vec128 z2_47

# qhasm: vec128 z2_811

# qhasm: vec128 z2_1215

# qhasm: vec128 z2_1619

# qhasm: vec128 x3_03

# qhasm: vec128 x3_47

# qhasm: vec128 x3_811

# qhasm: vec128 x3_1215

# qhasm: vec128 x3_1619

# qhasm: vec128 z3_03

# qhasm: vec128 z3_47

# qhasm: vec128 z3_811

# qhasm: vec128 z3_1215

# qhasm: vec128 z3_1619

# qhasm: vec128 z3_2023

# qhasm: vec128 z3_2427

# qhasm: vec128 z3_2831

# qhasm: vec128 z3_3235

# qhasm: vec128 z3_3639

# qhasm: vec128 a_03

# qhasm: vec128 a_47

# qhasm: vec128 a_811

# qhasm: vec128 a_1215

# qhasm: vec128 a_1619

# qhasm: vec128 b_03

# qhasm: vec128 b_47

# qhasm: vec128 b_811

# qhasm: vec128 b_1215

# qhasm: vec128 b_1619

# qhasm: vec128 c_03

# qhasm: vec128 c_47

# qhasm: vec128 c_811

# qhasm: vec128 c_1215

# qhasm: vec128 c_1619

# qhasm: vec128 d_03

# qhasm: vec128 d_47

# qhasm: vec128 d_811

# qhasm: vec128 d_1215

# qhasm: vec128 d_1619

# qhasm: vec128 acbd0

# qhasm: vec128 acbd1

# qhasm: vec128 acbd2

# qhasm: vec128 acbd3

# qhasm: vec128 acbd4

# qhasm: vec128 acbd5

# qhasm: vec128 acbd6

# qhasm: vec128 acbd7

# qhasm: vec128 acbd8

# qhasm: vec128 acbd9

# qhasm: vec128 acbd10

# qhasm: vec128 acbd11

# qhasm: vec128 acbd12

# qhasm: vec128 acbd13

# qhasm: vec128 acbd14

# qhasm: vec128 acbd15

# qhasm: vec128 acbd16

# qhasm: vec128 acbd17

# qhasm: vec128 acbd18

# qhasm: vec128 acbd19

# qhasm: vec128 t20

# qhasm: vec128 t21

# qhasm: vec128 t22

# qhasm: vec128 t23

# qhasm: vec128 t24

# qhasm: vec128 t25

# qhasm: vec128 t26

# qhasm: vec128 t27

# qhasm: vec128 t28

# qhasm: vec128 t29

# qhasm: vec128 t210

# qhasm: vec128 t211

# qhasm: vec128 t212

# qhasm: vec128 t213

# qhasm: vec128 t214

# qhasm: vec128 t215

# qhasm: vec128 t216

# qhasm: vec128 t217

# qhasm: vec128 t218

# qhasm: vec128 t219

# qhasm: vec128 abba0

# qhasm: vec128 abba1

# qhasm: vec128 abba2

# qhasm: vec128 abba3

# qhasm: vec128 abba4

# qhasm: vec128 abba5

# qhasm: vec128 abba6

# qhasm: vec128 abba7

# qhasm: vec128 abba8

# qhasm: vec128 abba9

# qhasm: vec128 abba10

# qhasm: vec128 abba11

# qhasm: vec128 abba12

# qhasm: vec128 abba13

# qhasm: vec128 abba14

# qhasm: vec128 abba15

# qhasm: vec128 abba16

# qhasm: vec128 abba17

# qhasm: vec128 abba18

# qhasm: vec128 abba19

# qhasm: vec128 et4t1t00

# qhasm: vec128 et4t1t01

# qhasm: vec128 et4t1t02

# qhasm: vec128 et4t1t03

# qhasm: vec128 et4t1t04

# qhasm: vec128 et4t1t05

# qhasm: vec128 et4t1t06

# qhasm: vec128 et4t1t07

# qhasm: vec128 et4t1t08

# qhasm: vec128 et4t1t09

# qhasm: vec128 et4t1t010

# qhasm: vec128 et4t1t011

# qhasm: vec128 et4t1t012

# qhasm: vec128 et4t1t013

# qhasm: vec128 et4t1t014

# qhasm: vec128 et4t1t015

# qhasm: vec128 et4t1t016

# qhasm: vec128 et4t1t017

# qhasm: vec128 et4t1t018

# qhasm: vec128 et4t1t019

# qhasm: vec128 aa_a24aadada0

# qhasm: vec128 aa_a24aadada1

# qhasm: vec128 aa_a24aadada2

# qhasm: vec128 aa_a24aadada3

# qhasm: vec128 aa_a24aadada4

# qhasm: vec128 aa_a24aadada5

# qhasm: vec128 aa_a24aadada6

# qhasm: vec128 aa_a24aadada7

# qhasm: vec128 aa_a24aadada8

# qhasm: vec128 aa_a24aadada9

# qhasm: vec128 aa_a24aadada10

# qhasm: vec128 aa_a24aadada11

# qhasm: vec128 aa_a24aadada12

# qhasm: vec128 aa_a24aadada13

# qhasm: vec128 aa_a24aadada14

# qhasm: vec128 aa_a24aadada15

# qhasm: vec128 aa_a24aadada16

# qhasm: vec128 aa_a24aadada17

# qhasm: vec128 aa_a24aadada18

# qhasm: vec128 aa_a24aadada19

# qhasm: vec128 bb_a24m1bbcb0

# qhasm: vec128 bb_a24m1bbcb1

# qhasm: vec128 bb_a24m1bbcb2

# qhasm: vec128 bb_a24m1bbcb3

# qhasm: vec128 bb_a24m1bbcb4

# qhasm: vec128 bb_a24m1bbcb5

# qhasm: vec128 bb_a24m1bbcb6

# qhasm: vec128 bb_a24m1bbcb7

# qhasm: vec128 bb_a24m1bbcb8

# qhasm: vec128 bb_a24m1bbcb9

# qhasm: vec128 bb_a24m1bbcb10

# qhasm: vec128 bb_a24m1bbcb11

# qhasm: vec128 bb_a24m1bbcb12

# qhasm: vec128 bb_a24m1bbcb13

# qhasm: vec128 bb_a24m1bbcb14

# qhasm: vec128 bb_a24m1bbcb15

# qhasm: vec128 bb_a24m1bbcb16

# qhasm: vec128 bb_a24m1bbcb17

# qhasm: vec128 bb_a24m1bbcb18

# qhasm: vec128 bb_a24m1bbcb19

# qhasm: vec128 2p2p2pcb0

# qhasm: vec128 2p2p2pcb1

# qhasm: vec128 2p2p2pcb2

# qhasm: vec128 2p2p2pcb3

# qhasm: vec128 2p2p2pcb4

# qhasm: vec128 2p2p2pcb5

# qhasm: vec128 2p2p2pcb6

# qhasm: vec128 2p2p2pcb7

# qhasm: vec128 2p2p2pcb8

# qhasm: vec128 2p2p2pcb9

# qhasm: vec128 2p2p2pcb10

# qhasm: vec128 2p2p2pcb11

# qhasm: vec128 2p2p2pcb12

# qhasm: vec128 2p2p2pcb13

# qhasm: vec128 2p2p2pcb14

# qhasm: vec128 2p2p2pcb15

# qhasm: vec128 2p2p2pcb16

# qhasm: vec128 2p2p2pcb17

# qhasm: vec128 2p2p2pcb18

# qhasm: vec128 2p2p2pcb19

# qhasm: vec128 vec19

# qhasm: vec128 tmp0

# qhasm: vec128 tmp1

# qhasm: vec128 tmp2

# qhasm: vec128 tmp3

# qhasm: vec128 tmp4

# qhasm: vec128 tmp5

# qhasm: vec128 tmp6

# qhasm: vec128 tmp7

# qhasm: vec128 tmp8

# qhasm: vec128 tmp9

# qhasm: vec128 tmp10

# qhasm: vec128 tmp11

# qhasm: vec128 tmp12

# qhasm: vec128 tmp13

# qhasm: vec128 tmp14

# qhasm: vec128 tmp15

# qhasm: vec128 tmp16

# qhasm: vec128 tmp17

# qhasm: vec128 tmp18

# qhasm: vec128 tmp19

# qhasm: vec128 et0aat10

# qhasm: vec128 et0aat11

# qhasm: vec128 et0aat12

# qhasm: vec128 et0aat13

# qhasm: vec128 et0aat14

# qhasm: vec128 et0aat15

# qhasm: vec128 et0aat16

# qhasm: vec128 et0aat17

# qhasm: vec128 et0aat18

# qhasm: vec128 et0aat19

# qhasm: vec128 et0aat110

# qhasm: vec128 et0aat111

# qhasm: vec128 et0aat112

# qhasm: vec128 et0aat113

# qhasm: vec128 et0aat114

# qhasm: vec128 et0aat115

# qhasm: vec128 et0aat116

# qhasm: vec128 et0aat117

# qhasm: vec128 et0aat118

# qhasm: vec128 et0aat119

# qhasm: vec128 t4t0bbt10

# qhasm: vec128 t4t0bbt11

# qhasm: vec128 t4t0bbt12

# qhasm: vec128 t4t0bbt13

# qhasm: vec128 t4t0bbt14

# qhasm: vec128 t4t0bbt15

# qhasm: vec128 t4t0bbt16

# qhasm: vec128 t4t0bbt17

# qhasm: vec128 t4t0bbt18

# qhasm: vec128 t4t0bbt19

# qhasm: vec128 t4t0bbt110

# qhasm: vec128 t4t0bbt111

# qhasm: vec128 t4t0bbt112

# qhasm: vec128 t4t0bbt113

# qhasm: vec128 t4t0bbt114

# qhasm: vec128 t4t0bbt115

# qhasm: vec128 t4t0bbt116

# qhasm: vec128 t4t0bbt117

# qhasm: vec128 t4t0bbt118

# qhasm: vec128 t4t0bbt119

# qhasm: vec128 aacbbbda0

# qhasm: vec128 aacbbbda1

# qhasm: vec128 aacbbbda2

# qhasm: vec128 aacbbbda3

# qhasm: vec128 aacbbbda4

# qhasm: vec128 aacbbbda5

# qhasm: vec128 aacbbbda6

# qhasm: vec128 aacbbbda7

# qhasm: vec128 aacbbbda8

# qhasm: vec128 aacbbbda9

# qhasm: vec128 aacbbbda10

# qhasm: vec128 aacbbbda11

# qhasm: vec128 aacbbbda12

# qhasm: vec128 aacbbbda13

# qhasm: vec128 aacbbbda14

# qhasm: vec128 aacbbbda15

# qhasm: vec128 aacbbbda16

# qhasm: vec128 aacbbbda17

# qhasm: vec128 aacbbbda18

# qhasm: vec128 aacbbbda19

# qhasm: vec128 aacbbbda20

# qhasm: vec128 aacbbbda21

# qhasm: vec128 aacbbbda22

# qhasm: vec128 aacbbbda23

# qhasm: vec128 aacbbbda24

# qhasm: vec128 aacbbbda25

# qhasm: vec128 aacbbbda26

# qhasm: vec128 aacbbbda27

# qhasm: vec128 aacbbbda28

# qhasm: vec128 aacbbbda29

# qhasm: vec128 aacbbbda30

# qhasm: vec128 aacbbbda31

# qhasm: vec128 aacbbbda32

# qhasm: vec128 aacbbbda33

# qhasm: vec128 aacbbbda34

# qhasm: vec128 aacbbbda35

# qhasm: vec128 aacbbbda36

# qhasm: vec128 aacbbbda37

# qhasm: vec128 aacbbbda38

# qhasm: vec128 aacbbbda39

# qhasm: vec128 z4x5x4t20

# qhasm: vec128 z4x5x4t21

# qhasm: vec128 z4x5x4t22

# qhasm: vec128 z4x5x4t23

# qhasm: vec128 z4x5x4t24

# qhasm: vec128 z4x5x4t25

# qhasm: vec128 z4x5x4t26

# qhasm: vec128 z4x5x4t27

# qhasm: vec128 z4x5x4t28

# qhasm: vec128 z4x5x4t29

# qhasm: vec128 z4x5x4t210

# qhasm: vec128 z4x5x4t211

# qhasm: vec128 z4x5x4t212

# qhasm: vec128 z4x5x4t213

# qhasm: vec128 z4x5x4t214

# qhasm: vec128 z4x5x4t215

# qhasm: vec128 z4x5x4t216

# qhasm: vec128 z4x5x4t217

# qhasm: vec128 z4x5x4t218

# qhasm: vec128 z4x5x4t219

# qhasm: vec128 z4x5x4t220

# qhasm: vec128 z4x5x4t221

# qhasm: vec128 z4x5x4t222

# qhasm: vec128 z4x5x4t223

# qhasm: vec128 z4x5x4t224

# qhasm: vec128 z4x5x4t225

# qhasm: vec128 z4x5x4t226

# qhasm: vec128 z4x5x4t227

# qhasm: vec128 z4x5x4t228

# qhasm: vec128 z4x5x4t229

# qhasm: vec128 z4x5x4t230

# qhasm: vec128 z4x5x4t231

# qhasm: vec128 z4x5x4t232

# qhasm: vec128 z4x5x4t233

# qhasm: vec128 z4x5x4t234

# qhasm: vec128 z4x5x4t235

# qhasm: vec128 z4x5x4t236

# qhasm: vec128 z4x5x4t237

# qhasm: vec128 z4x5x4t238

# qhasm: vec128 z4x5x4t239

# qhasm: vec128 carry

# qhasm: vec128 carry0

# qhasm: vec128 carry1

# qhasm: vec128 carry2

# qhasm: vec128 carry3

# qhasm: vec128 carry4

# qhasm: vec128 carry5

# qhasm: vec128 carry6

# qhasm: vec128 carry7

# qhasm: vec128 carry8

# qhasm: vec128 carry9

# qhasm: vec128 carry10

# qhasm: vec128 carry11

# qhasm: vec128 carry12

# qhasm: vec128 carry13

# qhasm: vec128 carry14

# qhasm: vec128 carry15

# qhasm: vec128 carry16

# qhasm: vec128 carry17

# qhasm: vec128 carry18

# qhasm: vec128 carry19

# qhasm: vec128 red

# qhasm: vec128 red0

# qhasm: vec128 red1

# qhasm: vec128 red2

# qhasm: vec128 red3

# qhasm: vec128 red4

# qhasm: vec128 comb13

# qhasm: vec128 comb22

# qhasm: vec128 comb31

# qhasm: vec128 redcoeffmask

# qhasm: vec128 redcoeffmaskend

# qhasm: vec128 redcoeffmaskveryend

# qhasm: vec128 shuf0_01

# qhasm: vec128 shuf0_2

# qhasm: vec128 shuf0_3

# qhasm: vec128 shuf1_01

# qhasm: vec128 shuf1_2

# qhasm: vec128 shuf1_3

# qhasm: vec128 shuf2_01

# qhasm: vec128 shuf2_2

# qhasm: vec128 shuf2_3

# qhasm: vec128 shuf3_01

# qhasm: vec128 shuf3_2

# qhasm: vec128 shuf3_3

# qhasm: vec128 selw0220

# qhasm: vec128 selw0105

# qhasm: vec128 selw2325

# qhasm: vec128 selw0433 

# qhasm: vec128 selw261c0 

# qhasm: vec128 selw0342

# qhasm: vec128 selw1362

# qhasm: vec128 selw3333

# qhasm: vec128 sel01

# qhasm: vec128 sel12

# qhasm: vec128 sel23

# qhasm: vec128 sel30

# qhasm: vec128 mask12

# qhasm: vec128 mask13

# qhasm: vec128 bit

# qhasm: vec128 done

# qhasm: vec128 extbit

# qhasm: vec128 check

# qhasm: vec128 prevextbit

# qhasm: vec128 nprevextbit

# qhasm: vec128 loopmask

# qhasm: vec128 flip

# qhasm: vec128 nflip

# qhasm: vec128 zero

# qhasm: vec128 one

# qhasm: vec128 a24vec

# qhasm: vec128 2pconsts0

# qhasm: vec128 2pconsts

# qhasm: vec128 2p_03

# qhasm: vec128 2p_47

# qhasm: vec128 2p_811

# qhasm: vec128 2p_1215

# qhasm: vec128 2p_1619

# qhasm: vec128 swapendian

# qhasm: vec128 tmp00

# qhasm: vec128 tmp00b

# qhasm: vec128 tmp01

# qhasm: vec128 tmp01b

# qhasm: vec128 tmp02

# qhasm: vec128 tmp02b

# qhasm: vec128 tmp03

# qhasm: vec128 tmp03b

# qhasm: vec128 tmp04

# qhasm: vec128 tmp04n

# qhasm: vec128 tmp04b

# qhasm: vec128 tmp04bn

# qhasm: vec128 tmp10a

# qhasm: vec128 tmp10b

# qhasm: vec128 tmp11a

# qhasm: vec128 tmp11b

# qhasm: vec128 tmp12a

# qhasm: vec128 tmp12b

# qhasm: vec128 tmp13a

# qhasm: vec128 tmp13b

# qhasm: vec128 tmp14n

# qhasm: vec128 tmp14a

# qhasm: vec128 tmp14an

# qhasm: vec128 tmp14b

# qhasm: vec128 tmp14bn

# qhasm: vec128 tmp20

# qhasm: vec128 tmp20a

# qhasm: vec128 tmp20b

# qhasm: vec128 tmp21

# qhasm: vec128 tmp21a

# qhasm: vec128 tmp21b

# qhasm: vec128 tmp22

# qhasm: vec128 tmp22a

# qhasm: vec128 tmp22b

# qhasm: vec128 tmp23

# qhasm: vec128 tmp23a

# qhasm: vec128 tmp23b

# qhasm: vec128 tmp24

# qhasm: vec128 tmp24n

# qhasm: vec128 tmp24a

# qhasm: vec128 tmp24an

# qhasm: vec128 tmp24b

# qhasm: vec128 tmp24bn

# qhasm: vec128 tmp30

# qhasm: vec128 tmp30a

# qhasm: vec128 tmp30b

# qhasm: vec128 tmp31

# qhasm: vec128 tmp31a

# qhasm: vec128 tmp31b

# qhasm: vec128 tmp32

# qhasm: vec128 tmp32a

# qhasm: vec128 tmp32b

# qhasm: vec128 tmp33

# qhasm: vec128 tmp33a

# qhasm: vec128 tmp33b

# qhasm: vec128 tmp34

# qhasm: vec128 tmp34n

# qhasm: vec128 tmp34a

# qhasm: vec128 tmp34an

# qhasm: vec128 tmp34b

# qhasm: vec128 tmp34bn

# qhasm: vec128 call0

# qhasm: vec128 call1

# qhasm: vec128 call2

# qhasm: vec128 call3

# qhasm: vec128 call4

# qhasm: vec128 call5

# qhasm: vec128 call6

# qhasm: vec128 call7

# qhasm: vec128 call8

# qhasm: vec128 call9

# qhasm: vec128 call10

# qhasm: vec128 call11

# qhasm: vec128 call12

# qhasm: vec128 call13

# qhasm: vec128 call14

# qhasm: vec128 call15

# qhasm: vec128 call16

# qhasm: vec128 call17

# qhasm: vec128 call18

# qhasm: vec128 call19

# qhasm: vec128 call20

# qhasm: vec128 call21

# qhasm: vec128 call22

# qhasm: vec128 call23

# qhasm: vec128 call24

# qhasm: vec128 call25

# qhasm: vec128 call26

# qhasm: vec128 call27

# qhasm: vec128 call28

# qhasm: vec128 call29

# qhasm: vec128 call30

# qhasm: vec128 call31

# qhasm: vec128 call32

# qhasm: vec128 call33

# qhasm: vec128 call34

# qhasm: vec128 call35

# qhasm: vec128 call36

# qhasm: vec128 call37

# qhasm: vec128 call38

# qhasm: vec128 call39

# qhasm: vec128 call40

# qhasm: vec128 call41

# qhasm: vec128 call42

# qhasm: vec128 call43

# qhasm: vec128 call44

# qhasm: vec128 call45

# qhasm: vec128 call46

# qhasm: vec128 call47

# qhasm: caller call0

# qhasm: caller call1

# qhasm: caller call2

# qhasm: caller call3

# qhasm: caller call4

# qhasm: caller call5

# qhasm: caller call6

# qhasm: caller call7

# qhasm: caller call8

# qhasm: caller call9

# qhasm: caller call10

# qhasm: caller call11

# qhasm: caller call12

# qhasm: caller call13

# qhasm: caller call14

# qhasm: caller call15

# qhasm: caller call16

# qhasm: caller call17

# qhasm: caller call18

# qhasm: caller call19

# qhasm: caller call20

# qhasm: caller call21

# qhasm: caller call22

# qhasm: caller call23

# qhasm: caller call24

# qhasm: caller call25

# qhasm: caller call26

# qhasm: caller call27

# qhasm: caller call28

# qhasm: caller call29

# qhasm: caller call30

# qhasm: caller call31

# qhasm: caller call32

# qhasm: caller call33

# qhasm: caller call34

# qhasm: caller call35

# qhasm: caller call36

# qhasm: caller call37

# qhasm: caller call38

# qhasm: caller call39

# qhasm: caller call40

# qhasm: caller call41

# qhasm: caller call42

# qhasm: caller call43

# qhasm: caller call44

# qhasm: caller call45

# qhasm: caller call46

# qhasm: caller call47

# qhasm: stack128 call0_stack

# qhasm: stack128 call1_stack

# qhasm: stack128 call2_stack

# qhasm: stack128 call3_stack

# qhasm: stack128 call4_stack

# qhasm: stack128 call5_stack

# qhasm: stack128 call6_stack

# qhasm: stack128 call7_stack

# qhasm: stack128 call8_stack

# qhasm: stack128 call9_stack

# qhasm: stack128 call10_stack

# qhasm: stack128 call11_stack

# qhasm: stack128 call12_stack

# qhasm: stack128 call13_stack

# qhasm: stack128 call14_stack

# qhasm: stack128 call15_stack

# qhasm: stack128 call16_stack

# qhasm: stack128 call17_stack

# qhasm: stack128 call18_stack

# qhasm: stack128 call19_stack

# qhasm: stack128 call20_stack

# qhasm: stack128 call21_stack

# qhasm: stack128 call22_stack

# qhasm: stack128 call23_stack

# qhasm: stack128 call24_stack

# qhasm: stack128 call25_stack

# qhasm: stack128 call26_stack

# qhasm: stack128 call27_stack

# qhasm: stack128 call28_stack

# qhasm: stack128 call29_stack

# qhasm: stack128 call30_stack

# qhasm: stack128 call31_stack

# qhasm: stack128 call32_stack

# qhasm: stack128 call33_stack

# qhasm: stack128 call34_stack

# qhasm: stack128 call35_stack

# qhasm: stack128 call36_stack

# qhasm: stack128 call37_stack

# qhasm: stack128 call38_stack

# qhasm: stack128 call39_stack

# qhasm: stack128 call40_stack

# qhasm: stack128 call41_stack

# qhasm: stack128 call42_stack

# qhasm: stack128 call43_stack

# qhasm: stack128 call44_stack

# qhasm: stack128 call45_stack

# qhasm: stack128 call46_stack

# qhasm: stack128 call47_stack

# qhasm: stack128 prevextbit_stack

# qhasm: vec128 try

# qhasm: enter mladder
.text
.align 3
.globl _mladder
.global mladder
.type mladder, @function
mladder:
stqd $lr, 16($sp)
stqd $sp,-816($sp)
ai $sp,$sp,-512
ai $sp,$sp,-304

# qhasm: call0_stack = call0
# asm 1: stqd <call0=vec128#78,[32+>call0_stack=stack128#1]($sp)
# asm 2: stqd <call0=$80,[32+>call0_stack=0]($sp)
stqd $80,[32+0]($sp)

# qhasm: call1_stack = call1
# asm 1: stqd <call1=vec128#79,[32+>call1_stack=stack128#2]($sp)
# asm 2: stqd <call1=$81,[32+>call1_stack=16]($sp)
stqd $81,[32+16]($sp)

# qhasm: call2_stack = call2
# asm 1: stqd <call2=vec128#80,[32+>call2_stack=stack128#3]($sp)
# asm 2: stqd <call2=$82,[32+>call2_stack=32]($sp)
stqd $82,[32+32]($sp)

# qhasm: call3_stack = call3
# asm 1: stqd <call3=vec128#81,[32+>call3_stack=stack128#4]($sp)
# asm 2: stqd <call3=$83,[32+>call3_stack=48]($sp)
stqd $83,[32+48]($sp)

# qhasm: call4_stack = call4
# asm 1: stqd <call4=vec128#82,[32+>call4_stack=stack128#5]($sp)
# asm 2: stqd <call4=$84,[32+>call4_stack=64]($sp)
stqd $84,[32+64]($sp)

# qhasm: call5_stack = call5
# asm 1: stqd <call5=vec128#83,[32+>call5_stack=stack128#6]($sp)
# asm 2: stqd <call5=$85,[32+>call5_stack=80]($sp)
stqd $85,[32+80]($sp)

# qhasm: call6_stack = call6
# asm 1: stqd <call6=vec128#84,[32+>call6_stack=stack128#7]($sp)
# asm 2: stqd <call6=$86,[32+>call6_stack=96]($sp)
stqd $86,[32+96]($sp)

# qhasm: call7_stack = call7
# asm 1: stqd <call7=vec128#85,[32+>call7_stack=stack128#8]($sp)
# asm 2: stqd <call7=$87,[32+>call7_stack=112]($sp)
stqd $87,[32+112]($sp)

# qhasm: call8_stack = call8
# asm 1: stqd <call8=vec128#86,[32+>call8_stack=stack128#9]($sp)
# asm 2: stqd <call8=$88,[32+>call8_stack=128]($sp)
stqd $88,[32+128]($sp)

# qhasm: call9_stack = call9
# asm 1: stqd <call9=vec128#87,[32+>call9_stack=stack128#10]($sp)
# asm 2: stqd <call9=$89,[32+>call9_stack=144]($sp)
stqd $89,[32+144]($sp)

# qhasm: call10_stack = call10
# asm 1: stqd <call10=vec128#88,[32+>call10_stack=stack128#11]($sp)
# asm 2: stqd <call10=$90,[32+>call10_stack=160]($sp)
stqd $90,[32+160]($sp)

# qhasm: call11_stack = call11
# asm 1: stqd <call11=vec128#89,[32+>call11_stack=stack128#12]($sp)
# asm 2: stqd <call11=$91,[32+>call11_stack=176]($sp)
stqd $91,[32+176]($sp)

# qhasm: call12_stack = call12
# asm 1: stqd <call12=vec128#90,[32+>call12_stack=stack128#13]($sp)
# asm 2: stqd <call12=$92,[32+>call12_stack=192]($sp)
stqd $92,[32+192]($sp)

# qhasm: call13_stack = call13
# asm 1: stqd <call13=vec128#91,[32+>call13_stack=stack128#14]($sp)
# asm 2: stqd <call13=$93,[32+>call13_stack=208]($sp)
stqd $93,[32+208]($sp)

# qhasm: call14_stack = call14
# asm 1: stqd <call14=vec128#92,[32+>call14_stack=stack128#15]($sp)
# asm 2: stqd <call14=$94,[32+>call14_stack=224]($sp)
stqd $94,[32+224]($sp)

# qhasm: call15_stack = call15
# asm 1: stqd <call15=vec128#93,[32+>call15_stack=stack128#16]($sp)
# asm 2: stqd <call15=$95,[32+>call15_stack=240]($sp)
stqd $95,[32+240]($sp)

# qhasm: call16_stack = call16
# asm 1: stqd <call16=vec128#94,[32+>call16_stack=stack128#17]($sp)
# asm 2: stqd <call16=$96,[32+>call16_stack=256]($sp)
stqd $96,[32+256]($sp)

# qhasm: call17_stack = call17
# asm 1: stqd <call17=vec128#95,[32+>call17_stack=stack128#18]($sp)
# asm 2: stqd <call17=$97,[32+>call17_stack=272]($sp)
stqd $97,[32+272]($sp)

# qhasm: call18_stack = call18
# asm 1: stqd <call18=vec128#96,[32+>call18_stack=stack128#19]($sp)
# asm 2: stqd <call18=$98,[32+>call18_stack=288]($sp)
stqd $98,[32+288]($sp)

# qhasm: call19_stack = call19
# asm 1: stqd <call19=vec128#97,[32+>call19_stack=stack128#20]($sp)
# asm 2: stqd <call19=$99,[32+>call19_stack=304]($sp)
stqd $99,[32+304]($sp)

# qhasm: call20_stack = call20
# asm 1: stqd <call20=vec128#98,[32+>call20_stack=stack128#21]($sp)
# asm 2: stqd <call20=$100,[32+>call20_stack=320]($sp)
stqd $100,[32+320]($sp)

# qhasm: call21_stack = call21
# asm 1: stqd <call21=vec128#99,[32+>call21_stack=stack128#22]($sp)
# asm 2: stqd <call21=$101,[32+>call21_stack=336]($sp)
stqd $101,[32+336]($sp)

# qhasm: call22_stack = call22
# asm 1: stqd <call22=vec128#100,[32+>call22_stack=stack128#23]($sp)
# asm 2: stqd <call22=$102,[32+>call22_stack=352]($sp)
stqd $102,[32+352]($sp)

# qhasm: call23_stack = call23
# asm 1: stqd <call23=vec128#101,[32+>call23_stack=stack128#24]($sp)
# asm 2: stqd <call23=$103,[32+>call23_stack=368]($sp)
stqd $103,[32+368]($sp)

# qhasm: call24_stack = call24
# asm 1: stqd <call24=vec128#102,[32+>call24_stack=stack128#25]($sp)
# asm 2: stqd <call24=$104,[32+>call24_stack=384]($sp)
stqd $104,[32+384]($sp)

# qhasm: call25_stack = call25
# asm 1: stqd <call25=vec128#103,[32+>call25_stack=stack128#26]($sp)
# asm 2: stqd <call25=$105,[32+>call25_stack=400]($sp)
stqd $105,[32+400]($sp)

# qhasm: call26_stack = call26
# asm 1: stqd <call26=vec128#104,[32+>call26_stack=stack128#27]($sp)
# asm 2: stqd <call26=$106,[32+>call26_stack=416]($sp)
stqd $106,[32+416]($sp)

# qhasm: call27_stack = call27
# asm 1: stqd <call27=vec128#105,[32+>call27_stack=stack128#28]($sp)
# asm 2: stqd <call27=$107,[32+>call27_stack=432]($sp)
stqd $107,[32+432]($sp)

# qhasm: call28_stack = call28
# asm 1: stqd <call28=vec128#106,[32+>call28_stack=stack128#29]($sp)
# asm 2: stqd <call28=$108,[32+>call28_stack=448]($sp)
stqd $108,[32+448]($sp)

# qhasm: call29_stack = call29
# asm 1: stqd <call29=vec128#107,[32+>call29_stack=stack128#30]($sp)
# asm 2: stqd <call29=$109,[32+>call29_stack=464]($sp)
stqd $109,[32+464]($sp)

# qhasm: call30_stack = call30
# asm 1: stqd <call30=vec128#108,[32+>call30_stack=stack128#31]($sp)
# asm 2: stqd <call30=$110,[32+>call30_stack=480]($sp)
stqd $110,[32+480]($sp)

# qhasm: call31_stack = call31
# asm 1: stqd <call31=vec128#109,[32+>call31_stack=stack128#32]($sp)
# asm 2: stqd <call31=$111,[32+>call31_stack=496]($sp)
stqd $111,[32+496]($sp)

# qhasm: call32_stack = call32
# asm 1: stqd <call32=vec128#110,[32+>call32_stack=stack128#33]($sp)
# asm 2: stqd <call32=$112,[32+>call32_stack=512]($sp)
stqd $112,[32+512]($sp)

# qhasm: call33_stack = call33
# asm 1: stqd <call33=vec128#111,[32+>call33_stack=stack128#34]($sp)
# asm 2: stqd <call33=$113,[32+>call33_stack=528]($sp)
stqd $113,[32+528]($sp)

# qhasm: call34_stack = call34
# asm 1: stqd <call34=vec128#112,[32+>call34_stack=stack128#35]($sp)
# asm 2: stqd <call34=$114,[32+>call34_stack=544]($sp)
stqd $114,[32+544]($sp)

# qhasm: call35_stack = call35
# asm 1: stqd <call35=vec128#113,[32+>call35_stack=stack128#36]($sp)
# asm 2: stqd <call35=$115,[32+>call35_stack=560]($sp)
stqd $115,[32+560]($sp)

# qhasm: call36_stack = call36
# asm 1: stqd <call36=vec128#114,[32+>call36_stack=stack128#37]($sp)
# asm 2: stqd <call36=$116,[32+>call36_stack=576]($sp)
stqd $116,[32+576]($sp)

# qhasm: call37_stack = call37
# asm 1: stqd <call37=vec128#115,[32+>call37_stack=stack128#38]($sp)
# asm 2: stqd <call37=$117,[32+>call37_stack=592]($sp)
stqd $117,[32+592]($sp)

# qhasm: call38_stack = call38
# asm 1: stqd <call38=vec128#116,[32+>call38_stack=stack128#39]($sp)
# asm 2: stqd <call38=$118,[32+>call38_stack=608]($sp)
stqd $118,[32+608]($sp)

# qhasm: call39_stack = call39
# asm 1: stqd <call39=vec128#117,[32+>call39_stack=stack128#40]($sp)
# asm 2: stqd <call39=$119,[32+>call39_stack=624]($sp)
stqd $119,[32+624]($sp)

# qhasm: call40_stack = call40
# asm 1: stqd <call40=vec128#118,[32+>call40_stack=stack128#41]($sp)
# asm 2: stqd <call40=$120,[32+>call40_stack=640]($sp)
stqd $120,[32+640]($sp)

# qhasm: call41_stack = call41
# asm 1: stqd <call41=vec128#119,[32+>call41_stack=stack128#42]($sp)
# asm 2: stqd <call41=$121,[32+>call41_stack=656]($sp)
stqd $121,[32+656]($sp)

# qhasm: call42_stack = call42
# asm 1: stqd <call42=vec128#120,[32+>call42_stack=stack128#43]($sp)
# asm 2: stqd <call42=$122,[32+>call42_stack=672]($sp)
stqd $122,[32+672]($sp)

# qhasm: call43_stack = call43
# asm 1: stqd <call43=vec128#121,[32+>call43_stack=stack128#44]($sp)
# asm 2: stqd <call43=$123,[32+>call43_stack=688]($sp)
stqd $123,[32+688]($sp)

# qhasm: call44_stack = call44
# asm 1: stqd <call44=vec128#122,[32+>call44_stack=stack128#45]($sp)
# asm 2: stqd <call44=$124,[32+>call44_stack=704]($sp)
stqd $124,[32+704]($sp)

# qhasm: call45_stack = call45
# asm 1: stqd <call45=vec128#123,[32+>call45_stack=stack128#46]($sp)
# asm 2: stqd <call45=$125,[32+>call45_stack=720]($sp)
stqd $125,[32+720]($sp)

# qhasm: call46_stack = call46
# asm 1: stqd <call46=vec128#124,[32+>call46_stack=stack128#47]($sp)
# asm 2: stqd <call46=$126,[32+>call46_stack=736]($sp)
stqd $126,[32+736]($sp)

# qhasm: call47_stack = call47
# asm 1: stqd <call47=vec128#125,[32+>call47_stack=stack128#48]($sp)
# asm 2: stqd <call47=$127,[32+>call47_stack=752]($sp)
stqd $127,[32+752]($sp)

# qhasm: int32323232 zero = 0
# asm 1: il >zero=vec128#4,0
# asm 2: il >zero=$6,0
il $6,0

# qhasm: one = extern(_one)
# asm 1: lqr <one=vec128#5,_one
# asm 2: lqr <one=$7,_one
lqr $7,_one

# qhasm: uint32323232 loopmask = 1
# asm 1: ila >loopmask=vec128#6,1
# asm 2: ila >loopmask=$8,1
ila $8,1

# qhasm: loopmask <<= (8 * 15)
# asm 1: shlqbyi >loopmask=vec128#6,<loopmask=vec128#6,15
# asm 2: shlqbyi >loopmask=$8,<loopmask=$8,15
shlqbyi $8,$8,15

# qhasm: loopmask <<= (6 % 8)
# asm 1: shlqbii >loopmask=vec128#6,<loopmask=vec128#6,6
# asm 2: shlqbii >loopmask=$8,<loopmask=$8,6
shlqbii $8,$8,6

# qhasm: uint32323232 prevextbit = 0
# asm 1: ila >prevextbit=vec128#7,0
# asm 2: ila >prevextbit=$9,0
ila $9,0

# qhasm: swapendian = extern(_swapendian)
# asm 1: lqr <swapendian=vec128#8,_swapendian
# asm 2: lqr <swapendian=$10,_swapendian
lqr $10,_swapendian

# qhasm: shuf0_01 = extern(_shuf0_01)
# asm 1: lqr <shuf0_01=vec128#9,_shuf0_01
# asm 2: lqr <shuf0_01=$11,_shuf0_01
lqr $11,_shuf0_01

# qhasm: shuf0_2 = extern(_shuf0_2)
# asm 1: lqr <shuf0_2=vec128#10,_shuf0_2
# asm 2: lqr <shuf0_2=$12,_shuf0_2
lqr $12,_shuf0_2

# qhasm: shuf0_3 = extern(_shuf0_3)
# asm 1: lqr <shuf0_3=vec128#11,_shuf0_3
# asm 2: lqr <shuf0_3=$13,_shuf0_3
lqr $13,_shuf0_3

# qhasm: shuf1_01 = extern(_shuf1_01)
# asm 1: lqr <shuf1_01=vec128#12,_shuf1_01
# asm 2: lqr <shuf1_01=$14,_shuf1_01
lqr $14,_shuf1_01

# qhasm: shuf1_2 = extern(_shuf1_2)
# asm 1: lqr <shuf1_2=vec128#13,_shuf1_2
# asm 2: lqr <shuf1_2=$15,_shuf1_2
lqr $15,_shuf1_2

# qhasm: shuf1_3 = extern(_shuf1_3)
# asm 1: lqr <shuf1_3=vec128#14,_shuf1_3
# asm 2: lqr <shuf1_3=$16,_shuf1_3
lqr $16,_shuf1_3

# qhasm: shuf2_01 = extern(_shuf2_01)
# asm 1: lqr <shuf2_01=vec128#15,_shuf2_01
# asm 2: lqr <shuf2_01=$17,_shuf2_01
lqr $17,_shuf2_01

# qhasm: shuf2_2 = extern(_shuf2_2)
# asm 1: lqr <shuf2_2=vec128#16,_shuf2_2
# asm 2: lqr <shuf2_2=$18,_shuf2_2
lqr $18,_shuf2_2

# qhasm: shuf2_3 = extern(_shuf2_3)
# asm 1: lqr <shuf2_3=vec128#17,_shuf2_3
# asm 2: lqr <shuf2_3=$19,_shuf2_3
lqr $19,_shuf2_3

# qhasm: shuf3_01 = extern(_shuf3_01)
# asm 1: lqr <shuf3_01=vec128#18,_shuf3_01
# asm 2: lqr <shuf3_01=$20,_shuf3_01
lqr $20,_shuf3_01

# qhasm: shuf3_2 = extern(_shuf3_2)
# asm 1: lqr <shuf3_2=vec128#19,_shuf3_2
# asm 2: lqr <shuf3_2=$21,_shuf3_2
lqr $21,_shuf3_2

# qhasm: shuf3_3 = extern(_shuf3_3)
# asm 1: lqr <shuf3_3=vec128#20,_shuf3_3
# asm 2: lqr <shuf3_3=$22,_shuf3_3
lqr $22,_shuf3_3

# qhasm: mask12 = extern(_mask12)
# asm 1: lqr <mask12=vec128#21,_mask12
# asm 2: lqr <mask12=$23,_mask12
lqr $23,_mask12

# qhasm: mask13 = extern(_mask13)
# asm 1: lqr <mask13=vec128#22,_mask13
# asm 2: lqr <mask13=$24,_mask13
lqr $24,_mask13

# qhasm: selw0220 = extern(_selw0220)
# asm 1: lqr <selw0220=vec128#23,_selw0220
# asm 2: lqr <selw0220=$25,_selw0220
lqr $25,_selw0220

# qhasm: selw0105 = extern(_selw0105)
# asm 1: lqr <selw0105=vec128#24,_selw0105
# asm 2: lqr <selw0105=$26,_selw0105
lqr $26,_selw0105

# qhasm: selw2325 = extern(_selw2325)
# asm 1: lqr <selw2325=vec128#25,_selw2325
# asm 2: lqr <selw2325=$27,_selw2325
lqr $27,_selw2325

# qhasm: selw0433 = extern (_selw0433)
# asm 1: lqr <selw0433=vec128#26,_selw0433
# asm 2: lqr <selw0433=$28,_selw0433
lqr $28,_selw0433

# qhasm: selw261c0 = extern(_selw261c0)
# asm 1: lqr <selw261c0=vec128#27,_selw261c0
# asm 2: lqr <selw261c0=$29,_selw261c0
lqr $29,_selw261c0

# qhasm: selw0342 = extern(_selw0342)
# asm 1: lqr <selw0342=vec128#28,_selw0342
# asm 2: lqr <selw0342=$30,_selw0342
lqr $30,_selw0342

# qhasm: selw1362 = extern(_selw1362)
# asm 1: lqr <selw1362=vec128#29,_selw1362
# asm 2: lqr <selw1362=$31,_selw1362
lqr $31,_selw1362

# qhasm: selw3333 = extern(_selw3333)
# asm 1: lqr <selw3333=vec128#30,_selw3333
# asm 2: lqr <selw3333=$32,_selw3333
lqr $32,_selw3333

# qhasm: sel01 = extern(select01)
# asm 1: lqr <sel01=vec128#31,select01
# asm 2: lqr <sel01=$33,select01
lqr $33,select01

# qhasm: sel12 = extern(select12)
# asm 1: lqr <sel12=vec128#32,select12
# asm 2: lqr <sel12=$34,select12
lqr $34,select12

# qhasm: sel23 = extern(select23)
# asm 1: lqr <sel23=vec128#33,select23
# asm 2: lqr <sel23=$35,select23
lqr $35,select23

# qhasm: sel30 = extern(select30)
# asm 1: lqr <sel30=vec128#34,select30
# asm 2: lqr <sel30=$36,select30
lqr $36,select30

# qhasm: 2pconsts0 = extern(_2pconsts0)
# asm 1: lqr <2pconsts0=vec128#35,_2pconsts0
# asm 2: lqr <2pconsts0=$37,_2pconsts0
lqr $37,_2pconsts0

# qhasm: 2pconsts = extern(_2pconsts)
# asm 1: lqr <2pconsts=vec128#36,_2pconsts
# asm 2: lqr <2pconsts=$38,_2pconsts
lqr $38,_2pconsts

# qhasm: redcoeffmask = extern(redCoeffMask)
# asm 1: lqr <redcoeffmask=vec128#37,redCoeffMask
# asm 2: lqr <redcoeffmask=$39,redCoeffMask
lqr $39,redCoeffMask

# qhasm: redcoeffmaskend = extern(redCoeffMaskEnd)
# asm 1: lqr <redcoeffmaskend=vec128#38,redCoeffMaskEnd
# asm 2: lqr <redcoeffmaskend=$40,redCoeffMaskEnd
lqr $40,redCoeffMaskEnd

# qhasm: redcoeffmaskveryend = extern(redCoeffMaskVeryEnd)
# asm 1: lqr <redcoeffmaskveryend=vec128#39,redCoeffMaskVeryEnd
# asm 2: lqr <redcoeffmaskveryend=$41,redCoeffMaskVeryEnd
lqr $41,redCoeffMaskVeryEnd

# qhasm: comb13 = extern(combine13)
# asm 1: lqr <comb13=vec128#40,combine13
# asm 2: lqr <comb13=$42,combine13
lqr $42,combine13

# qhasm: comb22 = extern(combine22)
# asm 1: lqr <comb22=vec128#41,combine22
# asm 2: lqr <comb22=$43,combine22
lqr $43,combine22

# qhasm: comb31 = extern(combine31)
# asm 1: lqr <comb31=vec128#42,combine31
# asm 2: lqr <comb31=$44,combine31
lqr $44,combine31

# qhasm: comb31 = extern(combine31)
# asm 1: lqr <comb31=vec128#42,combine31
# asm 2: lqr <comb31=$44,combine31
lqr $44,combine31

# qhasm: 2p_03   = extern(_2p_03)
# asm 1: lqr <2p_03=vec128#43,_2p_03
# asm 2: lqr <2p_03=$45,_2p_03
lqr $45,_2p_03

# qhasm: 2p_47   = extern(_2p_47)
# asm 1: lqr <2p_47=vec128#44,_2p_47
# asm 2: lqr <2p_47=$46,_2p_47
lqr $46,_2p_47

# qhasm: 2p_811  = extern(_2p_811)
# asm 1: lqr <2p_811=vec128#45,_2p_811
# asm 2: lqr <2p_811=$47,_2p_811
lqr $47,_2p_811

# qhasm: 2p_1215 = extern(_2p_1215)
# asm 1: lqr <2p_1215=vec128#46,_2p_1215
# asm 2: lqr <2p_1215=$48,_2p_1215
lqr $48,_2p_1215

# qhasm: 2p_1619 = extern(_2p_1619)
# asm 1: lqr <2p_1619=vec128#47,_2p_1619
# asm 2: lqr <2p_1619=$49,_2p_1619
lqr $49,_2p_1619

# qhasm: a24vec = extern(_a24vec)
# asm 1: lqr <a24vec=vec128#48,_a24vec
# asm 2: lqr <a24vec=$50,_a24vec
lqr $50,_a24vec

# qhasm: int32323232 done = 0
# asm 1: il >done=vec128#49,0
# asm 2: il >done=$51,0
il $51,0

# qhasm: sk = *(vec128 *) ((skp + 16) & ~15)
# asm 1: lqd >sk=vec128#50,16(<skp=vec128#2)
# asm 2: lqd >sk=$52,16(<skp=$4)
lqd $52,16($4)

# qhasm: sk = select bytes from sk by swapendian
# asm 1: shufb >sk=vec128#50,<sk=vec128#50,<sk=vec128#50,<swapendian=vec128#8
# asm 2: shufb >sk=$52,<sk=$52,<sk=$52,<swapendian=$10
shufb $52,$52,$52,$10

# qhasm: x1_03   = *(vec128 *) ((xp +  0) & ~15)
# asm 1: lqd >x1_03=vec128#51,0(<xp=vec128#3)
# asm 2: lqd >x1_03=$53,0(<xp=$5)
lqd $53,0($5)

# qhasm: x1_47   = *(vec128 *) ((xp + 16) & ~15)
# asm 1: lqd >x1_47=vec128#52,16(<xp=vec128#3)
# asm 2: lqd >x1_47=$54,16(<xp=$5)
lqd $54,16($5)

# qhasm: x1_811  = *(vec128 *) ((xp + 32) & ~15)
# asm 1: lqd >x1_811=vec128#53,32(<xp=vec128#3)
# asm 2: lqd >x1_811=$55,32(<xp=$5)
lqd $55,32($5)

# qhasm: x1_1215 = *(vec128 *) ((xp + 48) & ~15)
# asm 1: lqd >x1_1215=vec128#54,48(<xp=vec128#3)
# asm 2: lqd >x1_1215=$56,48(<xp=$5)
lqd $56,48($5)

# qhasm: x1_1619 = *(vec128 *) ((xp + 64) & ~15)
# asm 1: lqd >x1_1619=vec128#3,64(<xp=vec128#3)
# asm 2: lqd >x1_1619=$5,64(<xp=$5)
lqd $5,64($5)

# qhasm: x2_03     = one
# asm 1: ai >x2_03=vec128#55,<one=vec128#5,0
# asm 2: ai >x2_03=$57,<one=$7,0
ai $57,$7,0

# qhasm: x2_47     = zero
# asm 1: ai >x2_47=vec128#56,<zero=vec128#4,0
# asm 2: ai >x2_47=$58,<zero=$6,0
ai $58,$6,0

# qhasm: x2_811    = zero
# asm 1: ai >x2_811=vec128#57,<zero=vec128#4,0
# asm 2: ai >x2_811=$59,<zero=$6,0
ai $59,$6,0

# qhasm: x2_1215   = zero
# asm 1: ai >x2_1215=vec128#58,<zero=vec128#4,0
# asm 2: ai >x2_1215=$60,<zero=$6,0
ai $60,$6,0

# qhasm: x2_1619   = zero
# asm 1: ai >x2_1619=vec128#59,<zero=vec128#4,0
# asm 2: ai >x2_1619=$61,<zero=$6,0
ai $61,$6,0

# qhasm: z2_03     = zero
# asm 1: ai >z2_03=vec128#60,<zero=vec128#4,0
# asm 2: ai >z2_03=$62,<zero=$6,0
ai $62,$6,0

# qhasm: z2_47     = zero
# asm 1: ai >z2_47=vec128#61,<zero=vec128#4,0
# asm 2: ai >z2_47=$63,<zero=$6,0
ai $63,$6,0

# qhasm: z2_811    = zero
# asm 1: ai >z2_811=vec128#62,<zero=vec128#4,0
# asm 2: ai >z2_811=$64,<zero=$6,0
ai $64,$6,0

# qhasm: z2_1215   = zero
# asm 1: ai >z2_1215=vec128#63,<zero=vec128#4,0
# asm 2: ai >z2_1215=$65,<zero=$6,0
ai $65,$6,0

# qhasm: z2_1619   = zero
# asm 1: ai >z2_1619=vec128#64,<zero=vec128#4,0
# asm 2: ai >z2_1619=$66,<zero=$6,0
ai $66,$6,0

# qhasm: x3_03   = x1_03
# asm 1: ai >x3_03=vec128#65,<x1_03=vec128#51,0
# asm 2: ai >x3_03=$67,<x1_03=$53,0
ai $67,$53,0

# qhasm: x3_47   = x1_47
# asm 1: ai >x3_47=vec128#66,<x1_47=vec128#52,0
# asm 2: ai >x3_47=$68,<x1_47=$54,0
ai $68,$54,0

# qhasm: x3_811  = x1_811
# asm 1: ai >x3_811=vec128#67,<x1_811=vec128#53,0
# asm 2: ai >x3_811=$69,<x1_811=$55,0
ai $69,$55,0

# qhasm: x3_1215 = x1_1215
# asm 1: ai >x3_1215=vec128#68,<x1_1215=vec128#54,0
# asm 2: ai >x3_1215=$70,<x1_1215=$56,0
ai $70,$56,0

# qhasm: x3_1619 = x1_1619
# asm 1: ai >x3_1619=vec128#69,<x1_1619=vec128#3,0
# asm 2: ai >x3_1619=$71,<x1_1619=$5,0
ai $71,$5,0

# qhasm: z3_03   = one
# asm 1: ai >z3_03=vec128#5,<one=vec128#5,0
# asm 2: ai >z3_03=$7,<one=$7,0
ai $7,$7,0

# qhasm: z3_47   = zero
# asm 1: ai >z3_47=vec128#70,<zero=vec128#4,0
# asm 2: ai >z3_47=$72,<zero=$6,0
ai $72,$6,0

# qhasm: z3_811  = zero
# asm 1: ai >z3_811=vec128#71,<zero=vec128#4,0
# asm 2: ai >z3_811=$73,<zero=$6,0
ai $73,$6,0

# qhasm: z3_1215 = zero
# asm 1: ai >z3_1215=vec128#72,<zero=vec128#4,0
# asm 2: ai >z3_1215=$74,<zero=$6,0
ai $74,$6,0

# qhasm: z3_1619 = zero
# asm 1: ai >z3_1619=vec128#73,<zero=vec128#4,0
# asm 2: ai >z3_1619=$75,<zero=$6,0
ai $75,$6,0

# qhasm: loop:
._loop:

# qhasm: bit = sk & loopmask
# asm 1: and >bit=vec128#75,<sk=vec128#50,<loopmask=vec128#6
# asm 2: and >bit=$77,<sk=$52,<loopmask=$8
and $77,$52,$8

# qhasm: uint32323232 extbit = 0 - (bit > 0)
# asm 1: clgti >extbit=vec128#75,<bit=vec128#75,0
# asm 2: clgti >extbit=$77,<bit=$77,0
clgti $77,$77,0

# qhasm: tmp0 = extbit <<< (8*4)
# asm 1: rotqbyi >tmp0=vec128#76,<extbit=vec128#75,4
# asm 2: rotqbyi >tmp0=$78,<extbit=$77,4
rotqbyi $78,$77,4

# qhasm: extbit ^= tmp0
# asm 1: xor >extbit=vec128#75,<extbit=vec128#75,<tmp0=vec128#76
# asm 2: xor >extbit=$77,<extbit=$77,<tmp0=$78
xor $77,$77,$78

# qhasm: tmp0 = extbit <<< (8*8)
# asm 1: rotqbyi >tmp0=vec128#76,<extbit=vec128#75,8
# asm 2: rotqbyi >tmp0=$78,<extbit=$77,8
rotqbyi $78,$77,8

# qhasm: extbit ^= tmp0
# asm 1: xor >extbit=vec128#75,<extbit=vec128#75,<tmp0=vec128#76
# asm 2: xor >extbit=$77,<extbit=$77,<tmp0=$78
xor $77,$77,$78

# qhasm: loopmask >>= (1 % 8)
# asm 1: rotqmbii >loopmask=vec128#6,<loopmask=vec128#6,-1
# asm 2: rotqmbii >loopmask=$8,<loopmask=$8,-1
rotqmbii $8,$8,-1

# qhasm: flip = prevextbit ^ extbit
# asm 1: xor >flip=vec128#76,<prevextbit=vec128#7,<extbit=vec128#75
# asm 2: xor >flip=$78,<prevextbit=$9,<extbit=$77
xor $78,$9,$77

# qhasm: nflip = ~(flip | zero)
# asm 1: nor >nflip=vec128#77,<flip=vec128#76,<zero=vec128#4
# asm 2: nor >nflip=$79,<flip=$78,<zero=$6
nor $79,$78,$6

# qhasm: prevextbit = extbit
# asm 1: ai >prevextbit=vec128#7,<extbit=vec128#75,0
# asm 2: ai >prevextbit=$9,<extbit=$77,0
ai $9,$77,0

# qhasm: tmp0 = x2_03   & nflip
# asm 1: and >tmp0=vec128#78,<x2_03=vec128#55,<nflip=vec128#77
# asm 2: and >tmp0=$80,<x2_03=$57,<nflip=$79
and $80,$57,$79

# qhasm: tmp1 = x3_03   & flip
# asm 1: and >tmp1=vec128#79,<x3_03=vec128#65,<flip=vec128#76
# asm 2: and >tmp1=$81,<x3_03=$67,<flip=$78
and $81,$67,$78

# qhasm: tmp2 = x2_03   & flip
# asm 1: and >tmp2=vec128#55,<x2_03=vec128#55,<flip=vec128#76
# asm 2: and >tmp2=$57,<x2_03=$57,<flip=$78
and $57,$57,$78

# qhasm: tmp3 = x3_03   & nflip
# asm 1: and >tmp3=vec128#65,<x3_03=vec128#65,<nflip=vec128#77
# asm 2: and >tmp3=$67,<x3_03=$67,<nflip=$79
and $67,$67,$79

# qhasm: x2_03 = tmp0 ^ tmp1
# asm 1: xor >x2_03=vec128#78,<tmp0=vec128#78,<tmp1=vec128#79
# asm 2: xor >x2_03=$80,<tmp0=$80,<tmp1=$81
xor $80,$80,$81

# qhasm: x3_03 = tmp2 ^ tmp3
# asm 1: xor >x3_03=vec128#55,<tmp2=vec128#55,<tmp3=vec128#65
# asm 2: xor >x3_03=$57,<tmp2=$57,<tmp3=$67
xor $57,$57,$67

# qhasm: tmp0 = x2_47   & nflip
# asm 1: and >tmp0=vec128#65,<x2_47=vec128#56,<nflip=vec128#77
# asm 2: and >tmp0=$67,<x2_47=$58,<nflip=$79
and $67,$58,$79

# qhasm: tmp1 = x3_47   & flip
# asm 1: and >tmp1=vec128#79,<x3_47=vec128#66,<flip=vec128#76
# asm 2: and >tmp1=$81,<x3_47=$68,<flip=$78
and $81,$68,$78

# qhasm: tmp2 = x2_47   & flip
# asm 1: and >tmp2=vec128#56,<x2_47=vec128#56,<flip=vec128#76
# asm 2: and >tmp2=$58,<x2_47=$58,<flip=$78
and $58,$58,$78

# qhasm: tmp3 = x3_47   & nflip
# asm 1: and >tmp3=vec128#66,<x3_47=vec128#66,<nflip=vec128#77
# asm 2: and >tmp3=$68,<x3_47=$68,<nflip=$79
and $68,$68,$79

# qhasm: x2_47 = tmp0 ^ tmp1
# asm 1: xor >x2_47=vec128#65,<tmp0=vec128#65,<tmp1=vec128#79
# asm 2: xor >x2_47=$67,<tmp0=$67,<tmp1=$81
xor $67,$67,$81

# qhasm: x3_47 = tmp2 ^ tmp3
# asm 1: xor >x3_47=vec128#56,<tmp2=vec128#56,<tmp3=vec128#66
# asm 2: xor >x3_47=$58,<tmp2=$58,<tmp3=$68
xor $58,$58,$68

# qhasm: tmp0 = x2_811  & nflip
# asm 1: and >tmp0=vec128#66,<x2_811=vec128#57,<nflip=vec128#77
# asm 2: and >tmp0=$68,<x2_811=$59,<nflip=$79
and $68,$59,$79

# qhasm: tmp1 = x3_811  & flip
# asm 1: and >tmp1=vec128#79,<x3_811=vec128#67,<flip=vec128#76
# asm 2: and >tmp1=$81,<x3_811=$69,<flip=$78
and $81,$69,$78

# qhasm: tmp2 = x2_811  & flip
# asm 1: and >tmp2=vec128#57,<x2_811=vec128#57,<flip=vec128#76
# asm 2: and >tmp2=$59,<x2_811=$59,<flip=$78
and $59,$59,$78

# qhasm: tmp3 = x3_811  & nflip
# asm 1: and >tmp3=vec128#67,<x3_811=vec128#67,<nflip=vec128#77
# asm 2: and >tmp3=$69,<x3_811=$69,<nflip=$79
and $69,$69,$79

# qhasm: x2_811 = tmp0 ^ tmp1
# asm 1: xor >x2_811=vec128#66,<tmp0=vec128#66,<tmp1=vec128#79
# asm 2: xor >x2_811=$68,<tmp0=$68,<tmp1=$81
xor $68,$68,$81

# qhasm: x3_811 = tmp2 ^ tmp3
# asm 1: xor >x3_811=vec128#57,<tmp2=vec128#57,<tmp3=vec128#67
# asm 2: xor >x3_811=$59,<tmp2=$59,<tmp3=$69
xor $59,$59,$69

# qhasm: tmp0 = x2_1215 & nflip
# asm 1: and >tmp0=vec128#67,<x2_1215=vec128#58,<nflip=vec128#77
# asm 2: and >tmp0=$69,<x2_1215=$60,<nflip=$79
and $69,$60,$79

# qhasm: tmp1 = x3_1215 & flip
# asm 1: and >tmp1=vec128#79,<x3_1215=vec128#68,<flip=vec128#76
# asm 2: and >tmp1=$81,<x3_1215=$70,<flip=$78
and $81,$70,$78

# qhasm: tmp2 = x2_1215 & flip
# asm 1: and >tmp2=vec128#58,<x2_1215=vec128#58,<flip=vec128#76
# asm 2: and >tmp2=$60,<x2_1215=$60,<flip=$78
and $60,$60,$78

# qhasm: tmp3 = x3_1215 & nflip
# asm 1: and >tmp3=vec128#68,<x3_1215=vec128#68,<nflip=vec128#77
# asm 2: and >tmp3=$70,<x3_1215=$70,<nflip=$79
and $70,$70,$79

# qhasm: x2_1215 = tmp0 ^ tmp1
# asm 1: xor >x2_1215=vec128#67,<tmp0=vec128#67,<tmp1=vec128#79
# asm 2: xor >x2_1215=$69,<tmp0=$69,<tmp1=$81
xor $69,$69,$81

# qhasm: x3_1215 = tmp2 ^ tmp3
# asm 1: xor >x3_1215=vec128#58,<tmp2=vec128#58,<tmp3=vec128#68
# asm 2: xor >x3_1215=$60,<tmp2=$60,<tmp3=$70
xor $60,$60,$70

# qhasm: tmp0 = x2_1619 & nflip
# asm 1: and >tmp0=vec128#68,<x2_1619=vec128#59,<nflip=vec128#77
# asm 2: and >tmp0=$70,<x2_1619=$61,<nflip=$79
and $70,$61,$79

# qhasm: tmp1 = x3_1619 & flip
# asm 1: and >tmp1=vec128#79,<x3_1619=vec128#69,<flip=vec128#76
# asm 2: and >tmp1=$81,<x3_1619=$71,<flip=$78
and $81,$71,$78

# qhasm: tmp2 = x2_1619 & flip
# asm 1: and >tmp2=vec128#59,<x2_1619=vec128#59,<flip=vec128#76
# asm 2: and >tmp2=$61,<x2_1619=$61,<flip=$78
and $61,$61,$78

# qhasm: tmp3 = x3_1619 & nflip
# asm 1: and >tmp3=vec128#69,<x3_1619=vec128#69,<nflip=vec128#77
# asm 2: and >tmp3=$71,<x3_1619=$71,<nflip=$79
and $71,$71,$79

# qhasm: x2_1619 = tmp0 ^ tmp1
# asm 1: xor >x2_1619=vec128#68,<tmp0=vec128#68,<tmp1=vec128#79
# asm 2: xor >x2_1619=$70,<tmp0=$70,<tmp1=$81
xor $70,$70,$81

# qhasm: x3_1619 = tmp2 ^ tmp3
# asm 1: xor >x3_1619=vec128#59,<tmp2=vec128#59,<tmp3=vec128#69
# asm 2: xor >x3_1619=$61,<tmp2=$61,<tmp3=$71
xor $61,$61,$71

# qhasm: tmp0 = z2_03   & nflip
# asm 1: and >tmp0=vec128#69,<z2_03=vec128#60,<nflip=vec128#77
# asm 2: and >tmp0=$71,<z2_03=$62,<nflip=$79
and $71,$62,$79

# qhasm: tmp1 = z3_03   & flip
# asm 1: and >tmp1=vec128#79,<z3_03=vec128#5,<flip=vec128#76
# asm 2: and >tmp1=$81,<z3_03=$7,<flip=$78
and $81,$7,$78

# qhasm: tmp2 = z2_03   & flip
# asm 1: and >tmp2=vec128#60,<z2_03=vec128#60,<flip=vec128#76
# asm 2: and >tmp2=$62,<z2_03=$62,<flip=$78
and $62,$62,$78

# qhasm: tmp3 = z3_03   & nflip
# asm 1: and >tmp3=vec128#5,<z3_03=vec128#5,<nflip=vec128#77
# asm 2: and >tmp3=$7,<z3_03=$7,<nflip=$79
and $7,$7,$79

# qhasm: z2_03 = tmp0 ^ tmp1
# asm 1: xor >z2_03=vec128#69,<tmp0=vec128#69,<tmp1=vec128#79
# asm 2: xor >z2_03=$71,<tmp0=$71,<tmp1=$81
xor $71,$71,$81

# qhasm: z3_03 = tmp2 ^ tmp3
# asm 1: xor >z3_03=vec128#5,<tmp2=vec128#60,<tmp3=vec128#5
# asm 2: xor >z3_03=$7,<tmp2=$62,<tmp3=$7
xor $7,$62,$7

# qhasm: tmp0 = z2_47   & nflip
# asm 1: and >tmp0=vec128#60,<z2_47=vec128#61,<nflip=vec128#77
# asm 2: and >tmp0=$62,<z2_47=$63,<nflip=$79
and $62,$63,$79

# qhasm: tmp1 = z3_47   & flip
# asm 1: and >tmp1=vec128#79,<z3_47=vec128#70,<flip=vec128#76
# asm 2: and >tmp1=$81,<z3_47=$72,<flip=$78
and $81,$72,$78

# qhasm: tmp2 = z2_47   & flip
# asm 1: and >tmp2=vec128#61,<z2_47=vec128#61,<flip=vec128#76
# asm 2: and >tmp2=$63,<z2_47=$63,<flip=$78
and $63,$63,$78

# qhasm: tmp3 = z3_47   & nflip
# asm 1: and >tmp3=vec128#70,<z3_47=vec128#70,<nflip=vec128#77
# asm 2: and >tmp3=$72,<z3_47=$72,<nflip=$79
and $72,$72,$79

# qhasm: z2_47 = tmp0 ^ tmp1
# asm 1: xor >z2_47=vec128#60,<tmp0=vec128#60,<tmp1=vec128#79
# asm 2: xor >z2_47=$62,<tmp0=$62,<tmp1=$81
xor $62,$62,$81

# qhasm: z3_47 = tmp2 ^ tmp3
# asm 1: xor >z3_47=vec128#61,<tmp2=vec128#61,<tmp3=vec128#70
# asm 2: xor >z3_47=$63,<tmp2=$63,<tmp3=$72
xor $63,$63,$72

# qhasm: tmp0 = z2_811  & nflip
# asm 1: and >tmp0=vec128#70,<z2_811=vec128#62,<nflip=vec128#77
# asm 2: and >tmp0=$72,<z2_811=$64,<nflip=$79
and $72,$64,$79

# qhasm: tmp1 = z3_811  & flip
# asm 1: and >tmp1=vec128#79,<z3_811=vec128#71,<flip=vec128#76
# asm 2: and >tmp1=$81,<z3_811=$73,<flip=$78
and $81,$73,$78

# qhasm: tmp2 = z2_811  & flip
# asm 1: and >tmp2=vec128#62,<z2_811=vec128#62,<flip=vec128#76
# asm 2: and >tmp2=$64,<z2_811=$64,<flip=$78
and $64,$64,$78

# qhasm: tmp3 = z3_811  & nflip
# asm 1: and >tmp3=vec128#71,<z3_811=vec128#71,<nflip=vec128#77
# asm 2: and >tmp3=$73,<z3_811=$73,<nflip=$79
and $73,$73,$79

# qhasm: z2_811 = tmp0 ^ tmp1
# asm 1: xor >z2_811=vec128#70,<tmp0=vec128#70,<tmp1=vec128#79
# asm 2: xor >z2_811=$72,<tmp0=$72,<tmp1=$81
xor $72,$72,$81

# qhasm: z3_811 = tmp2 ^ tmp3
# asm 1: xor >z3_811=vec128#62,<tmp2=vec128#62,<tmp3=vec128#71
# asm 2: xor >z3_811=$64,<tmp2=$64,<tmp3=$73
xor $64,$64,$73

# qhasm: tmp0 = z2_1215 & nflip
# asm 1: and >tmp0=vec128#71,<z2_1215=vec128#63,<nflip=vec128#77
# asm 2: and >tmp0=$73,<z2_1215=$65,<nflip=$79
and $73,$65,$79

# qhasm: tmp1 = z3_1215 & flip
# asm 1: and >tmp1=vec128#79,<z3_1215=vec128#72,<flip=vec128#76
# asm 2: and >tmp1=$81,<z3_1215=$74,<flip=$78
and $81,$74,$78

# qhasm: tmp2 = z2_1215 & flip
# asm 1: and >tmp2=vec128#63,<z2_1215=vec128#63,<flip=vec128#76
# asm 2: and >tmp2=$65,<z2_1215=$65,<flip=$78
and $65,$65,$78

# qhasm: tmp3 = z3_1215 & nflip
# asm 1: and >tmp3=vec128#72,<z3_1215=vec128#72,<nflip=vec128#77
# asm 2: and >tmp3=$74,<z3_1215=$74,<nflip=$79
and $74,$74,$79

# qhasm: z2_1215 = tmp0 ^ tmp1
# asm 1: xor >z2_1215=vec128#71,<tmp0=vec128#71,<tmp1=vec128#79
# asm 2: xor >z2_1215=$73,<tmp0=$73,<tmp1=$81
xor $73,$73,$81

# qhasm: z3_1215 = tmp2 ^ tmp3
# asm 1: xor >z3_1215=vec128#63,<tmp2=vec128#63,<tmp3=vec128#72
# asm 2: xor >z3_1215=$65,<tmp2=$65,<tmp3=$74
xor $65,$65,$74

# qhasm: tmp0 = z2_1619 & nflip
# asm 1: and >tmp0=vec128#72,<z2_1619=vec128#64,<nflip=vec128#77
# asm 2: and >tmp0=$74,<z2_1619=$66,<nflip=$79
and $74,$66,$79

# qhasm: tmp1 = z3_1619 & flip
# asm 1: and >tmp1=vec128#79,<z3_1619=vec128#73,<flip=vec128#76
# asm 2: and >tmp1=$81,<z3_1619=$75,<flip=$78
and $81,$75,$78

# qhasm: tmp2 = z2_1619 & flip
# asm 1: and >tmp2=vec128#64,<z2_1619=vec128#64,<flip=vec128#76
# asm 2: and >tmp2=$66,<z2_1619=$66,<flip=$78
and $66,$66,$78

# qhasm: tmp3 = z3_1619 & nflip
# asm 1: and >tmp3=vec128#73,<z3_1619=vec128#73,<nflip=vec128#77
# asm 2: and >tmp3=$75,<z3_1619=$75,<nflip=$79
and $75,$75,$79

# qhasm: z2_1619 = tmp0 ^ tmp1
# asm 1: xor >z2_1619=vec128#72,<tmp0=vec128#72,<tmp1=vec128#79
# asm 2: xor >z2_1619=$74,<tmp0=$74,<tmp1=$81
xor $74,$74,$81

# qhasm: z3_1619 = tmp2 ^ tmp3
# asm 1: xor >z3_1619=vec128#64,<tmp2=vec128#64,<tmp3=vec128#73
# asm 2: xor >z3_1619=$66,<tmp2=$66,<tmp3=$75
xor $66,$66,$75

# qhasm: int32323232 b_1619 = x2_1619 + 2p_1619
# asm 1: a >b_1619=vec128#73,<x2_1619=vec128#68,<2p_1619=vec128#47
# asm 2: a >b_1619=$75,<x2_1619=$70,<2p_1619=$49
a $75,$70,$49

# qhasm: int32323232 d_1619 = x3_1619 + 2p_1619
# asm 1: a >d_1619=vec128#76,<x3_1619=vec128#59,<2p_1619=vec128#47
# asm 2: a >d_1619=$78,<x3_1619=$61,<2p_1619=$49
a $78,$61,$49

# qhasm: int32323232 a_1619 = x2_1619 + z2_1619
# asm 1: a >a_1619=vec128#68,<x2_1619=vec128#68,<z2_1619=vec128#72
# asm 2: a >a_1619=$70,<x2_1619=$70,<z2_1619=$74
a $70,$70,$74

# qhasm: int32323232 c_1619 = x3_1619 + z3_1619
# asm 1: a >c_1619=vec128#59,<x3_1619=vec128#59,<z3_1619=vec128#64
# asm 2: a >c_1619=$61,<x3_1619=$61,<z3_1619=$66
a $61,$61,$66

# qhasm: int32323232 b_1619 -= z2_1619
# asm 1: sf >b_1619=vec128#72,<z2_1619=vec128#72,<b_1619=vec128#73
# asm 2: sf >b_1619=$74,<z2_1619=$74,<b_1619=$75
sf $74,$74,$75

# qhasm: int32323232 d_1619 -= z3_1619
# asm 1: sf >d_1619=vec128#64,<z3_1619=vec128#64,<d_1619=vec128#76
# asm 2: sf >d_1619=$66,<z3_1619=$66,<d_1619=$78
sf $66,$66,$78

# qhasm: int32323232 b_03 = x2_03 + 2p_03
# asm 1: a >b_03=vec128#73,<x2_03=vec128#78,<2p_03=vec128#43
# asm 2: a >b_03=$75,<x2_03=$80,<2p_03=$45
a $75,$80,$45

# qhasm: acbd16 = combine a_1619 and c_1619 by shuf0_01
# asm 1: shufb >acbd16=vec128#76,<a_1619=vec128#68,<c_1619=vec128#59,<shuf0_01=vec128#9
# asm 2: shufb >acbd16=$78,<a_1619=$70,<c_1619=$61,<shuf0_01=$11
shufb $78,$70,$61,$11

# qhasm: int32323232 d_03 = x3_03 + 2p_03
# asm 1: a >d_03=vec128#77,<x3_03=vec128#55,<2p_03=vec128#43
# asm 2: a >d_03=$79,<x3_03=$57,<2p_03=$45
a $79,$57,$45

# qhasm: acbd17 = combine a_1619 and c_1619 by shuf1_01
# asm 1: shufb >acbd17=vec128#79,<a_1619=vec128#68,<c_1619=vec128#59,<shuf1_01=vec128#12
# asm 2: shufb >acbd17=$81,<a_1619=$70,<c_1619=$61,<shuf1_01=$14
shufb $81,$70,$61,$14

# qhasm: int32323232 a_03 = x2_03 + z2_03
# asm 1: a >a_03=vec128#78,<x2_03=vec128#78,<z2_03=vec128#69
# asm 2: a >a_03=$80,<x2_03=$80,<z2_03=$71
a $80,$80,$71

# qhasm: acbd18 = combine a_1619 and c_1619 by shuf2_01
# asm 1: shufb >acbd18=vec128#80,<a_1619=vec128#68,<c_1619=vec128#59,<shuf2_01=vec128#15
# asm 2: shufb >acbd18=$82,<a_1619=$70,<c_1619=$61,<shuf2_01=$17
shufb $82,$70,$61,$17

# qhasm: int32323232 c_03 = x3_03 + z3_03
# asm 1: a >c_03=vec128#55,<x3_03=vec128#55,<z3_03=vec128#5
# asm 2: a >c_03=$57,<x3_03=$57,<z3_03=$7
a $57,$57,$7

# qhasm: acbd19 = combine a_1619 and c_1619 by shuf3_01
# asm 1: shufb >acbd19=vec128#59,<a_1619=vec128#68,<c_1619=vec128#59,<shuf3_01=vec128#18
# asm 2: shufb >acbd19=$61,<a_1619=$70,<c_1619=$61,<shuf3_01=$20
shufb $61,$70,$61,$20

# qhasm: int32323232 b_03 -= z2_03
# asm 1: sf >b_03=vec128#68,<z2_03=vec128#69,<b_03=vec128#73
# asm 2: sf >b_03=$70,<z2_03=$71,<b_03=$75
sf $70,$71,$75

# qhasm: acbd16 = combine acbd16 and b_1619 by shuf0_2
# asm 1: shufb >acbd16=vec128#69,<acbd16=vec128#76,<b_1619=vec128#72,<shuf0_2=vec128#10
# asm 2: shufb >acbd16=$71,<acbd16=$78,<b_1619=$74,<shuf0_2=$12
shufb $71,$78,$74,$12

# qhasm: int32323232 d_03 -= z3_03
# asm 1: sf >d_03=vec128#5,<z3_03=vec128#5,<d_03=vec128#77
# asm 2: sf >d_03=$7,<z3_03=$7,<d_03=$79
sf $7,$7,$79

# qhasm: acbd17 = combine acbd17 and b_1619 by shuf1_2
# asm 1: shufb >acbd17=vec128#73,<acbd17=vec128#79,<b_1619=vec128#72,<shuf1_2=vec128#13
# asm 2: shufb >acbd17=$75,<acbd17=$81,<b_1619=$74,<shuf1_2=$15
shufb $75,$81,$74,$15

# qhasm: int32323232 b_47 = x2_47 + 2p_47
# asm 1: a >b_47=vec128#76,<x2_47=vec128#65,<2p_47=vec128#44
# asm 2: a >b_47=$78,<x2_47=$67,<2p_47=$46
a $78,$67,$46

# qhasm: acbd18 = combine acbd18 and b_1619 by shuf2_2
# asm 1: shufb >acbd18=vec128#77,<acbd18=vec128#80,<b_1619=vec128#72,<shuf2_2=vec128#16
# asm 2: shufb >acbd18=$79,<acbd18=$82,<b_1619=$74,<shuf2_2=$18
shufb $79,$82,$74,$18

# qhasm: int32323232 d_47 = x3_47 + 2p_47
# asm 1: a >d_47=vec128#79,<x3_47=vec128#56,<2p_47=vec128#44
# asm 2: a >d_47=$81,<x3_47=$58,<2p_47=$46
a $81,$58,$46

# qhasm: acbd19 = combine acbd19 and b_1619 by shuf3_2
# asm 1: shufb >acbd19=vec128#59,<acbd19=vec128#59,<b_1619=vec128#72,<shuf3_2=vec128#19
# asm 2: shufb >acbd19=$61,<acbd19=$61,<b_1619=$74,<shuf3_2=$21
shufb $61,$61,$74,$21

# qhasm: int32323232 a_47 = x2_47 + z2_47
# asm 1: a >a_47=vec128#65,<x2_47=vec128#65,<z2_47=vec128#60
# asm 2: a >a_47=$67,<x2_47=$67,<z2_47=$62
a $67,$67,$62

# qhasm: acbd16 = combine acbd16 and d_1619 by shuf0_3
# asm 1: shufb >acbd16=vec128#69,<acbd16=vec128#69,<d_1619=vec128#64,<shuf0_3=vec128#11
# asm 2: shufb >acbd16=$71,<acbd16=$71,<d_1619=$66,<shuf0_3=$13
shufb $71,$71,$66,$13

# qhasm: int32323232 c_47 = x3_47 + z3_47
# asm 1: a >c_47=vec128#56,<x3_47=vec128#56,<z3_47=vec128#61
# asm 2: a >c_47=$58,<x3_47=$58,<z3_47=$63
a $58,$58,$63

# qhasm: acbd17 = combine acbd17 and d_1619 by shuf1_3
# asm 1: shufb >acbd17=vec128#72,<acbd17=vec128#73,<d_1619=vec128#64,<shuf1_3=vec128#14
# asm 2: shufb >acbd17=$74,<acbd17=$75,<d_1619=$66,<shuf1_3=$16
shufb $74,$75,$66,$16

# qhasm: int32323232 b_47 -= z2_47
# asm 1: sf >b_47=vec128#60,<z2_47=vec128#60,<b_47=vec128#76
# asm 2: sf >b_47=$62,<z2_47=$62,<b_47=$78
sf $62,$62,$78

# qhasm: acbd18 = combine acbd18 and d_1619 by shuf2_3
# asm 1: shufb >acbd18=vec128#73,<acbd18=vec128#77,<d_1619=vec128#64,<shuf2_3=vec128#17
# asm 2: shufb >acbd18=$75,<acbd18=$79,<d_1619=$66,<shuf2_3=$19
shufb $75,$79,$66,$19

# qhasm: int32323232 d_47 -= z3_47
# asm 1: sf >d_47=vec128#61,<z3_47=vec128#61,<d_47=vec128#79
# asm 2: sf >d_47=$63,<z3_47=$63,<d_47=$81
sf $63,$63,$81

# qhasm: acbd19 = combine acbd19 and d_1619 by shuf3_3
# asm 1: shufb >acbd19=vec128#59,<acbd19=vec128#59,<d_1619=vec128#64,<shuf3_3=vec128#20
# asm 2: shufb >acbd19=$61,<acbd19=$61,<d_1619=$66,<shuf3_3=$22
shufb $61,$61,$66,$22

# qhasm: int32323232 b_811 = x2_811 + 2p_811
# asm 1: a >b_811=vec128#64,<x2_811=vec128#66,<2p_811=vec128#45
# asm 2: a >b_811=$66,<x2_811=$68,<2p_811=$47
a $66,$68,$47

# qhasm: acbd0 = combine a_03 and c_03 by shuf0_01
# asm 1: shufb >acbd0=vec128#76,<a_03=vec128#78,<c_03=vec128#55,<shuf0_01=vec128#9
# asm 2: shufb >acbd0=$78,<a_03=$80,<c_03=$57,<shuf0_01=$11
shufb $78,$80,$57,$11

# qhasm: int32323232 d_811 = x3_811 + 2p_811
# asm 1: a >d_811=vec128#77,<x3_811=vec128#57,<2p_811=vec128#45
# asm 2: a >d_811=$79,<x3_811=$59,<2p_811=$47
a $79,$59,$47

# qhasm: acbd1 = combine a_03 and c_03 by shuf1_01
# asm 1: shufb >acbd1=vec128#79,<a_03=vec128#78,<c_03=vec128#55,<shuf1_01=vec128#12
# asm 2: shufb >acbd1=$81,<a_03=$80,<c_03=$57,<shuf1_01=$14
shufb $81,$80,$57,$14

# qhasm: int32323232 a_811 = x2_811 + z2_811
# asm 1: a >a_811=vec128#66,<x2_811=vec128#66,<z2_811=vec128#70
# asm 2: a >a_811=$68,<x2_811=$68,<z2_811=$72
a $68,$68,$72

# qhasm: acbd4 = combine a_47 and c_47 by shuf0_01
# asm 1: shufb >acbd4=vec128#80,<a_47=vec128#65,<c_47=vec128#56,<shuf0_01=vec128#9
# asm 2: shufb >acbd4=$82,<a_47=$67,<c_47=$58,<shuf0_01=$11
shufb $82,$67,$58,$11

# qhasm: int32323232 c_811 = x3_811 + z3_811
# asm 1: a >c_811=vec128#57,<x3_811=vec128#57,<z3_811=vec128#62
# asm 2: a >c_811=$59,<x3_811=$59,<z3_811=$64
a $59,$59,$64

# qhasm: acbd5 = combine a_47 and c_47 by shuf1_01
# asm 1: shufb >acbd5=vec128#81,<a_47=vec128#65,<c_47=vec128#56,<shuf1_01=vec128#12
# asm 2: shufb >acbd5=$83,<a_47=$67,<c_47=$58,<shuf1_01=$14
shufb $83,$67,$58,$14

# qhasm: int32323232 b_811 -= z2_811
# asm 1: sf >b_811=vec128#64,<z2_811=vec128#70,<b_811=vec128#64
# asm 2: sf >b_811=$66,<z2_811=$72,<b_811=$66
sf $66,$72,$66

# qhasm: acbd0 = combine acbd0 and b_03 by shuf0_2
# asm 1: shufb >acbd0=vec128#70,<acbd0=vec128#76,<b_03=vec128#68,<shuf0_2=vec128#10
# asm 2: shufb >acbd0=$72,<acbd0=$78,<b_03=$70,<shuf0_2=$12
shufb $72,$78,$70,$12

# qhasm: int32323232 d_811 -= z3_811
# asm 1: sf >d_811=vec128#62,<z3_811=vec128#62,<d_811=vec128#77
# asm 2: sf >d_811=$64,<z3_811=$64,<d_811=$79
sf $64,$64,$79

# qhasm: acbd1 = combine acbd1 and b_03 by shuf1_2
# asm 1: shufb >acbd1=vec128#76,<acbd1=vec128#79,<b_03=vec128#68,<shuf1_2=vec128#13
# asm 2: shufb >acbd1=$78,<acbd1=$81,<b_03=$70,<shuf1_2=$15
shufb $78,$81,$70,$15

# qhasm: int32323232 b_1215 = x2_1215 + 2p_1215
# asm 1: a >b_1215=vec128#77,<x2_1215=vec128#67,<2p_1215=vec128#46
# asm 2: a >b_1215=$79,<x2_1215=$69,<2p_1215=$48
a $79,$69,$48

# qhasm: acbd4 = combine acbd4 and b_47 by shuf0_2
# asm 1: shufb >acbd4=vec128#79,<acbd4=vec128#80,<b_47=vec128#60,<shuf0_2=vec128#10
# asm 2: shufb >acbd4=$81,<acbd4=$82,<b_47=$62,<shuf0_2=$12
shufb $81,$82,$62,$12

# qhasm: int32323232 d_1215 = x3_1215 + 2p_1215
# asm 1: a >d_1215=vec128#80,<x3_1215=vec128#58,<2p_1215=vec128#46
# asm 2: a >d_1215=$82,<x3_1215=$60,<2p_1215=$48
a $82,$60,$48

# qhasm: acbd5 = combine acbd5 and b_47 by shuf1_2
# asm 1: shufb >acbd5=vec128#81,<acbd5=vec128#81,<b_47=vec128#60,<shuf1_2=vec128#13
# asm 2: shufb >acbd5=$83,<acbd5=$83,<b_47=$62,<shuf1_2=$15
shufb $83,$83,$62,$15

# qhasm: int32323232 a_1215 = x2_1215 + z2_1215
# asm 1: a >a_1215=vec128#67,<x2_1215=vec128#67,<z2_1215=vec128#71
# asm 2: a >a_1215=$69,<x2_1215=$69,<z2_1215=$73
a $69,$69,$73

# qhasm: acbd0 = combine acbd0 and d_03 by shuf0_3
# asm 1: shufb >acbd0=vec128#70,<acbd0=vec128#70,<d_03=vec128#5,<shuf0_3=vec128#11
# asm 2: shufb >acbd0=$72,<acbd0=$72,<d_03=$7,<shuf0_3=$13
shufb $72,$72,$7,$13

# qhasm: int32323232 c_1215 = x3_1215 + z3_1215
# asm 1: a >c_1215=vec128#58,<x3_1215=vec128#58,<z3_1215=vec128#63
# asm 2: a >c_1215=$60,<x3_1215=$60,<z3_1215=$65
a $60,$60,$65

# qhasm: acbd1 = combine acbd1 and d_03 by shuf1_3
# asm 1: shufb >acbd1=vec128#76,<acbd1=vec128#76,<d_03=vec128#5,<shuf1_3=vec128#14
# asm 2: shufb >acbd1=$78,<acbd1=$78,<d_03=$7,<shuf1_3=$16
shufb $78,$78,$7,$16

# qhasm: int32323232 b_1215 -= z2_1215
# asm 1: sf >b_1215=vec128#71,<z2_1215=vec128#71,<b_1215=vec128#77
# asm 2: sf >b_1215=$73,<z2_1215=$73,<b_1215=$79
sf $73,$73,$79

# qhasm: acbd4 = combine acbd4 and d_47 by shuf0_3
# asm 1: shufb >acbd4=vec128#77,<acbd4=vec128#79,<d_47=vec128#61,<shuf0_3=vec128#11
# asm 2: shufb >acbd4=$79,<acbd4=$81,<d_47=$63,<shuf0_3=$13
shufb $79,$81,$63,$13

# qhasm: int32323232 d_1215 -= z3_1215
# asm 1: sf >d_1215=vec128#63,<z3_1215=vec128#63,<d_1215=vec128#80
# asm 2: sf >d_1215=$65,<z3_1215=$65,<d_1215=$82
sf $65,$65,$82

# qhasm: acbd5 = combine acbd5 and d_47 by shuf1_3
# asm 1: shufb >acbd5=vec128#79,<acbd5=vec128#81,<d_47=vec128#61,<shuf1_3=vec128#14
# asm 2: shufb >acbd5=$81,<acbd5=$83,<d_47=$63,<shuf1_3=$16
shufb $81,$83,$63,$16

# qhasm: uint32323232 carry = acbd19 >> 12
# asm 1: rotmi >carry=vec128#80,<acbd19=vec128#59,-12
# asm 2: rotmi >carry=$82,<acbd19=$61,-12
rotmi $82,$61,-12

# qhasm: acbd8 = combine a_811 and c_811 by shuf0_01
# asm 1: shufb >acbd8=vec128#81,<a_811=vec128#66,<c_811=vec128#57,<shuf0_01=vec128#9
# asm 2: shufb >acbd8=$83,<a_811=$68,<c_811=$59,<shuf0_01=$11
shufb $83,$68,$59,$11

# qhasm: acbd9 = combine a_811 and c_811 by shuf1_01
# asm 1: shufb >acbd9=vec128#82,<a_811=vec128#66,<c_811=vec128#57,<shuf1_01=vec128#12
# asm 2: shufb >acbd9=$84,<a_811=$68,<c_811=$59,<shuf1_01=$14
shufb $84,$68,$59,$14

# qhasm: acbd12 = combine a_1215 and c_1215 by shuf0_01
# asm 1: shufb >acbd12=vec128#83,<a_1215=vec128#67,<c_1215=vec128#58,<shuf0_01=vec128#9
# asm 2: shufb >acbd12=$85,<a_1215=$69,<c_1215=$60,<shuf0_01=$11
shufb $85,$69,$60,$11

# qhasm: acbd19 &= mask12
# asm 1: and >acbd19=vec128#59,<acbd19=vec128#59,<mask12=vec128#21
# asm 2: and >acbd19=$61,<acbd19=$61,<mask12=$23
and $61,$61,$23

# qhasm: acbd13 = combine a_1215 and c_1215 by shuf1_01
# asm 1: shufb >acbd13=vec128#84,<a_1215=vec128#67,<c_1215=vec128#58,<shuf1_01=vec128#12
# asm 2: shufb >acbd13=$86,<a_1215=$69,<c_1215=$60,<shuf1_01=$14
shufb $86,$69,$60,$14

# qhasm: uint32323232 carry = (carry & 0xffff) * 19
# asm 1: mpyui >carry=vec128#80,<carry=vec128#80,19
# asm 2: mpyui >carry=$82,<carry=$82,19
mpyui $82,$82,19

# qhasm: acbd8 = combine acbd8 and b_811 by shuf0_2
# asm 1: shufb >acbd8=vec128#81,<acbd8=vec128#81,<b_811=vec128#64,<shuf0_2=vec128#10
# asm 2: shufb >acbd8=$83,<acbd8=$83,<b_811=$66,<shuf0_2=$12
shufb $83,$83,$66,$12

# qhasm: acbd9 = combine acbd9 and b_811 by shuf1_2
# asm 1: shufb >acbd9=vec128#82,<acbd9=vec128#82,<b_811=vec128#64,<shuf1_2=vec128#13
# asm 2: shufb >acbd9=$84,<acbd9=$84,<b_811=$66,<shuf1_2=$15
shufb $84,$84,$66,$15

# qhasm: acbd12 = combine acbd12 and b_1215 by shuf0_2
# asm 1: shufb >acbd12=vec128#83,<acbd12=vec128#83,<b_1215=vec128#71,<shuf0_2=vec128#10
# asm 2: shufb >acbd12=$85,<acbd12=$85,<b_1215=$73,<shuf0_2=$12
shufb $85,$85,$73,$12

# qhasm: acbd13 = combine acbd13 and b_1215 by shuf1_2
# asm 1: shufb >acbd13=vec128#84,<acbd13=vec128#84,<b_1215=vec128#71,<shuf1_2=vec128#13
# asm 2: shufb >acbd13=$86,<acbd13=$86,<b_1215=$73,<shuf1_2=$15
shufb $86,$86,$73,$15

# qhasm: acbd8 = combine acbd8 and d_811 by shuf0_3
# asm 1: shufb >acbd8=vec128#81,<acbd8=vec128#81,<d_811=vec128#62,<shuf0_3=vec128#11
# asm 2: shufb >acbd8=$83,<acbd8=$83,<d_811=$64,<shuf0_3=$13
shufb $83,$83,$64,$13

# qhasm: acbd9 = combine acbd9 and d_811 by shuf1_3
# asm 1: shufb >acbd9=vec128#82,<acbd9=vec128#82,<d_811=vec128#62,<shuf1_3=vec128#14
# asm 2: shufb >acbd9=$84,<acbd9=$84,<d_811=$64,<shuf1_3=$16
shufb $84,$84,$64,$16

# qhasm: acbd12 = combine acbd12 and d_1215 by shuf0_3
# asm 1: shufb >acbd12=vec128#83,<acbd12=vec128#83,<d_1215=vec128#63,<shuf0_3=vec128#11
# asm 2: shufb >acbd12=$85,<acbd12=$85,<d_1215=$65,<shuf0_3=$13
shufb $85,$85,$65,$13

# qhasm: int32323232 acbd0 += carry
# asm 1: a >acbd0=vec128#70,<acbd0=vec128#70,<carry=vec128#80
# asm 2: a >acbd0=$72,<acbd0=$72,<carry=$82
a $72,$72,$82

# qhasm: acbd13 = combine acbd13 and d_1215 by shuf1_3
# asm 1: shufb >acbd13=vec128#80,<acbd13=vec128#84,<d_1215=vec128#63,<shuf1_3=vec128#14
# asm 2: shufb >acbd13=$82,<acbd13=$86,<d_1215=$65,<shuf1_3=$16
shufb $82,$86,$65,$16

# qhasm: uint32323232 carry1 = acbd4  >> 13
# asm 1: rotmi >carry1=vec128#84,<acbd4=vec128#77,-13
# asm 2: rotmi >carry1=$86,<acbd4=$79,-13
rotmi $86,$79,-13

# qhasm: vec19 = extern(_vec19)
# asm 1: lqr <vec19=vec128#74,_vec19
# asm 2: lqr <vec19=$76,_vec19
lqr $76,_vec19

# qhasm: uint32323232 carry0 = acbd0  >> 13
# asm 1: rotmi >carry0=vec128#85,<acbd0=vec128#70,-13
# asm 2: rotmi >carry0=$87,<acbd0=$72,-13
rotmi $87,$72,-13

# qhasm: lnop
 lnop

# qhasm: uint32323232 carry2 = acbd8  >> 13
# asm 1: rotmi >carry2=vec128#86,<acbd8=vec128#81,-13
# asm 2: rotmi >carry2=$88,<acbd8=$83,-13
rotmi $88,$83,-13

# qhasm: uint32323232 carry3 = acbd12 >> 13
# asm 1: rotmi >carry3=vec128#87,<acbd12=vec128#83,-13
# asm 2: rotmi >carry3=$89,<acbd12=$85,-13
rotmi $89,$85,-13

# qhasm: int32323232 acbd5  += carry1
# asm 1: a >acbd5=vec128#79,<acbd5=vec128#79,<carry1=vec128#84
# asm 2: a >acbd5=$81,<acbd5=$81,<carry1=$86
a $81,$81,$86

# qhasm: acbd2 = combine a_03 and c_03 by shuf2_01
# asm 1: shufb >acbd2=vec128#84,<a_03=vec128#78,<c_03=vec128#55,<shuf2_01=vec128#15
# asm 2: shufb >acbd2=$86,<a_03=$80,<c_03=$57,<shuf2_01=$17
shufb $86,$80,$57,$17

# qhasm: acbd4  &= mask13
# asm 1: and >acbd4=vec128#77,<acbd4=vec128#77,<mask13=vec128#22
# asm 2: and >acbd4=$79,<acbd4=$79,<mask13=$24
and $79,$79,$24

# qhasm: acbd6 = combine a_47 and c_47 by shuf2_01
# asm 1: shufb >acbd6=vec128#88,<a_47=vec128#65,<c_47=vec128#56,<shuf2_01=vec128#15
# asm 2: shufb >acbd6=$90,<a_47=$67,<c_47=$58,<shuf2_01=$17
shufb $90,$67,$58,$17

# qhasm: int32323232 acbd1  += carry0
# asm 1: a >acbd1=vec128#76,<acbd1=vec128#76,<carry0=vec128#85
# asm 2: a >acbd1=$78,<acbd1=$78,<carry0=$87
a $78,$78,$87

# qhasm: acbd10 = combine a_811 and c_811 by shuf2_01
# asm 1: shufb >acbd10=vec128#85,<a_811=vec128#66,<c_811=vec128#57,<shuf2_01=vec128#15
# asm 2: shufb >acbd10=$87,<a_811=$68,<c_811=$59,<shuf2_01=$17
shufb $87,$68,$59,$17

# qhasm: acbd0  &= mask13
# asm 1: and >acbd0=vec128#70,<acbd0=vec128#70,<mask13=vec128#22
# asm 2: and >acbd0=$72,<acbd0=$72,<mask13=$24
and $72,$72,$24

# qhasm: acbd14 = combine a_1215 and c_1215 by shuf2_01
# asm 1: shufb >acbd14=vec128#89,<a_1215=vec128#67,<c_1215=vec128#58,<shuf2_01=vec128#15
# asm 2: shufb >acbd14=$91,<a_1215=$69,<c_1215=$60,<shuf2_01=$17
shufb $91,$69,$60,$17

# qhasm: int32323232 acbd9  += carry2
# asm 1: a >acbd9=vec128#82,<acbd9=vec128#82,<carry2=vec128#86
# asm 2: a >acbd9=$84,<acbd9=$84,<carry2=$88
a $84,$84,$88

# qhasm: acbd2 = combine acbd2 and b_03 by shuf2_2
# asm 1: shufb >acbd2=vec128#84,<acbd2=vec128#84,<b_03=vec128#68,<shuf2_2=vec128#16
# asm 2: shufb >acbd2=$86,<acbd2=$86,<b_03=$70,<shuf2_2=$18
shufb $86,$86,$70,$18

# qhasm: acbd8  &= mask13
# asm 1: and >acbd8=vec128#81,<acbd8=vec128#81,<mask13=vec128#22
# asm 2: and >acbd8=$83,<acbd8=$83,<mask13=$24
and $83,$83,$24

# qhasm: acbd6 = combine acbd6 and b_47 by shuf2_2
# asm 1: shufb >acbd6=vec128#86,<acbd6=vec128#88,<b_47=vec128#60,<shuf2_2=vec128#16
# asm 2: shufb >acbd6=$88,<acbd6=$90,<b_47=$62,<shuf2_2=$18
shufb $88,$90,$62,$18

# qhasm: int32323232 acbd13 += carry3
# asm 1: a >acbd13=vec128#80,<acbd13=vec128#80,<carry3=vec128#87
# asm 2: a >acbd13=$82,<acbd13=$82,<carry3=$89
a $82,$82,$89

# qhasm: acbd10 = combine acbd10 and b_811 by shuf2_2
# asm 1: shufb >acbd10=vec128#85,<acbd10=vec128#85,<b_811=vec128#64,<shuf2_2=vec128#16
# asm 2: shufb >acbd10=$87,<acbd10=$87,<b_811=$66,<shuf2_2=$18
shufb $87,$87,$66,$18

# qhasm: acbd12 &= mask13
# asm 1: and >acbd12=vec128#83,<acbd12=vec128#83,<mask13=vec128#22
# asm 2: and >acbd12=$85,<acbd12=$85,<mask13=$24
and $85,$85,$24

# qhasm: acbd14 = combine acbd14 and b_1215 by shuf2_2
# asm 1: shufb >acbd14=vec128#87,<acbd14=vec128#89,<b_1215=vec128#71,<shuf2_2=vec128#16
# asm 2: shufb >acbd14=$89,<acbd14=$91,<b_1215=$73,<shuf2_2=$18
shufb $89,$91,$73,$18

# qhasm: uint32323232 carry0 = acbd1  >> 13
# asm 1: rotmi >carry0=vec128#88,<acbd1=vec128#76,-13
# asm 2: rotmi >carry0=$90,<acbd1=$78,-13
rotmi $90,$78,-13

# qhasm: acbd2 = combine acbd2 and d_03 by shuf2_3
# asm 1: shufb >acbd2=vec128#84,<acbd2=vec128#84,<d_03=vec128#5,<shuf2_3=vec128#17
# asm 2: shufb >acbd2=$86,<acbd2=$86,<d_03=$7,<shuf2_3=$19
shufb $86,$86,$7,$19

# qhasm: uint32323232 carry1 = acbd5  >> 13
# asm 1: rotmi >carry1=vec128#89,<acbd5=vec128#79,-13
# asm 2: rotmi >carry1=$91,<acbd5=$81,-13
rotmi $91,$81,-13

# qhasm: acbd6 = combine acbd6 and d_47 by shuf2_3
# asm 1: shufb >acbd6=vec128#86,<acbd6=vec128#86,<d_47=vec128#61,<shuf2_3=vec128#17
# asm 2: shufb >acbd6=$88,<acbd6=$88,<d_47=$63,<shuf2_3=$19
shufb $88,$88,$63,$19

# qhasm: uint32323232 carry2 = acbd9  >> 13
# asm 1: rotmi >carry2=vec128#90,<acbd9=vec128#82,-13
# asm 2: rotmi >carry2=$92,<acbd9=$84,-13
rotmi $92,$84,-13

# qhasm: acbd10 = combine acbd10 and d_811 by shuf2_3
# asm 1: shufb >acbd10=vec128#85,<acbd10=vec128#85,<d_811=vec128#62,<shuf2_3=vec128#17
# asm 2: shufb >acbd10=$87,<acbd10=$87,<d_811=$64,<shuf2_3=$19
shufb $87,$87,$64,$19

# qhasm: uint32323232 carry3 = acbd13 >> 13
# asm 1: rotmi >carry3=vec128#91,<acbd13=vec128#80,-13
# asm 2: rotmi >carry3=$93,<acbd13=$82,-13
rotmi $93,$82,-13

# qhasm: acbd14 = combine acbd14 and d_1215 by shuf2_3
# asm 1: shufb >acbd14=vec128#87,<acbd14=vec128#87,<d_1215=vec128#63,<shuf2_3=vec128#17
# asm 2: shufb >acbd14=$89,<acbd14=$89,<d_1215=$65,<shuf2_3=$19
shufb $89,$89,$65,$19

# qhasm: int32323232 acbd2  += carry0
# asm 1: a >acbd2=vec128#84,<acbd2=vec128#84,<carry0=vec128#88
# asm 2: a >acbd2=$86,<acbd2=$86,<carry0=$90
a $86,$86,$90

# qhasm: acbd3 = combine a_03 and c_03 by shuf3_01
# asm 1: shufb >acbd3=vec128#55,<a_03=vec128#78,<c_03=vec128#55,<shuf3_01=vec128#18
# asm 2: shufb >acbd3=$57,<a_03=$80,<c_03=$57,<shuf3_01=$20
shufb $57,$80,$57,$20

# qhasm: acbd1  &= mask13
# asm 1: and >acbd1=vec128#76,<acbd1=vec128#76,<mask13=vec128#22
# asm 2: and >acbd1=$78,<acbd1=$78,<mask13=$24
and $78,$78,$24

# qhasm: acbd7 = combine a_47 and c_47 by shuf3_01
# asm 1: shufb >acbd7=vec128#56,<a_47=vec128#65,<c_47=vec128#56,<shuf3_01=vec128#18
# asm 2: shufb >acbd7=$58,<a_47=$67,<c_47=$58,<shuf3_01=$20
shufb $58,$67,$58,$20

# qhasm: int32323232 acbd6  += carry1
# asm 1: a >acbd6=vec128#65,<acbd6=vec128#86,<carry1=vec128#89
# asm 2: a >acbd6=$67,<acbd6=$88,<carry1=$91
a $67,$88,$91

# qhasm: acbd11 = combine a_811 and c_811 by shuf3_01
# asm 1: shufb >acbd11=vec128#57,<a_811=vec128#66,<c_811=vec128#57,<shuf3_01=vec128#18
# asm 2: shufb >acbd11=$59,<a_811=$68,<c_811=$59,<shuf3_01=$20
shufb $59,$68,$59,$20

# qhasm: acbd5  &= mask13
# asm 1: and >acbd5=vec128#66,<acbd5=vec128#79,<mask13=vec128#22
# asm 2: and >acbd5=$68,<acbd5=$81,<mask13=$24
and $68,$81,$24

# qhasm: acbd15 = combine a_1215 and c_1215 by shuf3_01
# asm 1: shufb >acbd15=vec128#58,<a_1215=vec128#67,<c_1215=vec128#58,<shuf3_01=vec128#18
# asm 2: shufb >acbd15=$60,<a_1215=$69,<c_1215=$60,<shuf3_01=$20
shufb $60,$69,$60,$20

# qhasm: int32323232 acbd10 += carry2
# asm 1: a >acbd10=vec128#67,<acbd10=vec128#85,<carry2=vec128#90
# asm 2: a >acbd10=$69,<acbd10=$87,<carry2=$92
a $69,$87,$92

# qhasm: acbd3 = combine acbd3 and b_03 by shuf3_2
# asm 1: shufb >acbd3=vec128#55,<acbd3=vec128#55,<b_03=vec128#68,<shuf3_2=vec128#19
# asm 2: shufb >acbd3=$57,<acbd3=$57,<b_03=$70,<shuf3_2=$21
shufb $57,$57,$70,$21

# qhasm: acbd9  &= mask13
# asm 1: and >acbd9=vec128#68,<acbd9=vec128#82,<mask13=vec128#22
# asm 2: and >acbd9=$70,<acbd9=$84,<mask13=$24
and $70,$84,$24

# qhasm: acbd7 = combine acbd7 and b_47 by shuf3_2
# asm 1: shufb >acbd7=vec128#56,<acbd7=vec128#56,<b_47=vec128#60,<shuf3_2=vec128#19
# asm 2: shufb >acbd7=$58,<acbd7=$58,<b_47=$62,<shuf3_2=$21
shufb $58,$58,$62,$21

# qhasm: int32323232 acbd14 += carry3
# asm 1: a >acbd14=vec128#60,<acbd14=vec128#87,<carry3=vec128#91
# asm 2: a >acbd14=$62,<acbd14=$89,<carry3=$93
a $62,$89,$93

# qhasm: acbd11 = combine acbd11 and b_811 by shuf3_2
# asm 1: shufb >acbd11=vec128#57,<acbd11=vec128#57,<b_811=vec128#64,<shuf3_2=vec128#19
# asm 2: shufb >acbd11=$59,<acbd11=$59,<b_811=$66,<shuf3_2=$21
shufb $59,$59,$66,$21

# qhasm: acbd13 &= mask13
# asm 1: and >acbd13=vec128#64,<acbd13=vec128#80,<mask13=vec128#22
# asm 2: and >acbd13=$66,<acbd13=$82,<mask13=$24
and $66,$82,$24

# qhasm: acbd15 = combine acbd15 and b_1215 by shuf3_2
# asm 1: shufb >acbd15=vec128#58,<acbd15=vec128#58,<b_1215=vec128#71,<shuf3_2=vec128#19
# asm 2: shufb >acbd15=$60,<acbd15=$60,<b_1215=$73,<shuf3_2=$21
shufb $60,$60,$73,$21

# qhasm: uint32323232 carry0 = acbd2  >> 13
# asm 1: rotmi >carry0=vec128#71,<acbd2=vec128#84,-13
# asm 2: rotmi >carry0=$73,<acbd2=$86,-13
rotmi $73,$86,-13

# qhasm: acbd3 = combine acbd3 and d_03 by shuf3_3
# asm 1: shufb >acbd3=vec128#5,<acbd3=vec128#55,<d_03=vec128#5,<shuf3_3=vec128#20
# asm 2: shufb >acbd3=$7,<acbd3=$57,<d_03=$7,<shuf3_3=$22
shufb $7,$57,$7,$22

# qhasm: uint32323232 carry1 = acbd6  >> 13
# asm 1: rotmi >carry1=vec128#55,<acbd6=vec128#65,-13
# asm 2: rotmi >carry1=$57,<acbd6=$67,-13
rotmi $57,$67,-13

# qhasm: acbd7 = combine acbd7 and d_47 by shuf3_3
# asm 1: shufb >acbd7=vec128#56,<acbd7=vec128#56,<d_47=vec128#61,<shuf3_3=vec128#20
# asm 2: shufb >acbd7=$58,<acbd7=$58,<d_47=$63,<shuf3_3=$22
shufb $58,$58,$63,$22

# qhasm: uint32323232 carry2 = acbd10 >> 13
# asm 1: rotmi >carry2=vec128#61,<acbd10=vec128#67,-13
# asm 2: rotmi >carry2=$63,<acbd10=$69,-13
rotmi $63,$69,-13

# qhasm: acbd11 = combine acbd11 and d_811 by shuf3_3
# asm 1: shufb >acbd11=vec128#57,<acbd11=vec128#57,<d_811=vec128#62,<shuf3_3=vec128#20
# asm 2: shufb >acbd11=$59,<acbd11=$59,<d_811=$64,<shuf3_3=$22
shufb $59,$59,$64,$22

# qhasm: uint32323232 carry3 = acbd14 >> 13
# asm 1: rotmi >carry3=vec128#62,<acbd14=vec128#60,-13
# asm 2: rotmi >carry3=$64,<acbd14=$62,-13
rotmi $64,$62,-13

# qhasm: acbd15 = combine acbd15 and d_1215 by shuf3_3
# asm 1: shufb >acbd15=vec128#58,<acbd15=vec128#58,<d_1215=vec128#63,<shuf3_3=vec128#20
# asm 2: shufb >acbd15=$60,<acbd15=$60,<d_1215=$65,<shuf3_3=$22
shufb $60,$60,$65,$22

# qhasm: acbd2  &= mask13
# asm 1: and >acbd2=vec128#63,<acbd2=vec128#84,<mask13=vec128#22
# asm 2: and >acbd2=$65,<acbd2=$86,<mask13=$24
and $65,$86,$24

# qhasm: acbd6  &= mask13
# asm 1: and >acbd6=vec128#65,<acbd6=vec128#65,<mask13=vec128#22
# asm 2: and >acbd6=$67,<acbd6=$67,<mask13=$24
and $67,$67,$24

# qhasm: acbd10 &= mask13
# asm 1: and >acbd10=vec128#67,<acbd10=vec128#67,<mask13=vec128#22
# asm 2: and >acbd10=$69,<acbd10=$69,<mask13=$24
and $69,$69,$24

# qhasm: acbd14 &= mask13
# asm 1: and >acbd14=vec128#60,<acbd14=vec128#60,<mask13=vec128#22
# asm 2: and >acbd14=$62,<acbd14=$62,<mask13=$24
and $62,$62,$24

# qhasm: int32323232 acbd3  += carry0
# asm 1: a >acbd3=vec128#5,<acbd3=vec128#5,<carry0=vec128#71
# asm 2: a >acbd3=$7,<acbd3=$7,<carry0=$73
a $7,$7,$73

# qhasm: int32323232 acbd7  += carry1
# asm 1: a >acbd7=vec128#55,<acbd7=vec128#56,<carry1=vec128#55
# asm 2: a >acbd7=$57,<acbd7=$58,<carry1=$57
a $57,$58,$57

# qhasm: int32323232 acbd11 += carry2
# asm 1: a >acbd11=vec128#56,<acbd11=vec128#57,<carry2=vec128#61
# asm 2: a >acbd11=$58,<acbd11=$59,<carry2=$63
a $58,$59,$63

# qhasm: int32323232 acbd15 += carry3
# asm 1: a >acbd15=vec128#57,<acbd15=vec128#58,<carry3=vec128#62
# asm 2: a >acbd15=$59,<acbd15=$60,<carry3=$64
a $59,$60,$64

# qhasm: uint32323232 carry0 = acbd3  >> 12
# asm 1: rotmi >carry0=vec128#58,<acbd3=vec128#5,-12
# asm 2: rotmi >carry0=$60,<acbd3=$7,-12
rotmi $60,$7,-12

# qhasm: uint32323232 carry1 = acbd7  >> 12
# asm 1: rotmi >carry1=vec128#61,<acbd7=vec128#55,-12
# asm 2: rotmi >carry1=$63,<acbd7=$57,-12
rotmi $63,$57,-12

# qhasm: uint32323232 carry2 = acbd11 >> 12
# asm 1: rotmi >carry2=vec128#62,<acbd11=vec128#56,-12
# asm 2: rotmi >carry2=$64,<acbd11=$58,-12
rotmi $64,$58,-12

# qhasm: uint32323232 carry3 = acbd15 >> 12
# asm 1: rotmi >carry3=vec128#71,<acbd15=vec128#57,-12
# asm 2: rotmi >carry3=$73,<acbd15=$59,-12
rotmi $73,$59,-12

# qhasm: acbd3  &= mask12
# asm 1: and >acbd3=vec128#5,<acbd3=vec128#5,<mask12=vec128#21
# asm 2: and >acbd3=$7,<acbd3=$7,<mask12=$23
and $7,$7,$23

# qhasm: acbd7  &= mask12
# asm 1: and >acbd7=vec128#55,<acbd7=vec128#55,<mask12=vec128#21
# asm 2: and >acbd7=$57,<acbd7=$57,<mask12=$23
and $57,$57,$23

# qhasm: acbd11 &= mask12
# asm 1: and >acbd11=vec128#56,<acbd11=vec128#56,<mask12=vec128#21
# asm 2: and >acbd11=$58,<acbd11=$58,<mask12=$23
and $58,$58,$23

# qhasm: abba0  = select bytes from acbd0  by selw0220
# asm 1: shufb >abba0=vec128#78,<acbd0=vec128#70,<acbd0=vec128#70,<selw0220=vec128#23
# asm 2: shufb >abba0=$80,<acbd0=$72,<acbd0=$72,<selw0220=$25
shufb $80,$72,$72,$25

# qhasm: acbd15 &= mask12
# asm 1: and >acbd15=vec128#57,<acbd15=vec128#57,<mask12=vec128#21
# asm 2: and >acbd15=$59,<acbd15=$59,<mask12=$23
and $59,$59,$23

# qhasm: abba1  = select bytes from acbd1  by selw0220
# asm 1: shufb >abba1=vec128#79,<acbd1=vec128#76,<acbd1=vec128#76,<selw0220=vec128#23
# asm 2: shufb >abba1=$81,<acbd1=$78,<acbd1=$78,<selw0220=$25
shufb $81,$78,$78,$25

# qhasm: int32323232 acbd4  += carry0
# asm 1: a >acbd4=vec128#58,<acbd4=vec128#77,<carry0=vec128#58
# asm 2: a >acbd4=$60,<acbd4=$79,<carry0=$60
a $60,$79,$60

# qhasm: abba2  = select bytes from acbd2  by selw0220
# asm 1: shufb >abba2=vec128#77,<acbd2=vec128#63,<acbd2=vec128#63,<selw0220=vec128#23
# asm 2: shufb >abba2=$79,<acbd2=$65,<acbd2=$65,<selw0220=$25
shufb $79,$65,$65,$25

# qhasm: int32323232 acbd8  += carry1
# asm 1: a >acbd8=vec128#61,<acbd8=vec128#81,<carry1=vec128#61
# asm 2: a >acbd8=$63,<acbd8=$83,<carry1=$63
a $63,$83,$63

# qhasm: abba3  = select bytes from acbd3  by selw0220
# asm 1: shufb >abba3=vec128#80,<acbd3=vec128#5,<acbd3=vec128#5,<selw0220=vec128#23
# asm 2: shufb >abba3=$82,<acbd3=$7,<acbd3=$7,<selw0220=$25
shufb $82,$7,$7,$25

# qhasm: int32323232 acbd12 += carry2
# asm 1: a >acbd12=vec128#62,<acbd12=vec128#83,<carry2=vec128#62
# asm 2: a >acbd12=$64,<acbd12=$85,<carry2=$64
a $64,$85,$64

# qhasm: int32323232 acbd16 += carry3
# asm 1: a >acbd16=vec128#69,<acbd16=vec128#69,<carry3=vec128#71
# asm 2: a >acbd16=$71,<acbd16=$71,<carry3=$73
a $71,$71,$73

# qhasm: uint32323232 carry1 = acbd4  >> 13
# asm 1: rotmi >carry1=vec128#71,<acbd4=vec128#58,-13
# asm 2: rotmi >carry1=$73,<acbd4=$60,-13
rotmi $73,$60,-13

# qhasm: uint32323232 carry2 = acbd8  >> 13
# asm 1: rotmi >carry2=vec128#81,<acbd8=vec128#61,-13
# asm 2: rotmi >carry2=$83,<acbd8=$63,-13
rotmi $83,$63,-13

# qhasm: uint32323232 carry3 = acbd12 >> 13
# asm 1: rotmi >carry3=vec128#82,<acbd12=vec128#62,-13
# asm 2: rotmi >carry3=$84,<acbd12=$64,-13
rotmi $84,$64,-13

# qhasm: uint32323232 carry4 = acbd16 >> 13
# asm 1: rotmi >carry4=vec128#83,<acbd16=vec128#69,-13
# asm 2: rotmi >carry4=$85,<acbd16=$71,-13
rotmi $85,$71,-13

# qhasm: acbd4  &= mask13
# asm 1: and >acbd4=vec128#58,<acbd4=vec128#58,<mask13=vec128#22
# asm 2: and >acbd4=$60,<acbd4=$60,<mask13=$24
and $60,$60,$24

# qhasm: acbd8  &= mask13
# asm 1: and >acbd8=vec128#61,<acbd8=vec128#61,<mask13=vec128#22
# asm 2: and >acbd8=$63,<acbd8=$63,<mask13=$24
and $63,$63,$24

# qhasm: acbd12 &= mask13
# asm 1: and >acbd12=vec128#62,<acbd12=vec128#62,<mask13=vec128#22
# asm 2: and >acbd12=$64,<acbd12=$64,<mask13=$24
and $64,$64,$24

# qhasm: abba4  = select bytes from acbd4  by selw0220
# asm 1: shufb >abba4=vec128#84,<acbd4=vec128#58,<acbd4=vec128#58,<selw0220=vec128#23
# asm 2: shufb >abba4=$86,<acbd4=$60,<acbd4=$60,<selw0220=$25
shufb $86,$60,$60,$25

# qhasm: acbd16  &= mask13
# asm 1: and >acbd16=vec128#69,<acbd16=vec128#69,<mask13=vec128#22
# asm 2: and >acbd16=$71,<acbd16=$71,<mask13=$24
and $71,$71,$24

# qhasm: abba8  = select bytes from acbd8  by selw0220
# asm 1: shufb >abba8=vec128#85,<acbd8=vec128#61,<acbd8=vec128#61,<selw0220=vec128#23
# asm 2: shufb >abba8=$87,<acbd8=$63,<acbd8=$63,<selw0220=$25
shufb $87,$63,$63,$25

# qhasm: int32323232 acbd5  += carry1
# asm 1: a >acbd5=vec128#66,<acbd5=vec128#66,<carry1=vec128#71
# asm 2: a >acbd5=$68,<acbd5=$68,<carry1=$73
a $68,$68,$73

# qhasm: abba12 = select bytes from acbd12 by selw0220
# asm 1: shufb >abba12=vec128#71,<acbd12=vec128#62,<acbd12=vec128#62,<selw0220=vec128#23
# asm 2: shufb >abba12=$73,<acbd12=$64,<acbd12=$64,<selw0220=$25
shufb $73,$64,$64,$25

# qhasm: int32323232 acbd9  += carry2
# asm 1: a >acbd9=vec128#68,<acbd9=vec128#68,<carry2=vec128#81
# asm 2: a >acbd9=$70,<acbd9=$70,<carry2=$83
a $70,$70,$83

# qhasm: abba16 = select bytes from acbd16 by selw0220
# asm 1: shufb >abba16=vec128#81,<acbd16=vec128#69,<acbd16=vec128#69,<selw0220=vec128#23
# asm 2: shufb >abba16=$83,<acbd16=$71,<acbd16=$71,<selw0220=$25
shufb $83,$71,$71,$25

# qhasm: int32323232 acbd13 += carry3
# asm 1: a >acbd13=vec128#64,<acbd13=vec128#64,<carry3=vec128#82
# asm 2: a >acbd13=$66,<acbd13=$66,<carry3=$84
a $66,$66,$84

# qhasm: int32323232 acbd17  += carry4
# asm 1: a >acbd17=vec128#72,<acbd17=vec128#72,<carry4=vec128#83
# asm 2: a >acbd17=$74,<acbd17=$74,<carry4=$85
a $74,$74,$85

# qhasm: uint32323232 carry1 = acbd5  >> 13
# asm 1: rotmi >carry1=vec128#82,<acbd5=vec128#66,-13
# asm 2: rotmi >carry1=$84,<acbd5=$68,-13
rotmi $84,$68,-13

# qhasm: uint32323232 carry2 = acbd9  >> 13
# asm 1: rotmi >carry2=vec128#83,<acbd9=vec128#68,-13
# asm 2: rotmi >carry2=$85,<acbd9=$70,-13
rotmi $85,$70,-13

# qhasm: uint32323232 carry3 = acbd13 >> 13
# asm 1: rotmi >carry3=vec128#86,<acbd13=vec128#64,-13
# asm 2: rotmi >carry3=$88,<acbd13=$66,-13
rotmi $88,$66,-13

# qhasm: uint32323232 carry4 = acbd17 >> 13
# asm 1: rotmi >carry4=vec128#87,<acbd17=vec128#72,-13
# asm 2: rotmi >carry4=$89,<acbd17=$74,-13
rotmi $89,$74,-13

# qhasm: acbd5  &= mask13
# asm 1: and >acbd5=vec128#66,<acbd5=vec128#66,<mask13=vec128#22
# asm 2: and >acbd5=$68,<acbd5=$68,<mask13=$24
and $68,$68,$24

# qhasm: acbd9  &= mask13
# asm 1: and >acbd9=vec128#68,<acbd9=vec128#68,<mask13=vec128#22
# asm 2: and >acbd9=$70,<acbd9=$70,<mask13=$24
and $70,$70,$24

# qhasm: acbd13 &= mask13
# asm 1: and >acbd13=vec128#64,<acbd13=vec128#64,<mask13=vec128#22
# asm 2: and >acbd13=$66,<acbd13=$66,<mask13=$24
and $66,$66,$24

# qhasm: abba5  = select bytes from acbd5  by selw0220
# asm 1: shufb >abba5=vec128#88,<acbd5=vec128#66,<acbd5=vec128#66,<selw0220=vec128#23
# asm 2: shufb >abba5=$90,<acbd5=$68,<acbd5=$68,<selw0220=$25
shufb $90,$68,$68,$25

# qhasm: acbd17 &= mask13
# asm 1: and >acbd17=vec128#72,<acbd17=vec128#72,<mask13=vec128#22
# asm 2: and >acbd17=$74,<acbd17=$74,<mask13=$24
and $74,$74,$24

# qhasm: abba9  = select bytes from acbd9  by selw0220
# asm 1: shufb >abba9=vec128#89,<acbd9=vec128#68,<acbd9=vec128#68,<selw0220=vec128#23
# asm 2: shufb >abba9=$91,<acbd9=$70,<acbd9=$70,<selw0220=$25
shufb $91,$70,$70,$25

# qhasm: int32323232 acbd6  += carry1
# asm 1: a >acbd6=vec128#65,<acbd6=vec128#65,<carry1=vec128#82
# asm 2: a >acbd6=$67,<acbd6=$67,<carry1=$84
a $67,$67,$84

# qhasm: abba13 = select bytes from acbd13 by selw0220
# asm 1: shufb >abba13=vec128#82,<acbd13=vec128#64,<acbd13=vec128#64,<selw0220=vec128#23
# asm 2: shufb >abba13=$84,<acbd13=$66,<acbd13=$66,<selw0220=$25
shufb $84,$66,$66,$25

# qhasm: int32323232 acbd10 += carry2
# asm 1: a >acbd10=vec128#67,<acbd10=vec128#67,<carry2=vec128#83
# asm 2: a >acbd10=$69,<acbd10=$69,<carry2=$85
a $69,$69,$85

# qhasm: abba17 = select bytes from acbd17 by selw0220
# asm 1: shufb >abba17=vec128#83,<acbd17=vec128#72,<acbd17=vec128#72,<selw0220=vec128#23
# asm 2: shufb >abba17=$85,<acbd17=$74,<acbd17=$74,<selw0220=$25
shufb $85,$74,$74,$25

# qhasm: int32323232 acbd14 += carry3
# asm 1: a >acbd14=vec128#60,<acbd14=vec128#60,<carry3=vec128#86
# asm 2: a >acbd14=$62,<acbd14=$62,<carry3=$88
a $62,$62,$88

# qhasm: int32323232 acbd18 += carry4
# asm 1: a >acbd18=vec128#73,<acbd18=vec128#73,<carry4=vec128#87
# asm 2: a >acbd18=$75,<acbd18=$75,<carry4=$89
a $75,$75,$89

# qhasm: uint32323232 carry1 = acbd6  >> 13
# asm 1: rotmi >carry1=vec128#86,<acbd6=vec128#65,-13
# asm 2: rotmi >carry1=$88,<acbd6=$67,-13
rotmi $88,$67,-13

# qhasm: uint32323232 carry2 = acbd10 >> 13
# asm 1: rotmi >carry2=vec128#87,<acbd10=vec128#67,-13
# asm 2: rotmi >carry2=$89,<acbd10=$69,-13
rotmi $89,$69,-13

# qhasm: uint32323232 carry3 = acbd14 >> 13
# asm 1: rotmi >carry3=vec128#90,<acbd14=vec128#60,-13
# asm 2: rotmi >carry3=$92,<acbd14=$62,-13
rotmi $92,$62,-13

# qhasm: uint32323232 carry4 = acbd18  >> 13
# asm 1: rotmi >carry4=vec128#91,<acbd18=vec128#73,-13
# asm 2: rotmi >carry4=$93,<acbd18=$75,-13
rotmi $93,$75,-13

# qhasm: acbd6  &= mask13
# asm 1: and >acbd6=vec128#65,<acbd6=vec128#65,<mask13=vec128#22
# asm 2: and >acbd6=$67,<acbd6=$67,<mask13=$24
and $67,$67,$24

# qhasm: acbd10 &= mask13
# asm 1: and >acbd10=vec128#67,<acbd10=vec128#67,<mask13=vec128#22
# asm 2: and >acbd10=$69,<acbd10=$69,<mask13=$24
and $69,$69,$24

# qhasm: acbd14 &= mask13
# asm 1: and >acbd14=vec128#60,<acbd14=vec128#60,<mask13=vec128#22
# asm 2: and >acbd14=$62,<acbd14=$62,<mask13=$24
and $62,$62,$24

# qhasm: abba6  = select bytes from acbd6  by selw0220
# asm 1: shufb >abba6=vec128#92,<acbd6=vec128#65,<acbd6=vec128#65,<selw0220=vec128#23
# asm 2: shufb >abba6=$94,<acbd6=$67,<acbd6=$67,<selw0220=$25
shufb $94,$67,$67,$25

# qhasm: acbd18  &= mask13
# asm 1: and >acbd18=vec128#73,<acbd18=vec128#73,<mask13=vec128#22
# asm 2: and >acbd18=$75,<acbd18=$75,<mask13=$24
and $75,$75,$24

# qhasm: abba10 = select bytes from acbd10 by selw0220
# asm 1: shufb >abba10=vec128#93,<acbd10=vec128#67,<acbd10=vec128#67,<selw0220=vec128#23
# asm 2: shufb >abba10=$95,<acbd10=$69,<acbd10=$69,<selw0220=$25
shufb $95,$69,$69,$25

# qhasm: int32323232 acbd7  += carry1
# asm 1: a >acbd7=vec128#55,<acbd7=vec128#55,<carry1=vec128#86
# asm 2: a >acbd7=$57,<acbd7=$57,<carry1=$88
a $57,$57,$88

# qhasm: abba14 = select bytes from acbd14 by selw0220
# asm 1: shufb >abba14=vec128#86,<acbd14=vec128#60,<acbd14=vec128#60,<selw0220=vec128#23
# asm 2: shufb >abba14=$88,<acbd14=$62,<acbd14=$62,<selw0220=$25
shufb $88,$62,$62,$25

# qhasm: int32323232 acbd11 += carry2
# asm 1: a >acbd11=vec128#56,<acbd11=vec128#56,<carry2=vec128#87
# asm 2: a >acbd11=$58,<acbd11=$58,<carry2=$89
a $58,$58,$89

# qhasm: abba18 = select bytes from acbd18 by selw0220
# asm 1: shufb >abba18=vec128#87,<acbd18=vec128#73,<acbd18=vec128#73,<selw0220=vec128#23
# asm 2: shufb >abba18=$89,<acbd18=$75,<acbd18=$75,<selw0220=$25
shufb $89,$75,$75,$25

# qhasm: int32323232 acbd15 += carry3
# asm 1: a >acbd15=vec128#57,<acbd15=vec128#57,<carry3=vec128#90
# asm 2: a >acbd15=$59,<acbd15=$59,<carry3=$92
a $59,$59,$92

# qhasm: int32323232 acbd19  += carry4
# asm 1: a >acbd19=vec128#59,<acbd19=vec128#59,<carry4=vec128#91
# asm 2: a >acbd19=$61,<acbd19=$61,<carry4=$93
a $61,$61,$93

# qhasm: int32323232 aacbbbda0  = (acbd0 & 0xffff) * (abba0 & 0xffff)
# asm 1: mpy >aacbbbda0=vec128#90,<acbd0=vec128#70,<abba0=vec128#78
# asm 2: mpy >aacbbbda0=$92,<acbd0=$72,<abba0=$80
mpy $92,$72,$80

# qhasm: abba7  = select bytes from acbd7  by selw0220
# asm 1: shufb >abba7=vec128#91,<acbd7=vec128#55,<acbd7=vec128#55,<selw0220=vec128#23
# asm 2: shufb >abba7=$93,<acbd7=$57,<acbd7=$57,<selw0220=$25
shufb $93,$57,$57,$25

# qhasm: int32323232 aacbbbda1  = (acbd0 & 0xffff) * (abba1 & 0xffff)
# asm 1: mpy >aacbbbda1=vec128#94,<acbd0=vec128#70,<abba1=vec128#79
# asm 2: mpy >aacbbbda1=$96,<acbd0=$72,<abba1=$81
mpy $96,$72,$81

# qhasm: abba11 = select bytes from acbd11 by selw0220
# asm 1: shufb >abba11=vec128#95,<acbd11=vec128#56,<acbd11=vec128#56,<selw0220=vec128#23
# asm 2: shufb >abba11=$97,<acbd11=$58,<acbd11=$58,<selw0220=$25
shufb $97,$58,$58,$25

# qhasm: int32323232 aacbbbda2  = (acbd0 & 0xffff) * (abba2 & 0xffff)
# asm 1: mpy >aacbbbda2=vec128#96,<acbd0=vec128#70,<abba2=vec128#77
# asm 2: mpy >aacbbbda2=$98,<acbd0=$72,<abba2=$79
mpy $98,$72,$79

# qhasm: abba15 = select bytes from acbd15 by selw0220
# asm 1: shufb >abba15=vec128#97,<acbd15=vec128#57,<acbd15=vec128#57,<selw0220=vec128#23
# asm 2: shufb >abba15=$99,<acbd15=$59,<acbd15=$59,<selw0220=$25
shufb $99,$59,$59,$25

# qhasm: int32323232 aacbbbda3  = (acbd0 & 0xffff) * (abba3 & 0xffff)
# asm 1: mpy >aacbbbda3=vec128#98,<acbd0=vec128#70,<abba3=vec128#80
# asm 2: mpy >aacbbbda3=$100,<acbd0=$72,<abba3=$82
mpy $100,$72,$82

# qhasm: abba19 = select bytes from acbd19 by selw0220
# asm 1: shufb >abba19=vec128#99,<acbd19=vec128#59,<acbd19=vec128#59,<selw0220=vec128#23
# asm 2: shufb >abba19=$101,<acbd19=$61,<acbd19=$61,<selw0220=$25
shufb $101,$61,$61,$25

# qhasm: int32323232 aacbbbda4  = (acbd1 & 0xffff) * (abba3 & 0xffff)
# asm 1: mpy >aacbbbda4=vec128#100,<acbd1=vec128#76,<abba3=vec128#80
# asm 2: mpy >aacbbbda4=$102,<acbd1=$78,<abba3=$82
mpy $102,$78,$82

# qhasm: int32323232 aacbbbda5  = (acbd2 & 0xffff) * (abba3 & 0xffff)
# asm 1: mpy >aacbbbda5=vec128#101,<acbd2=vec128#63,<abba3=vec128#80
# asm 2: mpy >aacbbbda5=$103,<acbd2=$65,<abba3=$82
mpy $103,$65,$82

# qhasm: int32323232 aacbbbda6  = (acbd3 & 0xffff) * (abba3 & 0xffff)
# asm 1: mpy >aacbbbda6=vec128#102,<acbd3=vec128#5,<abba3=vec128#80
# asm 2: mpy >aacbbbda6=$104,<acbd3=$7,<abba3=$82
mpy $104,$7,$82

# qhasm: int32323232 aacbbbda7  = (acbd0 & 0xffff) * (abba7 & 0xffff)
# asm 1: mpy >aacbbbda7=vec128#103,<acbd0=vec128#70,<abba7=vec128#91
# asm 2: mpy >aacbbbda7=$105,<acbd0=$72,<abba7=$93
mpy $105,$72,$93

# qhasm: int32323232 aacbbbda1 += (acbd1 & 0xffff) * (abba0 & 0xffff)
# asm 1: mpya >aacbbbda1=vec128#94,<acbd1=vec128#76,<abba0=vec128#78,<aacbbbda1=vec128#94
# asm 2: mpya >aacbbbda1=$96,<acbd1=$78,<abba0=$80,<aacbbbda1=$96
mpya $96,$78,$80,$96

# qhasm: int32323232 aacbbbda2 += (acbd1 & 0xffff) * (abba1 & 0xffff)
# asm 1: mpya >aacbbbda2=vec128#96,<acbd1=vec128#76,<abba1=vec128#79,<aacbbbda2=vec128#96
# asm 2: mpya >aacbbbda2=$98,<acbd1=$78,<abba1=$81,<aacbbbda2=$98
mpya $98,$78,$81,$98

# qhasm: int32323232 aacbbbda3 += (acbd1 & 0xffff) * (abba2 & 0xffff)
# asm 1: mpya >aacbbbda3=vec128#98,<acbd1=vec128#76,<abba2=vec128#77,<aacbbbda3=vec128#98
# asm 2: mpya >aacbbbda3=$100,<acbd1=$78,<abba2=$79,<aacbbbda3=$100
mpya $100,$78,$79,$100

# qhasm: int32323232 aacbbbda4 += (acbd2 & 0xffff) * (abba2 & 0xffff)
# asm 1: mpya >aacbbbda4=vec128#100,<acbd2=vec128#63,<abba2=vec128#77,<aacbbbda4=vec128#100
# asm 2: mpya >aacbbbda4=$102,<acbd2=$65,<abba2=$79,<aacbbbda4=$102
mpya $102,$65,$79,$102

# qhasm: int32323232 aacbbbda5 += (acbd3 & 0xffff) * (abba2 & 0xffff)
# asm 1: mpya >aacbbbda5=vec128#101,<acbd3=vec128#5,<abba2=vec128#77,<aacbbbda5=vec128#101
# asm 2: mpya >aacbbbda5=$103,<acbd3=$7,<abba2=$79,<aacbbbda5=$103
mpya $103,$7,$79,$103

# qhasm: int32323232 aacbbbda6 <<= 1
# asm 1: shli >aacbbbda6=vec128#102,<aacbbbda6=vec128#102,1
# asm 2: shli >aacbbbda6=$104,<aacbbbda6=$104,1
shli $104,$104,1

# qhasm: int32323232 aacbbbda7 += (acbd1 & 0xffff) * (abba6 & 0xffff)
# asm 1: mpya >aacbbbda7=vec128#103,<acbd1=vec128#76,<abba6=vec128#92,<aacbbbda7=vec128#103
# asm 2: mpya >aacbbbda7=$105,<acbd1=$78,<abba6=$94,<aacbbbda7=$105
mpya $105,$78,$94,$105

# qhasm: int32323232 aacbbbda8  = (acbd1 & 0xffff) * (abba7 & 0xffff)
# asm 1: mpy >aacbbbda8=vec128#104,<acbd1=vec128#76,<abba7=vec128#91
# asm 2: mpy >aacbbbda8=$106,<acbd1=$78,<abba7=$93
mpy $106,$78,$93

# qhasm: int32323232 aacbbbda2 += (acbd2 & 0xffff) * (abba0 & 0xffff)
# asm 1: mpya >aacbbbda2=vec128#96,<acbd2=vec128#63,<abba0=vec128#78,<aacbbbda2=vec128#96
# asm 2: mpya >aacbbbda2=$98,<acbd2=$65,<abba0=$80,<aacbbbda2=$98
mpya $98,$65,$80,$98

# qhasm: int32323232 aacbbbda3 += (acbd2 & 0xffff) * (abba1 & 0xffff)
# asm 1: mpya >aacbbbda3=vec128#98,<acbd2=vec128#63,<abba1=vec128#79,<aacbbbda3=vec128#98
# asm 2: mpya >aacbbbda3=$100,<acbd2=$65,<abba1=$81,<aacbbbda3=$100
mpya $100,$65,$81,$100

# qhasm: int32323232 aacbbbda4 += (acbd3 & 0xffff) * (abba1 & 0xffff)
# asm 1: mpya >aacbbbda4=vec128#100,<acbd3=vec128#5,<abba1=vec128#79,<aacbbbda4=vec128#100
# asm 2: mpya >aacbbbda4=$102,<acbd3=$7,<abba1=$81,<aacbbbda4=$102
mpya $102,$7,$81,$102

# qhasm: int32323232 aacbbbda5 <<= 1
# asm 1: shli >aacbbbda5=vec128#101,<aacbbbda5=vec128#101,1
# asm 2: shli >aacbbbda5=$103,<aacbbbda5=$103,1
shli $103,$103,1

# qhasm: int32323232 aacbbbda6 += (acbd0 & 0xffff) * (abba6 & 0xffff)
# asm 1: mpya >aacbbbda6=vec128#102,<acbd0=vec128#70,<abba6=vec128#92,<aacbbbda6=vec128#102
# asm 2: mpya >aacbbbda6=$104,<acbd0=$72,<abba6=$94,<aacbbbda6=$104
mpya $104,$72,$94,$104

# qhasm: int32323232 aacbbbda7 += (acbd2 & 0xffff) * (abba5 & 0xffff)
# asm 1: mpya >aacbbbda7=vec128#103,<acbd2=vec128#63,<abba5=vec128#88,<aacbbbda7=vec128#103
# asm 2: mpya >aacbbbda7=$105,<acbd2=$65,<abba5=$90,<aacbbbda7=$105
mpya $105,$65,$90,$105

# qhasm: int32323232 aacbbbda8 += (acbd2 & 0xffff) * (abba6 & 0xffff)
# asm 1: mpya >aacbbbda8=vec128#104,<acbd2=vec128#63,<abba6=vec128#92,<aacbbbda8=vec128#104
# asm 2: mpya >aacbbbda8=$106,<acbd2=$65,<abba6=$94,<aacbbbda8=$106
mpya $106,$65,$94,$106

# qhasm: int32323232 aacbbbda9  = (acbd2 & 0xffff) * (abba7 & 0xffff)
# asm 1: mpy >aacbbbda9=vec128#105,<acbd2=vec128#63,<abba7=vec128#91
# asm 2: mpy >aacbbbda9=$107,<acbd2=$65,<abba7=$93
mpy $107,$65,$93

# qhasm: int32323232 aacbbbda3 += (acbd3 & 0xffff) * (abba0 & 0xffff)
# asm 1: mpya >aacbbbda3=vec128#98,<acbd3=vec128#5,<abba0=vec128#78,<aacbbbda3=vec128#98
# asm 2: mpya >aacbbbda3=$100,<acbd3=$7,<abba0=$80,<aacbbbda3=$100
mpya $100,$7,$80,$100

# qhasm: int32323232 aacbbbda4 <<= 1 
# asm 1: shli >aacbbbda4=vec128#100,<aacbbbda4=vec128#100,1
# asm 2: shli >aacbbbda4=$102,<aacbbbda4=$102,1
shli $102,$102,1

# qhasm: int32323232 aacbbbda5 += (acbd0 & 0xffff) * (abba5 & 0xffff)
# asm 1: mpya >aacbbbda5=vec128#101,<acbd0=vec128#70,<abba5=vec128#88,<aacbbbda5=vec128#101
# asm 2: mpya >aacbbbda5=$103,<acbd0=$72,<abba5=$90,<aacbbbda5=$103
mpya $103,$72,$90,$103

# qhasm: int32323232 aacbbbda6 += (acbd1 & 0xffff) * (abba5 & 0xffff)
# asm 1: mpya >aacbbbda6=vec128#102,<acbd1=vec128#76,<abba5=vec128#88,<aacbbbda6=vec128#102
# asm 2: mpya >aacbbbda6=$104,<acbd1=$78,<abba5=$90,<aacbbbda6=$104
mpya $104,$78,$90,$104

# qhasm: int32323232 aacbbbda7 += (acbd3 & 0xffff) * (abba4 & 0xffff)
# asm 1: mpya >aacbbbda7=vec128#103,<acbd3=vec128#5,<abba4=vec128#84,<aacbbbda7=vec128#103
# asm 2: mpya >aacbbbda7=$105,<acbd3=$7,<abba4=$86,<aacbbbda7=$105
mpya $105,$7,$86,$105

# qhasm: int32323232 aacbbbda8 += (acbd3 & 0xffff) * (abba5 & 0xffff)
# asm 1: mpya >aacbbbda8=vec128#104,<acbd3=vec128#5,<abba5=vec128#88,<aacbbbda8=vec128#104
# asm 2: mpya >aacbbbda8=$106,<acbd3=$7,<abba5=$90,<aacbbbda8=$106
mpya $106,$7,$90,$106

# qhasm: int32323232 aacbbbda9 += (acbd3 & 0xffff) * (abba6 & 0xffff)
# asm 1: mpya >aacbbbda9=vec128#105,<acbd3=vec128#5,<abba6=vec128#92,<aacbbbda9=vec128#105
# asm 2: mpya >aacbbbda9=$107,<acbd3=$7,<abba6=$94,<aacbbbda9=$107
mpya $107,$7,$94,$107

# qhasm: int32323232 aacbbbda10  = (acbd3  & 0xffff) * (abba7  & 0xffff)
# asm 1: mpy >aacbbbda10=vec128#106,<acbd3=vec128#5,<abba7=vec128#91
# asm 2: mpy >aacbbbda10=$108,<acbd3=$7,<abba7=$93
mpy $108,$7,$93

# qhasm: int32323232 aacbbbda4 += (acbd0 & 0xffff) * (abba4 & 0xffff)
# asm 1: mpya >aacbbbda4=vec128#100,<acbd0=vec128#70,<abba4=vec128#84,<aacbbbda4=vec128#100
# asm 2: mpya >aacbbbda4=$102,<acbd0=$72,<abba4=$86,<aacbbbda4=$102
mpya $102,$72,$86,$102

# qhasm: int32323232 aacbbbda5 += (acbd1 & 0xffff) * (abba4 & 0xffff)
# asm 1: mpya >aacbbbda5=vec128#101,<acbd1=vec128#76,<abba4=vec128#84,<aacbbbda5=vec128#101
# asm 2: mpya >aacbbbda5=$103,<acbd1=$78,<abba4=$86,<aacbbbda5=$103
mpya $103,$78,$86,$103

# qhasm: int32323232 aacbbbda6 += (acbd2 & 0xffff) * (abba4 & 0xffff)
# asm 1: mpya >aacbbbda6=vec128#102,<acbd2=vec128#63,<abba4=vec128#84,<aacbbbda6=vec128#102
# asm 2: mpya >aacbbbda6=$104,<acbd2=$65,<abba4=$86,<aacbbbda6=$104
mpya $104,$65,$86,$104

# qhasm: int32323232 aacbbbda7 += (acbd4 & 0xffff) * (abba3 & 0xffff)
# asm 1: mpya >aacbbbda7=vec128#103,<acbd4=vec128#58,<abba3=vec128#80,<aacbbbda7=vec128#103
# asm 2: mpya >aacbbbda7=$105,<acbd4=$60,<abba3=$82,<aacbbbda7=$105
mpya $105,$60,$82,$105

# qhasm: int32323232 aacbbbda8 += (acbd5 & 0xffff) * (abba3 & 0xffff)
# asm 1: mpya >aacbbbda8=vec128#104,<acbd5=vec128#66,<abba3=vec128#80,<aacbbbda8=vec128#104
# asm 2: mpya >aacbbbda8=$106,<acbd5=$68,<abba3=$82,<aacbbbda8=$106
mpya $106,$68,$82,$106

# qhasm: int32323232 aacbbbda9 += (acbd6 & 0xffff) * (abba3 & 0xffff)
# asm 1: mpya >aacbbbda9=vec128#105,<acbd6=vec128#65,<abba3=vec128#80,<aacbbbda9=vec128#105
# asm 2: mpya >aacbbbda9=$107,<acbd6=$67,<abba3=$82,<aacbbbda9=$107
mpya $107,$67,$82,$107

# qhasm: int32323232 aacbbbda10 += (acbd7  & 0xffff) * (abba3  & 0xffff)
# asm 1: mpya >aacbbbda10=vec128#106,<acbd7=vec128#55,<abba3=vec128#80,<aacbbbda10=vec128#106
# asm 2: mpya >aacbbbda10=$108,<acbd7=$57,<abba3=$82,<aacbbbda10=$108
mpya $108,$57,$82,$108

# qhasm: int32323232 aacbbbda11  = (acbd11 & 0xffff) * (abba0  & 0xffff)
# asm 1: mpy >aacbbbda11=vec128#107,<acbd11=vec128#56,<abba0=vec128#78
# asm 2: mpy >aacbbbda11=$109,<acbd11=$58,<abba0=$80
mpy $109,$58,$80

# qhasm: int32323232 aacbbbda4 += (acbd4 & 0xffff) * (abba0 & 0xffff)
# asm 1: mpya >aacbbbda4=vec128#100,<acbd4=vec128#58,<abba0=vec128#78,<aacbbbda4=vec128#100
# asm 2: mpya >aacbbbda4=$102,<acbd4=$60,<abba0=$80,<aacbbbda4=$102
mpya $102,$60,$80,$102

# qhasm: int32323232 aacbbbda5 += (acbd4 & 0xffff) * (abba1 & 0xffff)
# asm 1: mpya >aacbbbda5=vec128#101,<acbd4=vec128#58,<abba1=vec128#79,<aacbbbda5=vec128#101
# asm 2: mpya >aacbbbda5=$103,<acbd4=$60,<abba1=$81,<aacbbbda5=$103
mpya $103,$60,$81,$103

# qhasm: int32323232 aacbbbda6 += (acbd4 & 0xffff) * (abba2 & 0xffff)
# asm 1: mpya >aacbbbda6=vec128#102,<acbd4=vec128#58,<abba2=vec128#77,<aacbbbda6=vec128#102
# asm 2: mpya >aacbbbda6=$104,<acbd4=$60,<abba2=$79,<aacbbbda6=$104
mpya $104,$60,$79,$104

# qhasm: int32323232 aacbbbda7 += (acbd5 & 0xffff) * (abba2 & 0xffff)
# asm 1: mpya >aacbbbda7=vec128#103,<acbd5=vec128#66,<abba2=vec128#77,<aacbbbda7=vec128#103
# asm 2: mpya >aacbbbda7=$105,<acbd5=$68,<abba2=$79,<aacbbbda7=$105
mpya $105,$68,$79,$105

# qhasm: int32323232 aacbbbda8 += (acbd6 & 0xffff) * (abba2 & 0xffff)
# asm 1: mpya >aacbbbda8=vec128#104,<acbd6=vec128#65,<abba2=vec128#77,<aacbbbda8=vec128#104
# asm 2: mpya >aacbbbda8=$106,<acbd6=$67,<abba2=$79,<aacbbbda8=$106
mpya $106,$67,$79,$106

# qhasm: int32323232 aacbbbda9 += (acbd7 & 0xffff) * (abba2 & 0xffff)
# asm 1: mpya >aacbbbda9=vec128#105,<acbd7=vec128#55,<abba2=vec128#77,<aacbbbda9=vec128#105
# asm 2: mpya >aacbbbda9=$107,<acbd7=$57,<abba2=$79,<aacbbbda9=$107
mpya $107,$57,$79,$107

# qhasm: int32323232 aacbbbda10 <<= 1
# asm 1: shli >aacbbbda10=vec128#106,<aacbbbda10=vec128#106,1
# asm 2: shli >aacbbbda10=$108,<aacbbbda10=$108,1
shli $108,$108,1

# qhasm: int32323232 aacbbbda11 += (acbd10 & 0xffff) * (abba1  & 0xffff)
# asm 1: mpya >aacbbbda11=vec128#107,<acbd10=vec128#67,<abba1=vec128#79,<aacbbbda11=vec128#107
# asm 2: mpya >aacbbbda11=$109,<acbd10=$69,<abba1=$81,<aacbbbda11=$109
mpya $109,$69,$81,$109

# qhasm: int32323232 aacbbbda12  = (acbd1  & 0xffff) * (abba11 & 0xffff)
# asm 1: mpy >aacbbbda12=vec128#108,<acbd1=vec128#76,<abba11=vec128#95
# asm 2: mpy >aacbbbda12=$110,<acbd1=$78,<abba11=$97
mpy $110,$78,$97

# qhasm: int32323232 aacbbbda5 += (acbd5 & 0xffff) * (abba0 & 0xffff)
# asm 1: mpya >aacbbbda5=vec128#101,<acbd5=vec128#66,<abba0=vec128#78,<aacbbbda5=vec128#101
# asm 2: mpya >aacbbbda5=$103,<acbd5=$68,<abba0=$80,<aacbbbda5=$103
mpya $103,$68,$80,$103

# qhasm: int32323232 aacbbbda6 += (acbd5 & 0xffff) * (abba1 & 0xffff)
# asm 1: mpya >aacbbbda6=vec128#102,<acbd5=vec128#66,<abba1=vec128#79,<aacbbbda6=vec128#102
# asm 2: mpya >aacbbbda6=$104,<acbd5=$68,<abba1=$81,<aacbbbda6=$104
mpya $104,$68,$81,$104

# qhasm: int32323232 aacbbbda7 += (acbd6 & 0xffff) * (abba1 & 0xffff)
# asm 1: mpya >aacbbbda7=vec128#103,<acbd6=vec128#65,<abba1=vec128#79,<aacbbbda7=vec128#103
# asm 2: mpya >aacbbbda7=$105,<acbd6=$67,<abba1=$81,<aacbbbda7=$105
mpya $105,$67,$81,$105

# qhasm: int32323232 aacbbbda8 += (acbd7 & 0xffff) * (abba1 & 0xffff)
# asm 1: mpya >aacbbbda8=vec128#104,<acbd7=vec128#55,<abba1=vec128#79,<aacbbbda8=vec128#104
# asm 2: mpya >aacbbbda8=$106,<acbd7=$57,<abba1=$81,<aacbbbda8=$106
mpya $106,$57,$81,$106

# qhasm: int32323232 aacbbbda9 <<= 1
# asm 1: shli >aacbbbda9=vec128#105,<aacbbbda9=vec128#105,1
# asm 2: shli >aacbbbda9=$107,<aacbbbda9=$107,1
shli $107,$107,1

# qhasm: int32323232 aacbbbda10 += (acbd0  & 0xffff) * (abba10 & 0xffff)
# asm 1: mpya >aacbbbda10=vec128#106,<acbd0=vec128#70,<abba10=vec128#93,<aacbbbda10=vec128#106
# asm 2: mpya >aacbbbda10=$108,<acbd0=$72,<abba10=$95,<aacbbbda10=$108
mpya $108,$72,$95,$108

# qhasm: int32323232 aacbbbda11 += (acbd9  & 0xffff) * (abba2  & 0xffff)
# asm 1: mpya >aacbbbda11=vec128#107,<acbd9=vec128#68,<abba2=vec128#77,<aacbbbda11=vec128#107
# asm 2: mpya >aacbbbda11=$109,<acbd9=$70,<abba2=$79,<aacbbbda11=$109
mpya $109,$70,$79,$109

# qhasm: int32323232 aacbbbda12 += (acbd2  & 0xffff) * (abba10 & 0xffff)
# asm 1: mpya >aacbbbda12=vec128#108,<acbd2=vec128#63,<abba10=vec128#93,<aacbbbda12=vec128#108
# asm 2: mpya >aacbbbda12=$110,<acbd2=$65,<abba10=$95,<aacbbbda12=$110
mpya $110,$65,$95,$110

# qhasm: int32323232 aacbbbda6 += (acbd6 & 0xffff) * (abba0 & 0xffff)
# asm 1: mpya >aacbbbda6=vec128#102,<acbd6=vec128#65,<abba0=vec128#78,<aacbbbda6=vec128#102
# asm 2: mpya >aacbbbda6=$104,<acbd6=$67,<abba0=$80,<aacbbbda6=$104
mpya $104,$67,$80,$104

# qhasm: int32323232 aacbbbda7 += (acbd7 & 0xffff) * (abba0 & 0xffff)
# asm 1: mpya >aacbbbda7=vec128#103,<acbd7=vec128#55,<abba0=vec128#78,<aacbbbda7=vec128#103
# asm 2: mpya >aacbbbda7=$105,<acbd7=$57,<abba0=$80,<aacbbbda7=$105
mpya $105,$57,$80,$105

# qhasm: int32323232 aacbbbda8 <<= 1
# asm 1: shli >aacbbbda8=vec128#104,<aacbbbda8=vec128#104,1
# asm 2: shli >aacbbbda8=$106,<aacbbbda8=$106,1
shli $106,$106,1

# qhasm: int32323232 aacbbbda9 += (acbd0 & 0xffff) * (abba9 & 0xffff)
# asm 1: mpya >aacbbbda9=vec128#105,<acbd0=vec128#70,<abba9=vec128#89,<aacbbbda9=vec128#105
# asm 2: mpya >aacbbbda9=$107,<acbd0=$72,<abba9=$91,<aacbbbda9=$107
mpya $107,$72,$91,$107

# qhasm: int32323232 aacbbbda10 += (acbd1  & 0xffff) * (abba9  & 0xffff)
# asm 1: mpya >aacbbbda10=vec128#106,<acbd1=vec128#76,<abba9=vec128#89,<aacbbbda10=vec128#106
# asm 2: mpya >aacbbbda10=$108,<acbd1=$78,<abba9=$91,<aacbbbda10=$108
mpya $108,$78,$91,$108

# qhasm: int32323232 aacbbbda11 += (acbd8  & 0xffff) * (abba3  & 0xffff)
# asm 1: mpya >aacbbbda11=vec128#107,<acbd8=vec128#61,<abba3=vec128#80,<aacbbbda11=vec128#107
# asm 2: mpya >aacbbbda11=$109,<acbd8=$63,<abba3=$82,<aacbbbda11=$109
mpya $109,$63,$82,$109

# qhasm: int32323232 aacbbbda12 += (acbd3  & 0xffff) * (abba9  & 0xffff)
# asm 1: mpya >aacbbbda12=vec128#108,<acbd3=vec128#5,<abba9=vec128#89,<aacbbbda12=vec128#108
# asm 2: mpya >aacbbbda12=$110,<acbd3=$7,<abba9=$91,<aacbbbda12=$110
mpya $110,$7,$91,$110

# qhasm: int32323232 aacbbbda13  = (acbd2  & 0xffff) * (abba11 & 0xffff)
# asm 1: mpy >aacbbbda13=vec128#109,<acbd2=vec128#63,<abba11=vec128#95
# asm 2: mpy >aacbbbda13=$111,<acbd2=$65,<abba11=$97
mpy $111,$65,$97

# qhasm: int32323232 aacbbbda14  = (acbd3  & 0xffff) * (abba11 & 0xffff)
# asm 1: mpy >aacbbbda14=vec128#110,<acbd3=vec128#5,<abba11=vec128#95
# asm 2: mpy >aacbbbda14=$112,<acbd3=$7,<abba11=$97
mpy $112,$7,$97

# qhasm: int32323232 aacbbbda8 += (acbd0 & 0xffff) * (abba8 & 0xffff)
# asm 1: mpya >aacbbbda8=vec128#104,<acbd0=vec128#70,<abba8=vec128#85,<aacbbbda8=vec128#104
# asm 2: mpya >aacbbbda8=$106,<acbd0=$72,<abba8=$87,<aacbbbda8=$106
mpya $106,$72,$87,$106

# qhasm: int32323232 aacbbbda9 += (acbd1 & 0xffff) * (abba8 & 0xffff)
# asm 1: mpya >aacbbbda9=vec128#105,<acbd1=vec128#76,<abba8=vec128#85,<aacbbbda9=vec128#105
# asm 2: mpya >aacbbbda9=$107,<acbd1=$78,<abba8=$87,<aacbbbda9=$107
mpya $107,$78,$87,$107

# qhasm: int32323232 aacbbbda10 += (acbd2  & 0xffff) * (abba8  & 0xffff)
# asm 1: mpya >aacbbbda10=vec128#106,<acbd2=vec128#63,<abba8=vec128#85,<aacbbbda10=vec128#106
# asm 2: mpya >aacbbbda10=$108,<acbd2=$65,<abba8=$87,<aacbbbda10=$108
mpya $108,$65,$87,$108

# qhasm: int32323232 aacbbbda11 += (acbd7  & 0xffff) * (abba4  & 0xffff)
# asm 1: mpya >aacbbbda11=vec128#107,<acbd7=vec128#55,<abba4=vec128#84,<aacbbbda11=vec128#107
# asm 2: mpya >aacbbbda11=$109,<acbd7=$57,<abba4=$86,<aacbbbda11=$109
mpya $109,$57,$86,$109

# qhasm: int32323232 aacbbbda12 += (acbd5  & 0xffff) * (abba7  & 0xffff)
# asm 1: mpya >aacbbbda12=vec128#108,<acbd5=vec128#66,<abba7=vec128#91,<aacbbbda12=vec128#108
# asm 2: mpya >aacbbbda12=$110,<acbd5=$68,<abba7=$93,<aacbbbda12=$110
mpya $110,$68,$93,$110

# qhasm: int32323232 aacbbbda13 += (acbd3  & 0xffff) * (abba10 & 0xffff)
# asm 1: mpya >aacbbbda13=vec128#109,<acbd3=vec128#5,<abba10=vec128#93,<aacbbbda13=vec128#109
# asm 2: mpya >aacbbbda13=$111,<acbd3=$7,<abba10=$95,<aacbbbda13=$111
mpya $111,$7,$95,$111

# qhasm: int32323232 aacbbbda14 += (acbd7  & 0xffff) * (abba7  & 0xffff)
# asm 1: mpya >aacbbbda14=vec128#110,<acbd7=vec128#55,<abba7=vec128#91,<aacbbbda14=vec128#110
# asm 2: mpya >aacbbbda14=$112,<acbd7=$57,<abba7=$93,<aacbbbda14=$112
mpya $112,$57,$93,$112

# qhasm: int32323232 aacbbbda8 += (acbd4 & 0xffff) * (abba4 & 0xffff)
# asm 1: mpya >aacbbbda8=vec128#104,<acbd4=vec128#58,<abba4=vec128#84,<aacbbbda8=vec128#104
# asm 2: mpya >aacbbbda8=$106,<acbd4=$60,<abba4=$86,<aacbbbda8=$106
mpya $106,$60,$86,$106

# qhasm: int32323232 aacbbbda9 += (acbd4 & 0xffff) * (abba5 & 0xffff)
# asm 1: mpya >aacbbbda9=vec128#105,<acbd4=vec128#58,<abba5=vec128#88,<aacbbbda9=vec128#105
# asm 2: mpya >aacbbbda9=$107,<acbd4=$60,<abba5=$90,<aacbbbda9=$107
mpya $107,$60,$90,$107

# qhasm: int32323232 aacbbbda10 += (acbd4  & 0xffff) * (abba6  & 0xffff)
# asm 1: mpya >aacbbbda10=vec128#106,<acbd4=vec128#58,<abba6=vec128#92,<aacbbbda10=vec128#106
# asm 2: mpya >aacbbbda10=$108,<acbd4=$60,<abba6=$94,<aacbbbda10=$108
mpya $108,$60,$94,$108

# qhasm: int32323232 aacbbbda11 += (acbd6  & 0xffff) * (abba5  & 0xffff)
# asm 1: mpya >aacbbbda11=vec128#107,<acbd6=vec128#65,<abba5=vec128#88,<aacbbbda11=vec128#107
# asm 2: mpya >aacbbbda11=$109,<acbd6=$67,<abba5=$90,<aacbbbda11=$109
mpya $109,$67,$90,$109

# qhasm: int32323232 aacbbbda12 += (acbd6  & 0xffff) * (abba6  & 0xffff)
# asm 1: mpya >aacbbbda12=vec128#108,<acbd6=vec128#65,<abba6=vec128#92,<aacbbbda12=vec128#108
# asm 2: mpya >aacbbbda12=$110,<acbd6=$67,<abba6=$94,<aacbbbda12=$110
mpya $110,$67,$94,$110

# qhasm: int32323232 aacbbbda13 += (acbd6  & 0xffff) * (abba7  & 0xffff)
# asm 1: mpya >aacbbbda13=vec128#109,<acbd6=vec128#65,<abba7=vec128#91,<aacbbbda13=vec128#109
# asm 2: mpya >aacbbbda13=$111,<acbd6=$67,<abba7=$93,<aacbbbda13=$111
mpya $111,$67,$93,$111

# qhasm: int32323232 aacbbbda14 += (acbd11 & 0xffff) * (abba3  & 0xffff)
# asm 1: mpya >aacbbbda14=vec128#110,<acbd11=vec128#56,<abba3=vec128#80,<aacbbbda14=vec128#110
# asm 2: mpya >aacbbbda14=$112,<acbd11=$58,<abba3=$82,<aacbbbda14=$112
mpya $112,$58,$82,$112

# qhasm: int32323232 aacbbbda8 += (acbd8 & 0xffff) * (abba0 & 0xffff)
# asm 1: mpya >aacbbbda8=vec128#104,<acbd8=vec128#61,<abba0=vec128#78,<aacbbbda8=vec128#104
# asm 2: mpya >aacbbbda8=$106,<acbd8=$63,<abba0=$80,<aacbbbda8=$106
mpya $106,$63,$80,$106

# qhasm: int32323232 aacbbbda9 += (acbd5 & 0xffff) * (abba4 & 0xffff)
# asm 1: mpya >aacbbbda9=vec128#105,<acbd5=vec128#66,<abba4=vec128#84,<aacbbbda9=vec128#105
# asm 2: mpya >aacbbbda9=$107,<acbd5=$68,<abba4=$86,<aacbbbda9=$107
mpya $107,$68,$86,$107

# qhasm: int32323232 aacbbbda10 += (acbd5  & 0xffff) * (abba5  & 0xffff)
# asm 1: mpya >aacbbbda10=vec128#106,<acbd5=vec128#66,<abba5=vec128#88,<aacbbbda10=vec128#106
# asm 2: mpya >aacbbbda10=$108,<acbd5=$68,<abba5=$90,<aacbbbda10=$108
mpya $108,$68,$90,$108

# qhasm: int32323232 aacbbbda11 += (acbd5  & 0xffff) * (abba6  & 0xffff)
# asm 1: mpya >aacbbbda11=vec128#107,<acbd5=vec128#66,<abba6=vec128#92,<aacbbbda11=vec128#107
# asm 2: mpya >aacbbbda11=$109,<acbd5=$68,<abba6=$94,<aacbbbda11=$109
mpya $109,$68,$94,$109

# qhasm: int32323232 aacbbbda12 += (acbd7  & 0xffff) * (abba5  & 0xffff)
# asm 1: mpya >aacbbbda12=vec128#108,<acbd7=vec128#55,<abba5=vec128#88,<aacbbbda12=vec128#108
# asm 2: mpya >aacbbbda12=$110,<acbd7=$57,<abba5=$90,<aacbbbda12=$110
mpya $110,$57,$90,$110

# qhasm: int32323232 aacbbbda13 += (acbd7  & 0xffff) * (abba6  & 0xffff)
# asm 1: mpya >aacbbbda13=vec128#109,<acbd7=vec128#55,<abba6=vec128#92,<aacbbbda13=vec128#109
# asm 2: mpya >aacbbbda13=$111,<acbd7=$57,<abba6=$94,<aacbbbda13=$111
mpya $111,$57,$94,$111

# qhasm: int32323232 aacbbbda14 <<= 1
# asm 1: shli >aacbbbda14=vec128#110,<aacbbbda14=vec128#110,1
# asm 2: shli >aacbbbda14=$112,<aacbbbda14=$112,1
shli $112,$112,1

# qhasm: int32323232 aacbbbda15  = (acbd0  & 0xffff) * (abba15 & 0xffff)
# asm 1: mpy >aacbbbda15=vec128#111,<acbd0=vec128#70,<abba15=vec128#97
# asm 2: mpy >aacbbbda15=$113,<acbd0=$72,<abba15=$99
mpy $113,$72,$99

# qhasm: int32323232 aacbbbda9 += (acbd8 & 0xffff) * (abba1 & 0xffff)
# asm 1: mpya >aacbbbda9=vec128#105,<acbd8=vec128#61,<abba1=vec128#79,<aacbbbda9=vec128#105
# asm 2: mpya >aacbbbda9=$107,<acbd8=$63,<abba1=$81,<aacbbbda9=$107
mpya $107,$63,$81,$107

# qhasm: int32323232 aacbbbda10 += (acbd6  & 0xffff) * (abba4  & 0xffff)
# asm 1: mpya >aacbbbda10=vec128#106,<acbd6=vec128#65,<abba4=vec128#84,<aacbbbda10=vec128#106
# asm 2: mpya >aacbbbda10=$108,<acbd6=$67,<abba4=$86,<aacbbbda10=$108
mpya $108,$67,$86,$108

# qhasm: int32323232 aacbbbda11 += (acbd4  & 0xffff) * (abba7  & 0xffff)
# asm 1: mpya >aacbbbda11=vec128#107,<acbd4=vec128#58,<abba7=vec128#91,<aacbbbda11=vec128#107
# asm 2: mpya >aacbbbda11=$109,<acbd4=$60,<abba7=$93,<aacbbbda11=$109
mpya $109,$60,$93,$109

# qhasm: int32323232 aacbbbda12 += (acbd9  & 0xffff) * (abba3  & 0xffff)
# asm 1: mpya >aacbbbda12=vec128#108,<acbd9=vec128#68,<abba3=vec128#80,<aacbbbda12=vec128#108
# asm 2: mpya >aacbbbda12=$110,<acbd9=$70,<abba3=$82,<aacbbbda12=$110
mpya $110,$70,$82,$110

# qhasm: int32323232 aacbbbda13 += (acbd10 & 0xffff) * (abba3  & 0xffff)
# asm 1: mpya >aacbbbda13=vec128#109,<acbd10=vec128#67,<abba3=vec128#80,<aacbbbda13=vec128#109
# asm 2: mpya >aacbbbda13=$111,<acbd10=$69,<abba3=$82,<aacbbbda13=$111
mpya $111,$69,$82,$111

# qhasm: int32323232 aacbbbda14 += (acbd0  & 0xffff) * (abba14 & 0xffff)
# asm 1: mpya >aacbbbda14=vec128#110,<acbd0=vec128#70,<abba14=vec128#86,<aacbbbda14=vec128#110
# asm 2: mpya >aacbbbda14=$112,<acbd0=$72,<abba14=$88,<aacbbbda14=$112
mpya $112,$72,$88,$112

# qhasm: int32323232 aacbbbda15 += (acbd1  & 0xffff) * (abba14 & 0xffff)
# asm 1: mpya >aacbbbda15=vec128#111,<acbd1=vec128#76,<abba14=vec128#86,<aacbbbda15=vec128#111
# asm 2: mpya >aacbbbda15=$113,<acbd1=$78,<abba14=$88,<aacbbbda15=$113
mpya $113,$78,$88,$113

# qhasm: int32323232 aacbbbda9 += (acbd9 & 0xffff) * (abba0 & 0xffff)
# asm 1: mpya >aacbbbda9=vec128#105,<acbd9=vec128#68,<abba0=vec128#78,<aacbbbda9=vec128#105
# asm 2: mpya >aacbbbda9=$107,<acbd9=$70,<abba0=$80,<aacbbbda9=$107
mpya $107,$70,$80,$107

# qhasm: int32323232 aacbbbda10 += (acbd8  & 0xffff) * (abba2  & 0xffff)
# asm 1: mpya >aacbbbda10=vec128#106,<acbd8=vec128#61,<abba2=vec128#77,<aacbbbda10=vec128#106
# asm 2: mpya >aacbbbda10=$108,<acbd8=$63,<abba2=$79,<aacbbbda10=$108
mpya $108,$63,$79,$108

# qhasm: int32323232 aacbbbda11 += (acbd3  & 0xffff) * (abba8  & 0xffff)
# asm 1: mpya >aacbbbda11=vec128#107,<acbd3=vec128#5,<abba8=vec128#85,<aacbbbda11=vec128#107
# asm 2: mpya >aacbbbda11=$109,<acbd3=$7,<abba8=$87,<aacbbbda11=$109
mpya $109,$7,$87,$109

# qhasm: int32323232 aacbbbda12 += (acbd10 & 0xffff) * (abba2  & 0xffff)
# asm 1: mpya >aacbbbda12=vec128#108,<acbd10=vec128#67,<abba2=vec128#77,<aacbbbda12=vec128#108
# asm 2: mpya >aacbbbda12=$110,<acbd10=$69,<abba2=$79,<aacbbbda12=$110
mpya $110,$69,$79,$110

# qhasm: int32323232 aacbbbda13 += (acbd11 & 0xffff) * (abba2  & 0xffff)
# asm 1: mpya >aacbbbda13=vec128#109,<acbd11=vec128#56,<abba2=vec128#77,<aacbbbda13=vec128#109
# asm 2: mpya >aacbbbda13=$111,<acbd11=$58,<abba2=$79,<aacbbbda13=$111
mpya $111,$58,$79,$111

# qhasm: int32323232 aacbbbda14 += (acbd1  & 0xffff) * (abba13 & 0xffff)
# asm 1: mpya >aacbbbda14=vec128#110,<acbd1=vec128#76,<abba13=vec128#82,<aacbbbda14=vec128#110
# asm 2: mpya >aacbbbda14=$112,<acbd1=$78,<abba13=$84,<aacbbbda14=$112
mpya $112,$78,$84,$112

# qhasm: int32323232 aacbbbda15 += (acbd2  & 0xffff) * (abba13 & 0xffff)
# asm 1: mpya >aacbbbda15=vec128#111,<acbd2=vec128#63,<abba13=vec128#82,<aacbbbda15=vec128#111
# asm 2: mpya >aacbbbda15=$113,<acbd2=$65,<abba13=$84,<aacbbbda15=$113
mpya $113,$65,$84,$113

# qhasm: int32323232 aacbbbda16  = (acbd1  & 0xffff) * (abba15 & 0xffff)
# asm 1: mpy >aacbbbda16=vec128#112,<acbd1=vec128#76,<abba15=vec128#97
# asm 2: mpy >aacbbbda16=$114,<acbd1=$78,<abba15=$99
mpy $114,$78,$99

# qhasm: int32323232 aacbbbda10 += (acbd9  & 0xffff) * (abba1  & 0xffff)
# asm 1: mpya >aacbbbda10=vec128#106,<acbd9=vec128#68,<abba1=vec128#79,<aacbbbda10=vec128#106
# asm 2: mpya >aacbbbda10=$108,<acbd9=$70,<abba1=$81,<aacbbbda10=$108
mpya $108,$70,$81,$108

# qhasm: int32323232 aacbbbda11 += (acbd2  & 0xffff) * (abba9  & 0xffff)
# asm 1: mpya >aacbbbda11=vec128#107,<acbd2=vec128#63,<abba9=vec128#89,<aacbbbda11=vec128#107
# asm 2: mpya >aacbbbda11=$109,<acbd2=$65,<abba9=$91,<aacbbbda11=$109
mpya $109,$65,$91,$109

# qhasm: int32323232 aacbbbda12 += (acbd11 & 0xffff) * (abba1  & 0xffff)
# asm 1: mpya >aacbbbda12=vec128#108,<acbd11=vec128#56,<abba1=vec128#79,<aacbbbda12=vec128#108
# asm 2: mpya >aacbbbda12=$110,<acbd11=$58,<abba1=$81,<aacbbbda12=$110
mpya $110,$58,$81,$110

# qhasm: int32323232 aacbbbda13 <<= 1
# asm 1: shli >aacbbbda13=vec128#109,<aacbbbda13=vec128#109,1
# asm 2: shli >aacbbbda13=$111,<aacbbbda13=$111,1
shli $111,$111,1

# qhasm: int32323232 aacbbbda14 += (acbd2  & 0xffff) * (abba12 & 0xffff)
# asm 1: mpya >aacbbbda14=vec128#110,<acbd2=vec128#63,<abba12=vec128#71,<aacbbbda14=vec128#110
# asm 2: mpya >aacbbbda14=$112,<acbd2=$65,<abba12=$73,<aacbbbda14=$112
mpya $112,$65,$73,$112

# qhasm: int32323232 aacbbbda15 += (acbd3  & 0xffff) * (abba12 & 0xffff)
# asm 1: mpya >aacbbbda15=vec128#111,<acbd3=vec128#5,<abba12=vec128#71,<aacbbbda15=vec128#111
# asm 2: mpya >aacbbbda15=$113,<acbd3=$7,<abba12=$73,<aacbbbda15=$113
mpya $113,$7,$73,$113

# qhasm: int32323232 aacbbbda16 += (acbd2  & 0xffff) * (abba14 & 0xffff)
# asm 1: mpya >aacbbbda16=vec128#112,<acbd2=vec128#63,<abba14=vec128#86,<aacbbbda16=vec128#112
# asm 2: mpya >aacbbbda16=$114,<acbd2=$65,<abba14=$88,<aacbbbda16=$114
mpya $114,$65,$88,$114

# qhasm: int32323232 aacbbbda10 += (acbd10 & 0xffff) * (abba0  & 0xffff)
# asm 1: mpya >aacbbbda10=vec128#106,<acbd10=vec128#67,<abba0=vec128#78,<aacbbbda10=vec128#106
# asm 2: mpya >aacbbbda10=$108,<acbd10=$69,<abba0=$80,<aacbbbda10=$108
mpya $108,$69,$80,$108

# qhasm: int32323232 aacbbbda11 += (acbd1  & 0xffff) * (abba10 & 0xffff)
# asm 1: mpya >aacbbbda11=vec128#107,<acbd1=vec128#76,<abba10=vec128#93,<aacbbbda11=vec128#107
# asm 2: mpya >aacbbbda11=$109,<acbd1=$78,<abba10=$95,<aacbbbda11=$109
mpya $109,$78,$95,$109

# qhasm: int32323232 aacbbbda12 <<= 1
# asm 1: shli >aacbbbda12=vec128#108,<aacbbbda12=vec128#108,1
# asm 2: shli >aacbbbda12=$110,<aacbbbda12=$110,1
shli $110,$110,1

# qhasm: int32323232 aacbbbda13 += (acbd0  & 0xffff) * (abba13 & 0xffff)
# asm 1: mpya >aacbbbda13=vec128#109,<acbd0=vec128#70,<abba13=vec128#82,<aacbbbda13=vec128#109
# asm 2: mpya >aacbbbda13=$111,<acbd0=$72,<abba13=$84,<aacbbbda13=$111
mpya $111,$72,$84,$111

# qhasm: int32323232 aacbbbda14 += (acbd4  & 0xffff) * (abba10 & 0xffff)
# asm 1: mpya >aacbbbda14=vec128#110,<acbd4=vec128#58,<abba10=vec128#93,<aacbbbda14=vec128#110
# asm 2: mpya >aacbbbda14=$112,<acbd4=$60,<abba10=$95,<aacbbbda14=$112
mpya $112,$60,$95,$112

# qhasm: int32323232 aacbbbda15 += (acbd4  & 0xffff) * (abba11 & 0xffff)
# asm 1: mpya >aacbbbda15=vec128#111,<acbd4=vec128#58,<abba11=vec128#95,<aacbbbda15=vec128#111
# asm 2: mpya >aacbbbda15=$113,<acbd4=$60,<abba11=$97,<aacbbbda15=$113
mpya $113,$60,$97,$113

# qhasm: int32323232 aacbbbda16 += (acbd3  & 0xffff) * (abba13 & 0xffff)
# asm 1: mpya >aacbbbda16=vec128#112,<acbd3=vec128#5,<abba13=vec128#82,<aacbbbda16=vec128#112
# asm 2: mpya >aacbbbda16=$114,<acbd3=$7,<abba13=$84,<aacbbbda16=$114
mpya $114,$7,$84,$114

# qhasm: int32323232 aacbbbda17  = (acbd2  & 0xffff) * (abba15 & 0xffff)
# asm 1: mpy >aacbbbda17=vec128#113,<acbd2=vec128#63,<abba15=vec128#97
# asm 2: mpy >aacbbbda17=$115,<acbd2=$65,<abba15=$99
mpy $115,$65,$99

# qhasm: int32323232 aacbbbda11 += (acbd0  & 0xffff) * (abba11 & 0xffff)
# asm 1: mpya >aacbbbda11=vec128#107,<acbd0=vec128#70,<abba11=vec128#95,<aacbbbda11=vec128#107
# asm 2: mpya >aacbbbda11=$109,<acbd0=$72,<abba11=$97,<aacbbbda11=$109
mpya $109,$72,$97,$109

# qhasm: int32323232 aacbbbda12 += (acbd0  & 0xffff) * (abba12 & 0xffff)
# asm 1: mpya >aacbbbda12=vec128#108,<acbd0=vec128#70,<abba12=vec128#71,<aacbbbda12=vec128#108
# asm 2: mpya >aacbbbda12=$110,<acbd0=$72,<abba12=$73,<aacbbbda12=$110
mpya $110,$72,$73,$110

# qhasm: int32323232 aacbbbda13 += (acbd1  & 0xffff) * (abba12 & 0xffff)
# asm 1: mpya >aacbbbda13=vec128#109,<acbd1=vec128#76,<abba12=vec128#71,<aacbbbda13=vec128#109
# asm 2: mpya >aacbbbda13=$111,<acbd1=$78,<abba12=$73,<aacbbbda13=$111
mpya $111,$78,$73,$111

# qhasm: int32323232 aacbbbda14 += (acbd5  & 0xffff) * (abba9  & 0xffff)
# asm 1: mpya >aacbbbda14=vec128#110,<acbd5=vec128#66,<abba9=vec128#89,<aacbbbda14=vec128#110
# asm 2: mpya >aacbbbda14=$112,<acbd5=$68,<abba9=$91,<aacbbbda14=$112
mpya $112,$68,$91,$112

# qhasm: int32323232 aacbbbda15 += (acbd5  & 0xffff) * (abba10 & 0xffff)
# asm 1: mpya >aacbbbda15=vec128#111,<acbd5=vec128#66,<abba10=vec128#93,<aacbbbda15=vec128#111
# asm 2: mpya >aacbbbda15=$113,<acbd5=$68,<abba10=$95,<aacbbbda15=$113
mpya $113,$68,$95,$113

# qhasm: int32323232 aacbbbda16 += (acbd5  & 0xffff) * (abba11 & 0xffff)
# asm 1: mpya >aacbbbda16=vec128#112,<acbd5=vec128#66,<abba11=vec128#95,<aacbbbda16=vec128#112
# asm 2: mpya >aacbbbda16=$114,<acbd5=$68,<abba11=$97,<aacbbbda16=$114
mpya $114,$68,$97,$114

# qhasm: int32323232 aacbbbda17 += (acbd3  & 0xffff) * (abba14 & 0xffff)
# asm 1: mpya >aacbbbda17=vec128#113,<acbd3=vec128#5,<abba14=vec128#86,<aacbbbda17=vec128#113
# asm 2: mpya >aacbbbda17=$115,<acbd3=$7,<abba14=$88,<aacbbbda17=$115
mpya $115,$7,$88,$115

# qhasm: int32323232 aacbbbda18  = (acbd3  & 0xffff) * (abba15 & 0xffff)
# asm 1: mpy >aacbbbda18=vec128#114,<acbd3=vec128#5,<abba15=vec128#97
# asm 2: mpy >aacbbbda18=$116,<acbd3=$7,<abba15=$99
mpy $116,$7,$99

# qhasm: int32323232 aacbbbda12 += (acbd4  & 0xffff) * (abba8  & 0xffff)
# asm 1: mpya >aacbbbda12=vec128#108,<acbd4=vec128#58,<abba8=vec128#85,<aacbbbda12=vec128#108
# asm 2: mpya >aacbbbda12=$110,<acbd4=$60,<abba8=$87,<aacbbbda12=$110
mpya $110,$60,$87,$110

# qhasm: int32323232 aacbbbda13 += (acbd4  & 0xffff) * (abba9  & 0xffff)
# asm 1: mpya >aacbbbda13=vec128#109,<acbd4=vec128#58,<abba9=vec128#89,<aacbbbda13=vec128#109
# asm 2: mpya >aacbbbda13=$111,<acbd4=$60,<abba9=$91,<aacbbbda13=$111
mpya $111,$60,$91,$111

# qhasm: int32323232 aacbbbda14 += (acbd6  & 0xffff) * (abba8  & 0xffff)
# asm 1: mpya >aacbbbda14=vec128#110,<acbd6=vec128#65,<abba8=vec128#85,<aacbbbda14=vec128#110
# asm 2: mpya >aacbbbda14=$112,<acbd6=$67,<abba8=$87,<aacbbbda14=$112
mpya $112,$67,$87,$112

# qhasm: int32323232 aacbbbda15 += (acbd6  & 0xffff) * (abba9  & 0xffff)
# asm 1: mpya >aacbbbda15=vec128#111,<acbd6=vec128#65,<abba9=vec128#89,<aacbbbda15=vec128#111
# asm 2: mpya >aacbbbda15=$113,<acbd6=$67,<abba9=$91,<aacbbbda15=$113
mpya $113,$67,$91,$113

# qhasm: int32323232 aacbbbda16 += (acbd6  & 0xffff) * (abba10 & 0xffff)
# asm 1: mpya >aacbbbda16=vec128#112,<acbd6=vec128#65,<abba10=vec128#93,<aacbbbda16=vec128#112
# asm 2: mpya >aacbbbda16=$114,<acbd6=$67,<abba10=$95,<aacbbbda16=$114
mpya $114,$67,$95,$114

# qhasm: int32323232 aacbbbda17 += (acbd6  & 0xffff) * (abba11 & 0xffff)
# asm 1: mpya >aacbbbda17=vec128#113,<acbd6=vec128#65,<abba11=vec128#95,<aacbbbda17=vec128#113
# asm 2: mpya >aacbbbda17=$115,<acbd6=$67,<abba11=$97,<aacbbbda17=$115
mpya $115,$67,$97,$115

# qhasm: int32323232 aacbbbda18 += (acbd7  & 0xffff) * (abba11 & 0xffff)
# asm 1: mpya >aacbbbda18=vec128#114,<acbd7=vec128#55,<abba11=vec128#95,<aacbbbda18=vec128#114
# asm 2: mpya >aacbbbda18=$116,<acbd7=$57,<abba11=$97,<aacbbbda18=$116
mpya $116,$57,$97,$116

# qhasm: int32323232 aacbbbda12 += (acbd8  & 0xffff) * (abba4  & 0xffff)
# asm 1: mpya >aacbbbda12=vec128#108,<acbd8=vec128#61,<abba4=vec128#84,<aacbbbda12=vec128#108
# asm 2: mpya >aacbbbda12=$110,<acbd8=$63,<abba4=$86,<aacbbbda12=$110
mpya $110,$63,$86,$110

# qhasm: int32323232 aacbbbda13 += (acbd5  & 0xffff) * (abba8  & 0xffff)
# asm 1: mpya >aacbbbda13=vec128#109,<acbd5=vec128#66,<abba8=vec128#85,<aacbbbda13=vec128#109
# asm 2: mpya >aacbbbda13=$111,<acbd5=$68,<abba8=$87,<aacbbbda13=$111
mpya $111,$68,$87,$111

# qhasm: int32323232 aacbbbda14 += (acbd8  & 0xffff) * (abba6  & 0xffff)
# asm 1: mpya >aacbbbda14=vec128#110,<acbd8=vec128#61,<abba6=vec128#92,<aacbbbda14=vec128#110
# asm 2: mpya >aacbbbda14=$112,<acbd8=$63,<abba6=$94,<aacbbbda14=$112
mpya $112,$63,$94,$112

# qhasm: int32323232 aacbbbda15 += (acbd7  & 0xffff) * (abba8  & 0xffff)
# asm 1: mpya >aacbbbda15=vec128#111,<acbd7=vec128#55,<abba8=vec128#85,<aacbbbda15=vec128#111
# asm 2: mpya >aacbbbda15=$113,<acbd7=$57,<abba8=$87,<aacbbbda15=$113
mpya $113,$57,$87,$113

# qhasm: int32323232 aacbbbda16 += (acbd7  & 0xffff) * (abba9  & 0xffff)
# asm 1: mpya >aacbbbda16=vec128#112,<acbd7=vec128#55,<abba9=vec128#89,<aacbbbda16=vec128#112
# asm 2: mpya >aacbbbda16=$114,<acbd7=$57,<abba9=$91,<aacbbbda16=$114
mpya $114,$57,$91,$114

# qhasm: int32323232 aacbbbda17 += (acbd7  & 0xffff) * (abba10 & 0xffff)
# asm 1: mpya >aacbbbda17=vec128#113,<acbd7=vec128#55,<abba10=vec128#93,<aacbbbda17=vec128#113
# asm 2: mpya >aacbbbda17=$115,<acbd7=$57,<abba10=$95,<aacbbbda17=$115
mpya $115,$57,$95,$115

# qhasm: int32323232 aacbbbda18 += (acbd11 & 0xffff) * (abba7  & 0xffff)
# asm 1: mpya >aacbbbda18=vec128#114,<acbd11=vec128#56,<abba7=vec128#91,<aacbbbda18=vec128#114
# asm 2: mpya >aacbbbda18=$116,<acbd11=$58,<abba7=$93,<aacbbbda18=$116
mpya $116,$58,$93,$116

# qhasm: int32323232 aacbbbda12 += (acbd12 & 0xffff) * (abba0  & 0xffff)
# asm 1: mpya >aacbbbda12=vec128#108,<acbd12=vec128#62,<abba0=vec128#78,<aacbbbda12=vec128#108
# asm 2: mpya >aacbbbda12=$110,<acbd12=$64,<abba0=$80,<aacbbbda12=$110
mpya $110,$64,$80,$110

# qhasm: int32323232 aacbbbda13 += (acbd8  & 0xffff) * (abba5  & 0xffff)
# asm 1: mpya >aacbbbda13=vec128#109,<acbd8=vec128#61,<abba5=vec128#88,<aacbbbda13=vec128#109
# asm 2: mpya >aacbbbda13=$111,<acbd8=$63,<abba5=$90,<aacbbbda13=$111
mpya $111,$63,$90,$111

# qhasm: int32323232 aacbbbda14 += (acbd9  & 0xffff) * (abba5  & 0xffff)
# asm 1: mpya >aacbbbda14=vec128#110,<acbd9=vec128#68,<abba5=vec128#88,<aacbbbda14=vec128#110
# asm 2: mpya >aacbbbda14=$112,<acbd9=$70,<abba5=$90,<aacbbbda14=$112
mpya $112,$70,$90,$112

# qhasm: int32323232 aacbbbda15 += (acbd8  & 0xffff) * (abba7  & 0xffff)
# asm 1: mpya >aacbbbda15=vec128#111,<acbd8=vec128#61,<abba7=vec128#91,<aacbbbda15=vec128#111
# asm 2: mpya >aacbbbda15=$113,<acbd8=$63,<abba7=$93,<aacbbbda15=$113
mpya $113,$63,$93,$113

# qhasm: int32323232 aacbbbda16 += (acbd9  & 0xffff) * (abba7  & 0xffff)
# asm 1: mpya >aacbbbda16=vec128#112,<acbd9=vec128#68,<abba7=vec128#91,<aacbbbda16=vec128#112
# asm 2: mpya >aacbbbda16=$114,<acbd9=$70,<abba7=$93,<aacbbbda16=$114
mpya $114,$70,$93,$114

# qhasm: int32323232 aacbbbda17 += (acbd10 & 0xffff) * (abba7  & 0xffff)
# asm 1: mpya >aacbbbda17=vec128#113,<acbd10=vec128#67,<abba7=vec128#91,<aacbbbda17=vec128#113
# asm 2: mpya >aacbbbda17=$115,<acbd10=$69,<abba7=$93,<aacbbbda17=$115
mpya $115,$69,$93,$115

# qhasm: int32323232 aacbbbda18 += (acbd15 & 0xffff) * (abba3  & 0xffff)
# asm 1: mpya >aacbbbda18=vec128#114,<acbd15=vec128#57,<abba3=vec128#80,<aacbbbda18=vec128#114
# asm 2: mpya >aacbbbda18=$116,<acbd15=$59,<abba3=$82,<aacbbbda18=$116
mpya $116,$59,$82,$116

# qhasm: int32323232 aacbbbda19  = (acbd0  & 0xffff) * (abba19 & 0xffff)
# asm 1: mpy >aacbbbda19=vec128#115,<acbd0=vec128#70,<abba19=vec128#99
# asm 2: mpy >aacbbbda19=$117,<acbd0=$72,<abba19=$101
mpy $117,$72,$101

# qhasm: int32323232 aacbbbda13 += (acbd9  & 0xffff) * (abba4  & 0xffff)
# asm 1: mpya >aacbbbda13=vec128#109,<acbd9=vec128#68,<abba4=vec128#84,<aacbbbda13=vec128#109
# asm 2: mpya >aacbbbda13=$111,<acbd9=$70,<abba4=$86,<aacbbbda13=$111
mpya $111,$70,$86,$111

# qhasm: int32323232 aacbbbda14 += (acbd10 & 0xffff) * (abba4  & 0xffff)
# asm 1: mpya >aacbbbda14=vec128#110,<acbd10=vec128#67,<abba4=vec128#84,<aacbbbda14=vec128#110
# asm 2: mpya >aacbbbda14=$112,<acbd10=$69,<abba4=$86,<aacbbbda14=$112
mpya $112,$69,$86,$112

# qhasm: int32323232 aacbbbda15 += (acbd9  & 0xffff) * (abba6  & 0xffff)
# asm 1: mpya >aacbbbda15=vec128#111,<acbd9=vec128#68,<abba6=vec128#92,<aacbbbda15=vec128#111
# asm 2: mpya >aacbbbda15=$113,<acbd9=$70,<abba6=$94,<aacbbbda15=$113
mpya $113,$70,$94,$113

# qhasm: int32323232 aacbbbda16 += (acbd10 & 0xffff) * (abba6  & 0xffff)
# asm 1: mpya >aacbbbda16=vec128#112,<acbd10=vec128#67,<abba6=vec128#92,<aacbbbda16=vec128#112
# asm 2: mpya >aacbbbda16=$114,<acbd10=$69,<abba6=$94,<aacbbbda16=$114
mpya $114,$69,$94,$114

# qhasm: int32323232 aacbbbda17 += (acbd11 & 0xffff) * (abba6  & 0xffff)
# asm 1: mpya >aacbbbda17=vec128#113,<acbd11=vec128#56,<abba6=vec128#92,<aacbbbda17=vec128#113
# asm 2: mpya >aacbbbda17=$115,<acbd11=$58,<abba6=$94,<aacbbbda17=$115
mpya $115,$58,$94,$115

# qhasm: int32323232 aacbbbda18 <<= 1
# asm 1: shli >aacbbbda18=vec128#114,<aacbbbda18=vec128#114,1
# asm 2: shli >aacbbbda18=$116,<aacbbbda18=$116,1
shli $116,$116,1

# qhasm: int32323232 aacbbbda19 += (acbd1  & 0xffff) * (abba18 & 0xffff)
# asm 1: mpya >aacbbbda19=vec128#115,<acbd1=vec128#76,<abba18=vec128#87,<aacbbbda19=vec128#115
# asm 2: mpya >aacbbbda19=$117,<acbd1=$78,<abba18=$89,<aacbbbda19=$117
mpya $117,$78,$89,$117

# qhasm: int32323232 aacbbbda13 += (acbd12 & 0xffff) * (abba1  & 0xffff)
# asm 1: mpya >aacbbbda13=vec128#109,<acbd12=vec128#62,<abba1=vec128#79,<aacbbbda13=vec128#109
# asm 2: mpya >aacbbbda13=$111,<acbd12=$64,<abba1=$81,<aacbbbda13=$111
mpya $111,$64,$81,$111

# qhasm: int32323232 aacbbbda14 += (acbd12 & 0xffff) * (abba2  & 0xffff)
# asm 1: mpya >aacbbbda14=vec128#110,<acbd12=vec128#62,<abba2=vec128#77,<aacbbbda14=vec128#110
# asm 2: mpya >aacbbbda14=$112,<acbd12=$64,<abba2=$79,<aacbbbda14=$112
mpya $112,$64,$79,$112

# qhasm: int32323232 aacbbbda15 += (acbd10 & 0xffff) * (abba5  & 0xffff)
# asm 1: mpya >aacbbbda15=vec128#111,<acbd10=vec128#67,<abba5=vec128#88,<aacbbbda15=vec128#111
# asm 2: mpya >aacbbbda15=$113,<acbd10=$69,<abba5=$90,<aacbbbda15=$113
mpya $113,$69,$90,$113

# qhasm: int32323232 aacbbbda16 += (acbd11 & 0xffff) * (abba5  & 0xffff)
# asm 1: mpya >aacbbbda16=vec128#112,<acbd11=vec128#56,<abba5=vec128#88,<aacbbbda16=vec128#112
# asm 2: mpya >aacbbbda16=$114,<acbd11=$58,<abba5=$90,<aacbbbda16=$114
mpya $114,$58,$90,$114

# qhasm: int32323232 aacbbbda17 += (acbd14 & 0xffff) * (abba3  & 0xffff)
# asm 1: mpya >aacbbbda17=vec128#113,<acbd14=vec128#60,<abba3=vec128#80,<aacbbbda17=vec128#113
# asm 2: mpya >aacbbbda17=$115,<acbd14=$62,<abba3=$82,<aacbbbda17=$115
mpya $115,$62,$82,$115

# qhasm: int32323232 aacbbbda18 += (acbd0  & 0xffff) * (abba18 & 0xffff)
# asm 1: mpya >aacbbbda18=vec128#114,<acbd0=vec128#70,<abba18=vec128#87,<aacbbbda18=vec128#114
# asm 2: mpya >aacbbbda18=$116,<acbd0=$72,<abba18=$89,<aacbbbda18=$116
mpya $116,$72,$89,$116

# qhasm: int32323232 aacbbbda19 += (acbd2  & 0xffff) * (abba17 & 0xffff)
# asm 1: mpya >aacbbbda19=vec128#115,<acbd2=vec128#63,<abba17=vec128#83,<aacbbbda19=vec128#115
# asm 2: mpya >aacbbbda19=$117,<acbd2=$65,<abba17=$85,<aacbbbda19=$117
mpya $117,$65,$85,$117

# qhasm: int32323232 aacbbbda13 += (acbd13 & 0xffff) * (abba0  & 0xffff)
# asm 1: mpya >aacbbbda13=vec128#109,<acbd13=vec128#64,<abba0=vec128#78,<aacbbbda13=vec128#109
# asm 2: mpya >aacbbbda13=$111,<acbd13=$66,<abba0=$80,<aacbbbda13=$111
mpya $111,$66,$80,$111

# qhasm: int32323232 aacbbbda14 += (acbd13 & 0xffff) * (abba1  & 0xffff)
# asm 1: mpya >aacbbbda14=vec128#110,<acbd13=vec128#64,<abba1=vec128#79,<aacbbbda14=vec128#110
# asm 2: mpya >aacbbbda14=$112,<acbd13=$66,<abba1=$81,<aacbbbda14=$112
mpya $112,$66,$81,$112

# qhasm: int32323232 aacbbbda15 += (acbd11 & 0xffff) * (abba4  & 0xffff)
# asm 1: mpya >aacbbbda15=vec128#111,<acbd11=vec128#56,<abba4=vec128#84,<aacbbbda15=vec128#111
# asm 2: mpya >aacbbbda15=$113,<acbd11=$58,<abba4=$86,<aacbbbda15=$113
mpya $113,$58,$86,$113

# qhasm: int32323232 aacbbbda16 += (acbd13 & 0xffff) * (abba3  & 0xffff)
# asm 1: mpya >aacbbbda16=vec128#112,<acbd13=vec128#64,<abba3=vec128#80,<aacbbbda16=vec128#112
# asm 2: mpya >aacbbbda16=$114,<acbd13=$66,<abba3=$82,<aacbbbda16=$114
mpya $114,$66,$82,$114

# qhasm: int32323232 aacbbbda17 += (acbd15 & 0xffff) * (abba2  & 0xffff)
# asm 1: mpya >aacbbbda17=vec128#113,<acbd15=vec128#57,<abba2=vec128#77,<aacbbbda17=vec128#113
# asm 2: mpya >aacbbbda17=$115,<acbd15=$59,<abba2=$79,<aacbbbda17=$115
mpya $115,$59,$79,$115

# qhasm: int32323232 aacbbbda18 += (acbd1  & 0xffff) * (abba17 & 0xffff)
# asm 1: mpya >aacbbbda18=vec128#114,<acbd1=vec128#76,<abba17=vec128#83,<aacbbbda18=vec128#114
# asm 2: mpya >aacbbbda18=$116,<acbd1=$78,<abba17=$85,<aacbbbda18=$116
mpya $116,$78,$85,$116

# qhasm: int32323232 aacbbbda19 += (acbd3  & 0xffff) * (abba16 & 0xffff)
# asm 1: mpya >aacbbbda19=vec128#115,<acbd3=vec128#5,<abba16=vec128#81,<aacbbbda19=vec128#115
# asm 2: mpya >aacbbbda19=$117,<acbd3=$7,<abba16=$83,<aacbbbda19=$117
mpya $117,$7,$83,$117

# qhasm: int32323232 aacbbbda20  = (acbd1  & 0xffff) * (abba19 & 0xffff)
# asm 1: mpy >aacbbbda20=vec128#116,<acbd1=vec128#76,<abba19=vec128#99
# asm 2: mpy >aacbbbda20=$118,<acbd1=$78,<abba19=$101
mpy $118,$78,$101

# qhasm: int32323232 aacbbbda14 += (acbd14 & 0xffff) * (abba0  & 0xffff)
# asm 1: mpya >aacbbbda14=vec128#110,<acbd14=vec128#60,<abba0=vec128#78,<aacbbbda14=vec128#110
# asm 2: mpya >aacbbbda14=$112,<acbd14=$62,<abba0=$80,<aacbbbda14=$112
mpya $112,$62,$80,$112

# qhasm: int32323232 aacbbbda15 += (acbd12 & 0xffff) * (abba3  & 0xffff)
# asm 1: mpya >aacbbbda15=vec128#111,<acbd12=vec128#62,<abba3=vec128#80,<aacbbbda15=vec128#111
# asm 2: mpya >aacbbbda15=$113,<acbd12=$64,<abba3=$82,<aacbbbda15=$113
mpya $113,$64,$82,$113

# qhasm: int32323232 aacbbbda16 += (acbd14 & 0xffff) * (abba2  & 0xffff)
# asm 1: mpya >aacbbbda16=vec128#112,<acbd14=vec128#60,<abba2=vec128#77,<aacbbbda16=vec128#112
# asm 2: mpya >aacbbbda16=$114,<acbd14=$62,<abba2=$79,<aacbbbda16=$114
mpya $114,$62,$79,$114

# qhasm: int32323232 aacbbbda17 <<= 1
# asm 1: shli >aacbbbda17=vec128#113,<aacbbbda17=vec128#113,1
# asm 2: shli >aacbbbda17=$115,<aacbbbda17=$115,1
shli $115,$115,1

# qhasm: int32323232 aacbbbda18 += (acbd2  & 0xffff) * (abba16 & 0xffff)
# asm 1: mpya >aacbbbda18=vec128#114,<acbd2=vec128#63,<abba16=vec128#81,<aacbbbda18=vec128#114
# asm 2: mpya >aacbbbda18=$116,<acbd2=$65,<abba16=$83,<aacbbbda18=$116
mpya $116,$65,$83,$116

# qhasm: int32323232 aacbbbda19 += (acbd4  & 0xffff) * (abba15 & 0xffff)
# asm 1: mpya >aacbbbda19=vec128#115,<acbd4=vec128#58,<abba15=vec128#97,<aacbbbda19=vec128#115
# asm 2: mpya >aacbbbda19=$117,<acbd4=$60,<abba15=$99,<aacbbbda19=$117
mpya $117,$60,$99,$117

# qhasm: int32323232 aacbbbda20 += (acbd2  & 0xffff) * (abba18 & 0xffff)
# asm 1: mpya >aacbbbda20=vec128#116,<acbd2=vec128#63,<abba18=vec128#87,<aacbbbda20=vec128#116
# asm 2: mpya >aacbbbda20=$118,<acbd2=$65,<abba18=$89,<aacbbbda20=$118
mpya $118,$65,$89,$118

# qhasm: int32323232 aacbbbda21  = (acbd2  & 0xffff) * (abba19 & 0xffff)
# asm 1: mpy >aacbbbda21=vec128#63,<acbd2=vec128#63,<abba19=vec128#99
# asm 2: mpy >aacbbbda21=$65,<acbd2=$65,<abba19=$101
mpy $65,$65,$101

# qhasm: int32323232 aacbbbda15 += (acbd13 & 0xffff) * (abba2  & 0xffff)
# asm 1: mpya >aacbbbda15=vec128#111,<acbd13=vec128#64,<abba2=vec128#77,<aacbbbda15=vec128#111
# asm 2: mpya >aacbbbda15=$113,<acbd13=$66,<abba2=$79,<aacbbbda15=$113
mpya $113,$66,$79,$113

# qhasm: int32323232 aacbbbda16 += (acbd15 & 0xffff) * (abba1  & 0xffff)
# asm 1: mpya >aacbbbda16=vec128#112,<acbd15=vec128#57,<abba1=vec128#79,<aacbbbda16=vec128#112
# asm 2: mpya >aacbbbda16=$114,<acbd15=$59,<abba1=$81,<aacbbbda16=$114
mpya $114,$59,$81,$114

# qhasm: int32323232 aacbbbda17 += (acbd0  & 0xffff) * (abba17 & 0xffff)
# asm 1: mpya >aacbbbda17=vec128#113,<acbd0=vec128#70,<abba17=vec128#83,<aacbbbda17=vec128#113
# asm 2: mpya >aacbbbda17=$115,<acbd0=$72,<abba17=$85,<aacbbbda17=$115
mpya $115,$72,$85,$115

# qhasm: int32323232 aacbbbda18 += (acbd4  & 0xffff) * (abba14 & 0xffff)
# asm 1: mpya >aacbbbda18=vec128#114,<acbd4=vec128#58,<abba14=vec128#86,<aacbbbda18=vec128#114
# asm 2: mpya >aacbbbda18=$116,<acbd4=$60,<abba14=$88,<aacbbbda18=$116
mpya $116,$60,$88,$116

# qhasm: int32323232 aacbbbda19 += (acbd5  & 0xffff) * (abba14 & 0xffff)
# asm 1: mpya >aacbbbda19=vec128#115,<acbd5=vec128#66,<abba14=vec128#86,<aacbbbda19=vec128#115
# asm 2: mpya >aacbbbda19=$117,<acbd5=$68,<abba14=$88,<aacbbbda19=$117
mpya $117,$68,$88,$117

# qhasm: int32323232 aacbbbda20 += (acbd3  & 0xffff) * (abba17 & 0xffff)
# asm 1: mpya >aacbbbda20=vec128#116,<acbd3=vec128#5,<abba17=vec128#83,<aacbbbda20=vec128#116
# asm 2: mpya >aacbbbda20=$118,<acbd3=$7,<abba17=$85,<aacbbbda20=$118
mpya $118,$7,$85,$118

# qhasm: int32323232 aacbbbda21 += (acbd3  & 0xffff) * (abba18 & 0xffff)
# asm 1: mpya >aacbbbda21=vec128#63,<acbd3=vec128#5,<abba18=vec128#87,<aacbbbda21=vec128#63
# asm 2: mpya >aacbbbda21=$65,<acbd3=$7,<abba18=$89,<aacbbbda21=$65
mpya $65,$7,$89,$65

# qhasm: int32323232 aacbbbda15 += (acbd14 & 0xffff) * (abba1  & 0xffff)
# asm 1: mpya >aacbbbda15=vec128#111,<acbd14=vec128#60,<abba1=vec128#79,<aacbbbda15=vec128#111
# asm 2: mpya >aacbbbda15=$113,<acbd14=$62,<abba1=$81,<aacbbbda15=$113
mpya $113,$62,$81,$113

# qhasm: int32323232 aacbbbda16 <<= 1
# asm 1: shli >aacbbbda16=vec128#112,<aacbbbda16=vec128#112,1
# asm 2: shli >aacbbbda16=$114,<aacbbbda16=$114,1
shli $114,$114,1

# qhasm: int32323232 aacbbbda17 += (acbd1  & 0xffff) * (abba16 & 0xffff)
# asm 1: mpya >aacbbbda17=vec128#76,<acbd1=vec128#76,<abba16=vec128#81,<aacbbbda17=vec128#113
# asm 2: mpya >aacbbbda17=$78,<acbd1=$78,<abba16=$83,<aacbbbda17=$115
mpya $78,$78,$83,$115

# qhasm: int32323232 aacbbbda18 += (acbd5  & 0xffff) * (abba13 & 0xffff)
# asm 1: mpya >aacbbbda18=vec128#113,<acbd5=vec128#66,<abba13=vec128#82,<aacbbbda18=vec128#114
# asm 2: mpya >aacbbbda18=$115,<acbd5=$68,<abba13=$84,<aacbbbda18=$116
mpya $115,$68,$84,$116

# qhasm: int32323232 aacbbbda19 += (acbd6  & 0xffff) * (abba13 & 0xffff)
# asm 1: mpya >aacbbbda19=vec128#114,<acbd6=vec128#65,<abba13=vec128#82,<aacbbbda19=vec128#115
# asm 2: mpya >aacbbbda19=$116,<acbd6=$67,<abba13=$84,<aacbbbda19=$117
mpya $116,$67,$84,$117

# qhasm: int32323232 aacbbbda20 += (acbd5  & 0xffff) * (abba15 & 0xffff)
# asm 1: mpya >aacbbbda20=vec128#115,<acbd5=vec128#66,<abba15=vec128#97,<aacbbbda20=vec128#116
# asm 2: mpya >aacbbbda20=$117,<acbd5=$68,<abba15=$99,<aacbbbda20=$118
mpya $117,$68,$99,$118

# qhasm: int32323232 aacbbbda21 += (acbd6  & 0xffff) * (abba15 & 0xffff)
# asm 1: mpya >aacbbbda21=vec128#63,<acbd6=vec128#65,<abba15=vec128#97,<aacbbbda21=vec128#63
# asm 2: mpya >aacbbbda21=$65,<acbd6=$67,<abba15=$99,<aacbbbda21=$65
mpya $65,$67,$99,$65

# qhasm: int32323232 aacbbbda15 += (acbd15 & 0xffff) * (abba0  & 0xffff)
# asm 1: mpya >aacbbbda15=vec128#111,<acbd15=vec128#57,<abba0=vec128#78,<aacbbbda15=vec128#111
# asm 2: mpya >aacbbbda15=$113,<acbd15=$59,<abba0=$80,<aacbbbda15=$113
mpya $113,$59,$80,$113

# qhasm: int32323232 aacbbbda16 += (acbd0  & 0xffff) * (abba16 & 0xffff)
# asm 1: mpya >aacbbbda16=vec128#70,<acbd0=vec128#70,<abba16=vec128#81,<aacbbbda16=vec128#112
# asm 2: mpya >aacbbbda16=$72,<acbd0=$72,<abba16=$83,<aacbbbda16=$114
mpya $72,$72,$83,$114

# qhasm: int32323232 aacbbbda17 += (acbd4  & 0xffff) * (abba13 & 0xffff)
# asm 1: mpya >aacbbbda17=vec128#76,<acbd4=vec128#58,<abba13=vec128#82,<aacbbbda17=vec128#76
# asm 2: mpya >aacbbbda17=$78,<acbd4=$60,<abba13=$84,<aacbbbda17=$78
mpya $78,$60,$84,$78

# qhasm: int32323232 aacbbbda18 += (acbd6  & 0xffff) * (abba12 & 0xffff)
# asm 1: mpya >aacbbbda18=vec128#112,<acbd6=vec128#65,<abba12=vec128#71,<aacbbbda18=vec128#113
# asm 2: mpya >aacbbbda18=$114,<acbd6=$67,<abba12=$73,<aacbbbda18=$115
mpya $114,$67,$73,$115

# qhasm: int32323232 aacbbbda19 += (acbd7  & 0xffff) * (abba12 & 0xffff)
# asm 1: mpya >aacbbbda19=vec128#113,<acbd7=vec128#55,<abba12=vec128#71,<aacbbbda19=vec128#114
# asm 2: mpya >aacbbbda19=$115,<acbd7=$57,<abba12=$73,<aacbbbda19=$116
mpya $115,$57,$73,$116

# qhasm: int32323232 aacbbbda20 += (acbd6  & 0xffff) * (abba14 & 0xffff)
# asm 1: mpya >aacbbbda20=vec128#114,<acbd6=vec128#65,<abba14=vec128#86,<aacbbbda20=vec128#115
# asm 2: mpya >aacbbbda20=$116,<acbd6=$67,<abba14=$88,<aacbbbda20=$117
mpya $116,$67,$88,$117

# qhasm: int32323232 aacbbbda21 += (acbd7  & 0xffff) * (abba14 & 0xffff)
# asm 1: mpya >aacbbbda21=vec128#63,<acbd7=vec128#55,<abba14=vec128#86,<aacbbbda21=vec128#63
# asm 2: mpya >aacbbbda21=$65,<acbd7=$57,<abba14=$88,<aacbbbda21=$65
mpya $65,$57,$88,$65

# qhasm: int32323232 aacbbbda22  = (acbd3  & 0xffff) * (abba19 & 0xffff)
# asm 1: mpy >aacbbbda22=vec128#5,<acbd3=vec128#5,<abba19=vec128#99
# asm 2: mpy >aacbbbda22=$7,<acbd3=$7,<abba19=$101
mpy $7,$7,$101

# qhasm: int32323232 aacbbbda16 += (acbd4  & 0xffff) * (abba12 & 0xffff)
# asm 1: mpya >aacbbbda16=vec128#70,<acbd4=vec128#58,<abba12=vec128#71,<aacbbbda16=vec128#70
# asm 2: mpya >aacbbbda16=$72,<acbd4=$60,<abba12=$73,<aacbbbda16=$72
mpya $72,$60,$73,$72

# qhasm: int32323232 aacbbbda17 += (acbd5  & 0xffff) * (abba12 & 0xffff)
# asm 1: mpya >aacbbbda17=vec128#76,<acbd5=vec128#66,<abba12=vec128#71,<aacbbbda17=vec128#76
# asm 2: mpya >aacbbbda17=$78,<acbd5=$68,<abba12=$73,<aacbbbda17=$78
mpya $78,$68,$73,$78

# qhasm: int32323232 aacbbbda18 += (acbd8  & 0xffff) * (abba10 & 0xffff)
# asm 1: mpya >aacbbbda18=vec128#112,<acbd8=vec128#61,<abba10=vec128#93,<aacbbbda18=vec128#112
# asm 2: mpya >aacbbbda18=$114,<acbd8=$63,<abba10=$95,<aacbbbda18=$114
mpya $114,$63,$95,$114

# qhasm: int32323232 aacbbbda19 += (acbd8  & 0xffff) * (abba11 & 0xffff)
# asm 1: mpya >aacbbbda19=vec128#113,<acbd8=vec128#61,<abba11=vec128#95,<aacbbbda19=vec128#113
# asm 2: mpya >aacbbbda19=$115,<acbd8=$63,<abba11=$97,<aacbbbda19=$115
mpya $115,$63,$97,$115

# qhasm: int32323232 aacbbbda20 += (acbd7  & 0xffff) * (abba13 & 0xffff)
# asm 1: mpya >aacbbbda20=vec128#114,<acbd7=vec128#55,<abba13=vec128#82,<aacbbbda20=vec128#114
# asm 2: mpya >aacbbbda20=$116,<acbd7=$57,<abba13=$84,<aacbbbda20=$116
mpya $116,$57,$84,$116

# qhasm: int32323232 aacbbbda21 += (acbd10 & 0xffff) * (abba11 & 0xffff)
# asm 1: mpya >aacbbbda21=vec128#63,<acbd10=vec128#67,<abba11=vec128#95,<aacbbbda21=vec128#63
# asm 2: mpya >aacbbbda21=$65,<acbd10=$69,<abba11=$97,<aacbbbda21=$65
mpya $65,$69,$97,$65

# qhasm: int32323232 aacbbbda22 += (acbd7  & 0xffff) * (abba15 & 0xffff)
# asm 1: mpya >aacbbbda22=vec128#5,<acbd7=vec128#55,<abba15=vec128#97,<aacbbbda22=vec128#5
# asm 2: mpya >aacbbbda22=$7,<acbd7=$57,<abba15=$99,<aacbbbda22=$7
mpya $7,$57,$99,$7

# qhasm: int32323232 aacbbbda16 += (acbd8  & 0xffff) * (abba8  & 0xffff)
# asm 1: mpya >aacbbbda16=vec128#70,<acbd8=vec128#61,<abba8=vec128#85,<aacbbbda16=vec128#70
# asm 2: mpya >aacbbbda16=$72,<acbd8=$63,<abba8=$87,<aacbbbda16=$72
mpya $72,$63,$87,$72

# qhasm: int32323232 aacbbbda17 += (acbd8  & 0xffff) * (abba9  & 0xffff)
# asm 1: mpya >aacbbbda17=vec128#76,<acbd8=vec128#61,<abba9=vec128#89,<aacbbbda17=vec128#76
# asm 2: mpya >aacbbbda17=$78,<acbd8=$63,<abba9=$91,<aacbbbda17=$78
mpya $78,$63,$91,$78

# qhasm: int32323232 aacbbbda18 += (acbd9  & 0xffff) * (abba9  & 0xffff)
# asm 1: mpya >aacbbbda18=vec128#112,<acbd9=vec128#68,<abba9=vec128#89,<aacbbbda18=vec128#112
# asm 2: mpya >aacbbbda18=$114,<acbd9=$70,<abba9=$91,<aacbbbda18=$114
mpya $114,$70,$91,$114

# qhasm: int32323232 aacbbbda19 += (acbd9  & 0xffff) * (abba10 & 0xffff)
# asm 1: mpya >aacbbbda19=vec128#113,<acbd9=vec128#68,<abba10=vec128#93,<aacbbbda19=vec128#113
# asm 2: mpya >aacbbbda19=$115,<acbd9=$70,<abba10=$95,<aacbbbda19=$115
mpya $115,$70,$95,$115

# qhasm: int32323232 aacbbbda20 += (acbd9  & 0xffff) * (abba11 & 0xffff)
# asm 1: mpya >aacbbbda20=vec128#114,<acbd9=vec128#68,<abba11=vec128#95,<aacbbbda20=vec128#114
# asm 2: mpya >aacbbbda20=$116,<acbd9=$70,<abba11=$97,<aacbbbda20=$116
mpya $116,$70,$97,$116

# qhasm: int32323232 aacbbbda21 += (acbd11 & 0xffff) * (abba10 & 0xffff)
# asm 1: mpya >aacbbbda21=vec128#63,<acbd11=vec128#56,<abba10=vec128#93,<aacbbbda21=vec128#63
# asm 2: mpya >aacbbbda21=$65,<acbd11=$58,<abba10=$95,<aacbbbda21=$65
mpya $65,$58,$95,$65

# qhasm: int32323232 aacbbbda22 += (acbd11 & 0xffff) * (abba11 & 0xffff)
# asm 1: mpya >aacbbbda22=vec128#5,<acbd11=vec128#56,<abba11=vec128#95,<aacbbbda22=vec128#5
# asm 2: mpya >aacbbbda22=$7,<acbd11=$58,<abba11=$97,<aacbbbda22=$7
mpya $7,$58,$97,$7

# qhasm: int32323232 aacbbbda16 += (acbd12 & 0xffff) * (abba4  & 0xffff)
# asm 1: mpya >aacbbbda16=vec128#70,<acbd12=vec128#62,<abba4=vec128#84,<aacbbbda16=vec128#70
# asm 2: mpya >aacbbbda16=$72,<acbd12=$64,<abba4=$86,<aacbbbda16=$72
mpya $72,$64,$86,$72

# qhasm: int32323232 aacbbbda17 += (acbd9  & 0xffff) * (abba8  & 0xffff)
# asm 1: mpya >aacbbbda17=vec128#76,<acbd9=vec128#68,<abba8=vec128#85,<aacbbbda17=vec128#76
# asm 2: mpya >aacbbbda17=$78,<acbd9=$70,<abba8=$87,<aacbbbda17=$78
mpya $78,$70,$87,$78

# qhasm: int32323232 aacbbbda18 += (acbd10 & 0xffff) * (abba8  & 0xffff)
# asm 1: mpya >aacbbbda18=vec128#112,<acbd10=vec128#67,<abba8=vec128#85,<aacbbbda18=vec128#112
# asm 2: mpya >aacbbbda18=$114,<acbd10=$69,<abba8=$87,<aacbbbda18=$114
mpya $114,$69,$87,$114

# qhasm: int32323232 aacbbbda19 += (acbd10 & 0xffff) * (abba9  & 0xffff)
# asm 1: mpya >aacbbbda19=vec128#113,<acbd10=vec128#67,<abba9=vec128#89,<aacbbbda19=vec128#113
# asm 2: mpya >aacbbbda19=$115,<acbd10=$69,<abba9=$91,<aacbbbda19=$115
mpya $115,$69,$91,$115

# qhasm: int32323232 aacbbbda20 += (acbd10 & 0xffff) * (abba10 & 0xffff)
# asm 1: mpya >aacbbbda20=vec128#114,<acbd10=vec128#67,<abba10=vec128#93,<aacbbbda20=vec128#114
# asm 2: mpya >aacbbbda20=$116,<acbd10=$69,<abba10=$95,<aacbbbda20=$116
mpya $116,$69,$95,$116

# qhasm: int32323232 aacbbbda21 += (acbd14 & 0xffff) * (abba7  & 0xffff)
# asm 1: mpya >aacbbbda21=vec128#63,<acbd14=vec128#60,<abba7=vec128#91,<aacbbbda21=vec128#63
# asm 2: mpya >aacbbbda21=$65,<acbd14=$62,<abba7=$93,<aacbbbda21=$65
mpya $65,$62,$93,$65

# qhasm: int32323232 aacbbbda22 += (acbd15 & 0xffff) * (abba7  & 0xffff)
# asm 1: mpya >aacbbbda22=vec128#5,<acbd15=vec128#57,<abba7=vec128#91,<aacbbbda22=vec128#5
# asm 2: mpya >aacbbbda22=$7,<acbd15=$59,<abba7=$93,<aacbbbda22=$7
mpya $7,$59,$93,$7

# qhasm: int32323232 aacbbbda16 += (acbd16 & 0xffff) * (abba0  & 0xffff)
# asm 1: mpya >aacbbbda16=vec128#70,<acbd16=vec128#69,<abba0=vec128#78,<aacbbbda16=vec128#70
# asm 2: mpya >aacbbbda16=$72,<acbd16=$71,<abba0=$80,<aacbbbda16=$72
mpya $72,$71,$80,$72

# qhasm: int32323232 aacbbbda17 += (acbd12 & 0xffff) * (abba5  & 0xffff)
# asm 1: mpya >aacbbbda17=vec128#76,<acbd12=vec128#62,<abba5=vec128#88,<aacbbbda17=vec128#76
# asm 2: mpya >aacbbbda17=$78,<acbd12=$64,<abba5=$90,<aacbbbda17=$78
mpya $78,$64,$90,$78

# qhasm: int32323232 aacbbbda18 += (acbd12 & 0xffff) * (abba6  & 0xffff)
# asm 1: mpya >aacbbbda18=vec128#112,<acbd12=vec128#62,<abba6=vec128#92,<aacbbbda18=vec128#112
# asm 2: mpya >aacbbbda18=$114,<acbd12=$64,<abba6=$94,<aacbbbda18=$114
mpya $114,$64,$94,$114

# qhasm: int32323232 aacbbbda19 += (acbd11 & 0xffff) * (abba8  & 0xffff)
# asm 1: mpya >aacbbbda19=vec128#113,<acbd11=vec128#56,<abba8=vec128#85,<aacbbbda19=vec128#113
# asm 2: mpya >aacbbbda19=$115,<acbd11=$58,<abba8=$87,<aacbbbda19=$115
mpya $115,$58,$87,$115

# qhasm: int32323232 aacbbbda20 += (acbd11 & 0xffff) * (abba9  & 0xffff)
# asm 1: mpya >aacbbbda20=vec128#114,<acbd11=vec128#56,<abba9=vec128#89,<aacbbbda20=vec128#114
# asm 2: mpya >aacbbbda20=$116,<acbd11=$58,<abba9=$91,<aacbbbda20=$116
mpya $116,$58,$91,$116

# qhasm: int32323232 aacbbbda21 += (acbd15 & 0xffff) * (abba6  & 0xffff)
# asm 1: mpya >aacbbbda21=vec128#63,<acbd15=vec128#57,<abba6=vec128#92,<aacbbbda21=vec128#63
# asm 2: mpya >aacbbbda21=$65,<acbd15=$59,<abba6=$94,<aacbbbda21=$65
mpya $65,$59,$94,$65

# qhasm: int32323232 aacbbbda22 += (acbd19 & 0xffff) * (abba3  & 0xffff)
# asm 1: mpya >aacbbbda22=vec128#5,<acbd19=vec128#59,<abba3=vec128#80,<aacbbbda22=vec128#5
# asm 2: mpya >aacbbbda22=$7,<acbd19=$61,<abba3=$82,<aacbbbda22=$7
mpya $7,$61,$82,$7

# qhasm: int32323232 aacbbbda23  = (acbd4  & 0xffff) * (abba19 & 0xffff)
# asm 1: mpy >aacbbbda23=vec128#115,<acbd4=vec128#58,<abba19=vec128#99
# asm 2: mpy >aacbbbda23=$117,<acbd4=$60,<abba19=$101
mpy $117,$60,$101

# qhasm: int32323232 aacbbbda17 += (acbd13 & 0xffff) * (abba4  & 0xffff)
# asm 1: mpya >aacbbbda17=vec128#76,<acbd13=vec128#64,<abba4=vec128#84,<aacbbbda17=vec128#76
# asm 2: mpya >aacbbbda17=$78,<acbd13=$66,<abba4=$86,<aacbbbda17=$78
mpya $78,$66,$86,$78

# qhasm: int32323232 aacbbbda18 += (acbd13 & 0xffff) * (abba5  & 0xffff)
# asm 1: mpya >aacbbbda18=vec128#112,<acbd13=vec128#64,<abba5=vec128#88,<aacbbbda18=vec128#112
# asm 2: mpya >aacbbbda18=$114,<acbd13=$66,<abba5=$90,<aacbbbda18=$114
mpya $114,$66,$90,$114

# qhasm: int32323232 aacbbbda19 += (acbd12 & 0xffff) * (abba7  & 0xffff)
# asm 1: mpya >aacbbbda19=vec128#113,<acbd12=vec128#62,<abba7=vec128#91,<aacbbbda19=vec128#113
# asm 2: mpya >aacbbbda19=$115,<acbd12=$64,<abba7=$93,<aacbbbda19=$115
mpya $115,$64,$93,$115

# qhasm: int32323232 aacbbbda20 += (acbd13 & 0xffff) * (abba7  & 0xffff)
# asm 1: mpya >aacbbbda20=vec128#114,<acbd13=vec128#64,<abba7=vec128#91,<aacbbbda20=vec128#114
# asm 2: mpya >aacbbbda20=$116,<acbd13=$66,<abba7=$93,<aacbbbda20=$116
mpya $116,$66,$93,$116

# qhasm: int32323232 aacbbbda21 += (acbd18 & 0xffff) * (abba3  & 0xffff)
# asm 1: mpya >aacbbbda21=vec128#63,<acbd18=vec128#73,<abba3=vec128#80,<aacbbbda21=vec128#63
# asm 2: mpya >aacbbbda21=$65,<acbd18=$75,<abba3=$82,<aacbbbda21=$65
mpya $65,$75,$82,$65

# qhasm: int32323232 aacbbbda22 <<= 1
# asm 1: shli >aacbbbda22=vec128#5,<aacbbbda22=vec128#5,1
# asm 2: shli >aacbbbda22=$7,<aacbbbda22=$7,1
shli $7,$7,1

# qhasm: int32323232 aacbbbda23 += (acbd5  & 0xffff) * (abba18 & 0xffff)
# asm 1: mpya >aacbbbda23=vec128#115,<acbd5=vec128#66,<abba18=vec128#87,<aacbbbda23=vec128#115
# asm 2: mpya >aacbbbda23=$117,<acbd5=$68,<abba18=$89,<aacbbbda23=$117
mpya $117,$68,$89,$117

# qhasm: int32323232 aacbbbda17 += (acbd16 & 0xffff) * (abba1  & 0xffff)
# asm 1: mpya >aacbbbda17=vec128#76,<acbd16=vec128#69,<abba1=vec128#79,<aacbbbda17=vec128#76
# asm 2: mpya >aacbbbda17=$78,<acbd16=$71,<abba1=$81,<aacbbbda17=$78
mpya $78,$71,$81,$78

# qhasm: int32323232 aacbbbda18 += (acbd14 & 0xffff) * (abba4  & 0xffff)
# asm 1: mpya >aacbbbda18=vec128#112,<acbd14=vec128#60,<abba4=vec128#84,<aacbbbda18=vec128#112
# asm 2: mpya >aacbbbda18=$114,<acbd14=$62,<abba4=$86,<aacbbbda18=$114
mpya $114,$62,$86,$114

# qhasm: int32323232 aacbbbda19 += (acbd13 & 0xffff) * (abba6  & 0xffff)
# asm 1: mpya >aacbbbda19=vec128#113,<acbd13=vec128#64,<abba6=vec128#92,<aacbbbda19=vec128#113
# asm 2: mpya >aacbbbda19=$115,<acbd13=$66,<abba6=$94,<aacbbbda19=$115
mpya $115,$66,$94,$115

# qhasm: int32323232 aacbbbda20 += (acbd14 & 0xffff) * (abba6  & 0xffff)
# asm 1: mpya >aacbbbda20=vec128#114,<acbd14=vec128#60,<abba6=vec128#92,<aacbbbda20=vec128#114
# asm 2: mpya >aacbbbda20=$116,<acbd14=$62,<abba6=$94,<aacbbbda20=$116
mpya $116,$62,$94,$116

# qhasm: int32323232 aacbbbda21 += (acbd19 & 0xffff) * (abba2  & 0xffff)
# asm 1: mpya >aacbbbda21=vec128#63,<acbd19=vec128#59,<abba2=vec128#77,<aacbbbda21=vec128#63
# asm 2: mpya >aacbbbda21=$65,<acbd19=$61,<abba2=$79,<aacbbbda21=$65
mpya $65,$61,$79,$65

# qhasm: int32323232 aacbbbda22 += (acbd4  & 0xffff) * (abba18 & 0xffff)
# asm 1: mpya >aacbbbda22=vec128#5,<acbd4=vec128#58,<abba18=vec128#87,<aacbbbda22=vec128#5
# asm 2: mpya >aacbbbda22=$7,<acbd4=$60,<abba18=$89,<aacbbbda22=$7
mpya $7,$60,$89,$7

# qhasm: int32323232 aacbbbda23 += (acbd6  & 0xffff) * (abba17 & 0xffff)
# asm 1: mpya >aacbbbda23=vec128#115,<acbd6=vec128#65,<abba17=vec128#83,<aacbbbda23=vec128#115
# asm 2: mpya >aacbbbda23=$117,<acbd6=$67,<abba17=$85,<aacbbbda23=$117
mpya $117,$67,$85,$117

# qhasm: int32323232 aacbbbda17 += (acbd17 & 0xffff) * (abba0  & 0xffff)
# asm 1: mpya >aacbbbda17=vec128#76,<acbd17=vec128#72,<abba0=vec128#78,<aacbbbda17=vec128#76
# asm 2: mpya >aacbbbda17=$78,<acbd17=$74,<abba0=$80,<aacbbbda17=$78
mpya $78,$74,$80,$78

# qhasm: int32323232 aacbbbda18 += (acbd16 & 0xffff) * (abba2  & 0xffff)
# asm 1: mpya >aacbbbda18=vec128#112,<acbd16=vec128#69,<abba2=vec128#77,<aacbbbda18=vec128#112
# asm 2: mpya >aacbbbda18=$114,<acbd16=$71,<abba2=$79,<aacbbbda18=$114
mpya $114,$71,$79,$114

# qhasm: int32323232 aacbbbda19 += (acbd14 & 0xffff) * (abba5  & 0xffff)
# asm 1: mpya >aacbbbda19=vec128#113,<acbd14=vec128#60,<abba5=vec128#88,<aacbbbda19=vec128#113
# asm 2: mpya >aacbbbda19=$115,<acbd14=$62,<abba5=$90,<aacbbbda19=$115
mpya $115,$62,$90,$115

# qhasm: int32323232 aacbbbda20 += (acbd15 & 0xffff) * (abba5  & 0xffff)
# asm 1: mpya >aacbbbda20=vec128#114,<acbd15=vec128#57,<abba5=vec128#88,<aacbbbda20=vec128#114
# asm 2: mpya >aacbbbda20=$116,<acbd15=$59,<abba5=$90,<aacbbbda20=$116
mpya $116,$59,$90,$116

# qhasm: int32323232 aacbbbda21 <<= 1
# asm 1: shli >aacbbbda21=vec128#63,<aacbbbda21=vec128#63,1
# asm 2: shli >aacbbbda21=$65,<aacbbbda21=$65,1
shli $65,$65,1

# qhasm: int32323232 aacbbbda22 += (acbd5  & 0xffff) * (abba17 & 0xffff)
# asm 1: mpya >aacbbbda22=vec128#5,<acbd5=vec128#66,<abba17=vec128#83,<aacbbbda22=vec128#5
# asm 2: mpya >aacbbbda22=$7,<acbd5=$68,<abba17=$85,<aacbbbda22=$7
mpya $7,$68,$85,$7

# qhasm: int32323232 aacbbbda23 += (acbd7  & 0xffff) * (abba16 & 0xffff)
# asm 1: mpya >aacbbbda23=vec128#115,<acbd7=vec128#55,<abba16=vec128#81,<aacbbbda23=vec128#115
# asm 2: mpya >aacbbbda23=$117,<acbd7=$57,<abba16=$83,<aacbbbda23=$117
mpya $117,$57,$83,$117

# qhasm: int32323232 aacbbbda24  = (acbd5  & 0xffff) * (abba19  & 0xffff)
# asm 1: mpy >aacbbbda24=vec128#116,<acbd5=vec128#66,<abba19=vec128#99
# asm 2: mpy >aacbbbda24=$118,<acbd5=$68,<abba19=$101
mpy $118,$68,$101

# qhasm: int32323232 aacbbbda18 += (acbd17 & 0xffff) * (abba1  & 0xffff)
# asm 1: mpya >aacbbbda18=vec128#112,<acbd17=vec128#72,<abba1=vec128#79,<aacbbbda18=vec128#112
# asm 2: mpya >aacbbbda18=$114,<acbd17=$74,<abba1=$81,<aacbbbda18=$114
mpya $114,$74,$81,$114

# qhasm: int32323232 aacbbbda19 += (acbd15 & 0xffff) * (abba4  & 0xffff)
# asm 1: mpya >aacbbbda19=vec128#113,<acbd15=vec128#57,<abba4=vec128#84,<aacbbbda19=vec128#113
# asm 2: mpya >aacbbbda19=$115,<acbd15=$59,<abba4=$86,<aacbbbda19=$115
mpya $115,$59,$86,$115

# qhasm: int32323232 aacbbbda20 += (acbd17 & 0xffff) * (abba3  & 0xffff)
# asm 1: mpya >aacbbbda20=vec128#114,<acbd17=vec128#72,<abba3=vec128#80,<aacbbbda20=vec128#114
# asm 2: mpya >aacbbbda20=$116,<acbd17=$74,<abba3=$82,<aacbbbda20=$116
mpya $116,$74,$82,$116

# qhasm: int32323232 aacbbbda21 += (acbd4  & 0xffff) * (abba17  & 0xffff)
# asm 1: mpya >aacbbbda21=vec128#63,<acbd4=vec128#58,<abba17=vec128#83,<aacbbbda21=vec128#63
# asm 2: mpya >aacbbbda21=$65,<acbd4=$60,<abba17=$85,<aacbbbda21=$65
mpya $65,$60,$85,$65

# qhasm: int32323232 aacbbbda22 += (acbd6  & 0xffff) * (abba16 & 0xffff)
# asm 1: mpya >aacbbbda22=vec128#5,<acbd6=vec128#65,<abba16=vec128#81,<aacbbbda22=vec128#5
# asm 2: mpya >aacbbbda22=$7,<acbd6=$67,<abba16=$83,<aacbbbda22=$7
mpya $7,$67,$83,$7

# qhasm: int32323232 aacbbbda23 += (acbd8  & 0xffff) * (abba15 & 0xffff)
# asm 1: mpya >aacbbbda23=vec128#115,<acbd8=vec128#61,<abba15=vec128#97,<aacbbbda23=vec128#115
# asm 2: mpya >aacbbbda23=$117,<acbd8=$63,<abba15=$99,<aacbbbda23=$117
mpya $117,$63,$99,$117

# qhasm: int32323232 aacbbbda24 += (acbd6  & 0xffff) * (abba18  & 0xffff)
# asm 1: mpya >aacbbbda24=vec128#116,<acbd6=vec128#65,<abba18=vec128#87,<aacbbbda24=vec128#116
# asm 2: mpya >aacbbbda24=$118,<acbd6=$67,<abba18=$89,<aacbbbda24=$118
mpya $118,$67,$89,$118

# qhasm: int32323232 aacbbbda18 += (acbd18 & 0xffff) * (abba0  & 0xffff)
# asm 1: mpya >aacbbbda18=vec128#112,<acbd18=vec128#73,<abba0=vec128#78,<aacbbbda18=vec128#112
# asm 2: mpya >aacbbbda18=$114,<acbd18=$75,<abba0=$80,<aacbbbda18=$114
mpya $114,$75,$80,$114

# qhasm: int32323232 aacbbbda19 += (acbd16 & 0xffff) * (abba3  & 0xffff)
# asm 1: mpya >aacbbbda19=vec128#80,<acbd16=vec128#69,<abba3=vec128#80,<aacbbbda19=vec128#113
# asm 2: mpya >aacbbbda19=$82,<acbd16=$71,<abba3=$82,<aacbbbda19=$115
mpya $82,$71,$82,$115

# qhasm: int32323232 aacbbbda20 += (acbd18 & 0xffff) * (abba2  & 0xffff)
# asm 1: mpya >aacbbbda20=vec128#113,<acbd18=vec128#73,<abba2=vec128#77,<aacbbbda20=vec128#114
# asm 2: mpya >aacbbbda20=$115,<acbd18=$75,<abba2=$79,<aacbbbda20=$116
mpya $115,$75,$79,$116

# qhasm: int32323232 aacbbbda21 += (acbd5  & 0xffff) * (abba16  & 0xffff)
# asm 1: mpya >aacbbbda21=vec128#63,<acbd5=vec128#66,<abba16=vec128#81,<aacbbbda21=vec128#63
# asm 2: mpya >aacbbbda21=$65,<acbd5=$68,<abba16=$83,<aacbbbda21=$65
mpya $65,$68,$83,$65

# qhasm: int32323232 aacbbbda22 += (acbd8  & 0xffff) * (abba14 & 0xffff)
# asm 1: mpya >aacbbbda22=vec128#5,<acbd8=vec128#61,<abba14=vec128#86,<aacbbbda22=vec128#5
# asm 2: mpya >aacbbbda22=$7,<acbd8=$63,<abba14=$88,<aacbbbda22=$7
mpya $7,$63,$88,$7

# qhasm: int32323232 aacbbbda23 += (acbd9  & 0xffff) * (abba14 & 0xffff)
# asm 1: mpya >aacbbbda23=vec128#66,<acbd9=vec128#68,<abba14=vec128#86,<aacbbbda23=vec128#115
# asm 2: mpya >aacbbbda23=$68,<acbd9=$70,<abba14=$88,<aacbbbda23=$117
mpya $68,$70,$88,$117

# qhasm: int32323232 aacbbbda24 += (acbd7  & 0xffff) * (abba17  & 0xffff)
# asm 1: mpya >aacbbbda24=vec128#114,<acbd7=vec128#55,<abba17=vec128#83,<aacbbbda24=vec128#116
# asm 2: mpya >aacbbbda24=$116,<acbd7=$57,<abba17=$85,<aacbbbda24=$118
mpya $116,$57,$85,$118

# qhasm: int32323232 aacbbbda25  = (acbd6  & 0xffff) * (abba19 & 0xffff)
# asm 1: mpy >aacbbbda25=vec128#65,<acbd6=vec128#65,<abba19=vec128#99
# asm 2: mpy >aacbbbda25=$67,<acbd6=$67,<abba19=$101
mpy $67,$67,$101

# qhasm: int32323232 aacbbbda19 += (acbd17 & 0xffff) * (abba2  & 0xffff)
# asm 1: mpya >aacbbbda19=vec128#77,<acbd17=vec128#72,<abba2=vec128#77,<aacbbbda19=vec128#80
# asm 2: mpya >aacbbbda19=$79,<acbd17=$74,<abba2=$79,<aacbbbda19=$82
mpya $79,$74,$79,$82

# qhasm: int32323232 aacbbbda20 += (acbd19 & 0xffff) * (abba1  & 0xffff)
# asm 1: mpya >aacbbbda20=vec128#80,<acbd19=vec128#59,<abba1=vec128#79,<aacbbbda20=vec128#113
# asm 2: mpya >aacbbbda20=$82,<acbd19=$61,<abba1=$81,<aacbbbda20=$115
mpya $82,$61,$81,$115

# qhasm: int32323232 aacbbbda21 += (acbd8  & 0xffff) * (abba13  & 0xffff)
# asm 1: mpya >aacbbbda21=vec128#63,<acbd8=vec128#61,<abba13=vec128#82,<aacbbbda21=vec128#63
# asm 2: mpya >aacbbbda21=$65,<acbd8=$63,<abba13=$84,<aacbbbda21=$65
mpya $65,$63,$84,$65

# qhasm: int32323232 aacbbbda22 += (acbd9  & 0xffff) * (abba13 & 0xffff)
# asm 1: mpya >aacbbbda22=vec128#5,<acbd9=vec128#68,<abba13=vec128#82,<aacbbbda22=vec128#5
# asm 2: mpya >aacbbbda22=$7,<acbd9=$70,<abba13=$84,<aacbbbda22=$7
mpya $7,$70,$84,$7

# qhasm: int32323232 aacbbbda23 += (acbd10 & 0xffff) * (abba13 & 0xffff)
# asm 1: mpya >aacbbbda23=vec128#66,<acbd10=vec128#67,<abba13=vec128#82,<aacbbbda23=vec128#66
# asm 2: mpya >aacbbbda23=$68,<acbd10=$69,<abba13=$84,<aacbbbda23=$68
mpya $68,$69,$84,$68

# qhasm: int32323232 aacbbbda24 += (acbd9  & 0xffff) * (abba15  & 0xffff)
# asm 1: mpya >aacbbbda24=vec128#113,<acbd9=vec128#68,<abba15=vec128#97,<aacbbbda24=vec128#114
# asm 2: mpya >aacbbbda24=$115,<acbd9=$70,<abba15=$99,<aacbbbda24=$116
mpya $115,$70,$99,$116

# qhasm: int32323232 aacbbbda25 += (acbd7  & 0xffff) * (abba18 & 0xffff)
# asm 1: mpya >aacbbbda25=vec128#65,<acbd7=vec128#55,<abba18=vec128#87,<aacbbbda25=vec128#65
# asm 2: mpya >aacbbbda25=$67,<acbd7=$57,<abba18=$89,<aacbbbda25=$67
mpya $67,$57,$89,$67

# qhasm: int32323232 aacbbbda19 += (acbd18 & 0xffff) * (abba1  & 0xffff)
# asm 1: mpya >aacbbbda19=vec128#77,<acbd18=vec128#73,<abba1=vec128#79,<aacbbbda19=vec128#77
# asm 2: mpya >aacbbbda19=$79,<acbd18=$75,<abba1=$81,<aacbbbda19=$79
mpya $79,$75,$81,$79

# qhasm: int32323232 aacbbbda20 <<= 1
# asm 1: shli >aacbbbda20=vec128#79,<aacbbbda20=vec128#80,1
# asm 2: shli >aacbbbda20=$81,<aacbbbda20=$82,1
shli $81,$82,1

# qhasm: int32323232 aacbbbda21 += (acbd9  & 0xffff) * (abba12  & 0xffff)
# asm 1: mpya >aacbbbda21=vec128#63,<acbd9=vec128#68,<abba12=vec128#71,<aacbbbda21=vec128#63
# asm 2: mpya >aacbbbda21=$65,<acbd9=$70,<abba12=$73,<aacbbbda21=$65
mpya $65,$70,$73,$65

# qhasm: int32323232 aacbbbda22 += (acbd10 & 0xffff) * (abba12 & 0xffff)
# asm 1: mpya >aacbbbda22=vec128#5,<acbd10=vec128#67,<abba12=vec128#71,<aacbbbda22=vec128#5
# asm 2: mpya >aacbbbda22=$7,<acbd10=$69,<abba12=$73,<aacbbbda22=$7
mpya $7,$69,$73,$7

# qhasm: int32323232 aacbbbda23 += (acbd11 & 0xffff) * (abba12 & 0xffff)
# asm 1: mpya >aacbbbda23=vec128#66,<acbd11=vec128#56,<abba12=vec128#71,<aacbbbda23=vec128#66
# asm 2: mpya >aacbbbda23=$68,<acbd11=$58,<abba12=$73,<aacbbbda23=$68
mpya $68,$58,$73,$68

# qhasm: int32323232 aacbbbda24 += (acbd10 & 0xffff) * (abba14  & 0xffff)
# asm 1: mpya >aacbbbda24=vec128#80,<acbd10=vec128#67,<abba14=vec128#86,<aacbbbda24=vec128#113
# asm 2: mpya >aacbbbda24=$82,<acbd10=$69,<abba14=$88,<aacbbbda24=$115
mpya $82,$69,$88,$115

# qhasm: int32323232 aacbbbda25 += (acbd10 & 0xffff) * (abba15 & 0xffff)
# asm 1: mpya >aacbbbda25=vec128#65,<acbd10=vec128#67,<abba15=vec128#97,<aacbbbda25=vec128#65
# asm 2: mpya >aacbbbda25=$67,<acbd10=$69,<abba15=$99,<aacbbbda25=$67
mpya $67,$69,$99,$67

# qhasm: int32323232 aacbbbda19 += (acbd19 & 0xffff) * (abba0  & 0xffff)
# asm 1: mpya >aacbbbda19=vec128#77,<acbd19=vec128#59,<abba0=vec128#78,<aacbbbda19=vec128#77
# asm 2: mpya >aacbbbda19=$79,<acbd19=$61,<abba0=$80,<aacbbbda19=$79
mpya $79,$61,$80,$79

# qhasm: int32323232 aacbbbda20 += (acbd4  & 0xffff) * (abba16 & 0xffff)
# asm 1: mpya >aacbbbda20=vec128#58,<acbd4=vec128#58,<abba16=vec128#81,<aacbbbda20=vec128#79
# asm 2: mpya >aacbbbda20=$60,<acbd4=$60,<abba16=$83,<aacbbbda20=$81
mpya $60,$60,$83,$81

# qhasm: int32323232 aacbbbda21 += (acbd12 & 0xffff) * (abba9  & 0xffff)
# asm 1: mpya >aacbbbda21=vec128#63,<acbd12=vec128#62,<abba9=vec128#89,<aacbbbda21=vec128#63
# asm 2: mpya >aacbbbda21=$65,<acbd12=$64,<abba9=$91,<aacbbbda21=$65
mpya $65,$64,$91,$65

# qhasm: int32323232 aacbbbda22 += (acbd12 & 0xffff) * (abba10 & 0xffff)
# asm 1: mpya >aacbbbda22=vec128#5,<acbd12=vec128#62,<abba10=vec128#93,<aacbbbda22=vec128#5
# asm 2: mpya >aacbbbda22=$7,<acbd12=$64,<abba10=$95,<aacbbbda22=$7
mpya $7,$64,$95,$7

# qhasm: int32323232 aacbbbda23 += (acbd12 & 0xffff) * (abba11 & 0xffff)
# asm 1: mpya >aacbbbda23=vec128#66,<acbd12=vec128#62,<abba11=vec128#95,<aacbbbda23=vec128#66
# asm 2: mpya >aacbbbda23=$68,<acbd12=$64,<abba11=$97,<aacbbbda23=$68
mpya $68,$64,$97,$68

# qhasm: int32323232 aacbbbda24 += (acbd11 & 0xffff) * (abba13  & 0xffff)
# asm 1: mpya >aacbbbda24=vec128#78,<acbd11=vec128#56,<abba13=vec128#82,<aacbbbda24=vec128#80
# asm 2: mpya >aacbbbda24=$80,<acbd11=$58,<abba13=$84,<aacbbbda24=$82
mpya $80,$58,$84,$82

# qhasm: int32323232 aacbbbda25 += (acbd11 & 0xffff) * (abba14 & 0xffff)
# asm 1: mpya >aacbbbda25=vec128#65,<acbd11=vec128#56,<abba14=vec128#86,<aacbbbda25=vec128#65
# asm 2: mpya >aacbbbda25=$67,<acbd11=$58,<abba14=$88,<aacbbbda25=$67
mpya $67,$58,$88,$67

# qhasm: int32323232 aacbbbda26  = (acbd7  & 0xffff) * (abba19 & 0xffff)
# asm 1: mpy >aacbbbda26=vec128#55,<acbd7=vec128#55,<abba19=vec128#99
# asm 2: mpy >aacbbbda26=$57,<acbd7=$57,<abba19=$101
mpy $57,$57,$101

# qhasm: int32323232 aacbbbda20 += (acbd8  & 0xffff) * (abba12 & 0xffff)
# asm 1: mpya >aacbbbda20=vec128#58,<acbd8=vec128#61,<abba12=vec128#71,<aacbbbda20=vec128#58
# asm 2: mpya >aacbbbda20=$60,<acbd8=$63,<abba12=$73,<aacbbbda20=$60
mpya $60,$63,$73,$60

# qhasm: int32323232 aacbbbda21 += (acbd13 & 0xffff) * (abba8  & 0xffff)
# asm 1: mpya >aacbbbda21=vec128#63,<acbd13=vec128#64,<abba8=vec128#85,<aacbbbda21=vec128#63
# asm 2: mpya >aacbbbda21=$65,<acbd13=$66,<abba8=$87,<aacbbbda21=$65
mpya $65,$66,$87,$65

# qhasm: int32323232 aacbbbda22 += (acbd13 & 0xffff) * (abba9  & 0xffff)
# asm 1: mpya >aacbbbda22=vec128#5,<acbd13=vec128#64,<abba9=vec128#89,<aacbbbda22=vec128#5
# asm 2: mpya >aacbbbda22=$7,<acbd13=$66,<abba9=$91,<aacbbbda22=$7
mpya $7,$66,$91,$7

# qhasm: int32323232 aacbbbda23 += (acbd13 & 0xffff) * (abba10 & 0xffff)
# asm 1: mpya >aacbbbda23=vec128#66,<acbd13=vec128#64,<abba10=vec128#93,<aacbbbda23=vec128#66
# asm 2: mpya >aacbbbda23=$68,<acbd13=$66,<abba10=$95,<aacbbbda23=$68
mpya $68,$66,$95,$68

# qhasm: int32323232 aacbbbda24 += (acbd13 & 0xffff) * (abba11  & 0xffff)
# asm 1: mpya >aacbbbda24=vec128#78,<acbd13=vec128#64,<abba11=vec128#95,<aacbbbda24=vec128#78
# asm 2: mpya >aacbbbda24=$80,<acbd13=$66,<abba11=$97,<aacbbbda24=$80
mpya $80,$66,$97,$80

# qhasm: int32323232 aacbbbda25 += (acbd14 & 0xffff) * (abba11 & 0xffff)
# asm 1: mpya >aacbbbda25=vec128#65,<acbd14=vec128#60,<abba11=vec128#95,<aacbbbda25=vec128#65
# asm 2: mpya >aacbbbda25=$67,<acbd14=$62,<abba11=$97,<aacbbbda25=$67
mpya $67,$62,$97,$67

# qhasm: int32323232 aacbbbda26 += (acbd11 & 0xffff) * (abba15 & 0xffff)
# asm 1: mpya >aacbbbda26=vec128#55,<acbd11=vec128#56,<abba15=vec128#97,<aacbbbda26=vec128#55
# asm 2: mpya >aacbbbda26=$57,<acbd11=$58,<abba15=$99,<aacbbbda26=$57
mpya $57,$58,$99,$57

# qhasm: int32323232 aacbbbda20 += (acbd12 & 0xffff) * (abba8  & 0xffff)
# asm 1: mpya >aacbbbda20=vec128#58,<acbd12=vec128#62,<abba8=vec128#85,<aacbbbda20=vec128#58
# asm 2: mpya >aacbbbda20=$60,<acbd12=$64,<abba8=$87,<aacbbbda20=$60
mpya $60,$64,$87,$60

# qhasm: int32323232 aacbbbda21 += (acbd16 & 0xffff) * (abba5  & 0xffff)
# asm 1: mpya >aacbbbda21=vec128#63,<acbd16=vec128#69,<abba5=vec128#88,<aacbbbda21=vec128#63
# asm 2: mpya >aacbbbda21=$65,<acbd16=$71,<abba5=$90,<aacbbbda21=$65
mpya $65,$71,$90,$65

# qhasm: int32323232 aacbbbda22 += (acbd14 & 0xffff) * (abba8  & 0xffff)
# asm 1: mpya >aacbbbda22=vec128#5,<acbd14=vec128#60,<abba8=vec128#85,<aacbbbda22=vec128#5
# asm 2: mpya >aacbbbda22=$7,<acbd14=$62,<abba8=$87,<aacbbbda22=$7
mpya $7,$62,$87,$7

# qhasm: int32323232 aacbbbda23 += (acbd14 & 0xffff) * (abba9  & 0xffff)
# asm 1: mpya >aacbbbda23=vec128#66,<acbd14=vec128#60,<abba9=vec128#89,<aacbbbda23=vec128#66
# asm 2: mpya >aacbbbda23=$68,<acbd14=$62,<abba9=$91,<aacbbbda23=$68
mpya $68,$62,$91,$68

# qhasm: int32323232 aacbbbda24 += (acbd14 & 0xffff) * (abba10  & 0xffff)
# asm 1: mpya >aacbbbda24=vec128#78,<acbd14=vec128#60,<abba10=vec128#93,<aacbbbda24=vec128#78
# asm 2: mpya >aacbbbda24=$80,<acbd14=$62,<abba10=$95,<aacbbbda24=$80
mpya $80,$62,$95,$80

# qhasm: int32323232 aacbbbda25 += (acbd15 & 0xffff) * (abba10 & 0xffff)
# asm 1: mpya >aacbbbda25=vec128#65,<acbd15=vec128#57,<abba10=vec128#93,<aacbbbda25=vec128#65
# asm 2: mpya >aacbbbda25=$67,<acbd15=$59,<abba10=$95,<aacbbbda25=$67
mpya $67,$59,$95,$67

# qhasm: int32323232 aacbbbda26 += (acbd15 & 0xffff) * (abba11 & 0xffff)
# asm 1: mpya >aacbbbda26=vec128#55,<acbd15=vec128#57,<abba11=vec128#95,<aacbbbda26=vec128#55
# asm 2: mpya >aacbbbda26=$57,<acbd15=$59,<abba11=$97,<aacbbbda26=$57
mpya $57,$59,$97,$57

# qhasm: int32323232 aacbbbda20 += (acbd16 & 0xffff) * (abba4  & 0xffff)
# asm 1: mpya >aacbbbda20=vec128#58,<acbd16=vec128#69,<abba4=vec128#84,<aacbbbda20=vec128#58
# asm 2: mpya >aacbbbda20=$60,<acbd16=$71,<abba4=$86,<aacbbbda20=$60
mpya $60,$71,$86,$60

# qhasm: int32323232 aacbbbda21 += (acbd17 & 0xffff) * (abba4  & 0xffff)
# asm 1: mpya >aacbbbda21=vec128#63,<acbd17=vec128#72,<abba4=vec128#84,<aacbbbda21=vec128#63
# asm 2: mpya >aacbbbda21=$65,<acbd17=$74,<abba4=$86,<aacbbbda21=$65
mpya $65,$74,$86,$65

# qhasm: int32323232 aacbbbda22 += (acbd16 & 0xffff) * (abba6  & 0xffff)
# asm 1: mpya >aacbbbda22=vec128#5,<acbd16=vec128#69,<abba6=vec128#92,<aacbbbda22=vec128#5
# asm 2: mpya >aacbbbda22=$7,<acbd16=$71,<abba6=$94,<aacbbbda22=$7
mpya $7,$71,$94,$7

# qhasm: int32323232 aacbbbda23 += (acbd15 & 0xffff) * (abba8  & 0xffff)
# asm 1: mpya >aacbbbda23=vec128#66,<acbd15=vec128#57,<abba8=vec128#85,<aacbbbda23=vec128#66
# asm 2: mpya >aacbbbda23=$68,<acbd15=$59,<abba8=$87,<aacbbbda23=$68
mpya $68,$59,$87,$68

# qhasm: int32323232 aacbbbda24 += (acbd15 & 0xffff) * (abba9  & 0xffff)
# asm 1: mpya >aacbbbda24=vec128#78,<acbd15=vec128#57,<abba9=vec128#89,<aacbbbda24=vec128#78
# asm 2: mpya >aacbbbda24=$80,<acbd15=$59,<abba9=$91,<aacbbbda24=$80
mpya $80,$59,$91,$80

# qhasm: int32323232 aacbbbda25 += (acbd18 & 0xffff) * (abba7  & 0xffff)
# asm 1: mpya >aacbbbda25=vec128#65,<acbd18=vec128#73,<abba7=vec128#91,<aacbbbda25=vec128#65
# asm 2: mpya >aacbbbda25=$67,<acbd18=$75,<abba7=$93,<aacbbbda25=$67
mpya $67,$75,$93,$67

# qhasm: int32323232 aacbbbda26 += (acbd19 & 0xffff) * (abba7  & 0xffff)
# asm 1: mpya >aacbbbda26=vec128#55,<acbd19=vec128#59,<abba7=vec128#91,<aacbbbda26=vec128#55
# asm 2: mpya >aacbbbda26=$57,<acbd19=$61,<abba7=$93,<aacbbbda26=$57
mpya $57,$61,$93,$57

# qhasm: int32323232 aacbbbda27  = (acbd8  & 0xffff) * (abba19 & 0xffff)
# asm 1: mpy >aacbbbda27=vec128#79,<acbd8=vec128#61,<abba19=vec128#99
# asm 2: mpy >aacbbbda27=$81,<acbd8=$63,<abba19=$101
mpy $81,$63,$101

# qhasm: int32323232 aacbbbda28  = (acbd9  & 0xffff) * (abba19 & 0xffff)
# asm 1: mpy >aacbbbda28=vec128#80,<acbd9=vec128#68,<abba19=vec128#99
# asm 2: mpy >aacbbbda28=$82,<acbd9=$70,<abba19=$101
mpy $82,$70,$101

# qhasm: int32323232 aacbbbda22 += (acbd17 & 0xffff) * (abba5  & 0xffff)
# asm 1: mpya >aacbbbda22=vec128#5,<acbd17=vec128#72,<abba5=vec128#88,<aacbbbda22=vec128#5
# asm 2: mpya >aacbbbda22=$7,<acbd17=$74,<abba5=$90,<aacbbbda22=$7
mpya $7,$74,$90,$7

# qhasm: int32323232 aacbbbda23 += (acbd16 & 0xffff) * (abba7  & 0xffff)
# asm 1: mpya >aacbbbda23=vec128#66,<acbd16=vec128#69,<abba7=vec128#91,<aacbbbda23=vec128#66
# asm 2: mpya >aacbbbda23=$68,<acbd16=$71,<abba7=$93,<aacbbbda23=$68
mpya $68,$71,$93,$68

# qhasm: int32323232 aacbbbda24 += (acbd17 & 0xffff) * (abba7  & 0xffff)
# asm 1: mpya >aacbbbda24=vec128#78,<acbd17=vec128#72,<abba7=vec128#91,<aacbbbda24=vec128#78
# asm 2: mpya >aacbbbda24=$80,<acbd17=$74,<abba7=$93,<aacbbbda24=$80
mpya $80,$74,$93,$80

# qhasm: int32323232 aacbbbda25 += (acbd19 & 0xffff) * (abba6  & 0xffff)
# asm 1: mpya >aacbbbda25=vec128#65,<acbd19=vec128#59,<abba6=vec128#92,<aacbbbda25=vec128#65
# asm 2: mpya >aacbbbda25=$67,<acbd19=$61,<abba6=$94,<aacbbbda25=$67
mpya $67,$61,$94,$67

# qhasm: int32323232 aacbbbda26 <<= 1
# asm 1: shli >aacbbbda26=vec128#55,<aacbbbda26=vec128#55,1
# asm 2: shli >aacbbbda26=$57,<aacbbbda26=$57,1
shli $57,$57,1

# qhasm: int32323232 aacbbbda27 += (acbd9  & 0xffff) * (abba18 & 0xffff)
# asm 1: mpya >aacbbbda27=vec128#79,<acbd9=vec128#68,<abba18=vec128#87,<aacbbbda27=vec128#79
# asm 2: mpya >aacbbbda27=$81,<acbd9=$70,<abba18=$89,<aacbbbda27=$81
mpya $81,$70,$89,$81

# qhasm: int32323232 aacbbbda28 += (acbd10 & 0xffff) * (abba18 & 0xffff)
# asm 1: mpya >aacbbbda28=vec128#80,<acbd10=vec128#67,<abba18=vec128#87,<aacbbbda28=vec128#80
# asm 2: mpya >aacbbbda28=$82,<acbd10=$69,<abba18=$89,<aacbbbda28=$82
mpya $82,$69,$89,$82

# qhasm: int32323232 aacbbbda22 += (acbd18 & 0xffff) * (abba4  & 0xffff)
# asm 1: mpya >aacbbbda22=vec128#5,<acbd18=vec128#73,<abba4=vec128#84,<aacbbbda22=vec128#5
# asm 2: mpya >aacbbbda22=$7,<acbd18=$75,<abba4=$86,<aacbbbda22=$7
mpya $7,$75,$86,$7

# qhasm: int32323232 aacbbbda23 += (acbd17 & 0xffff) * (abba6  & 0xffff)
# asm 1: mpya >aacbbbda23=vec128#66,<acbd17=vec128#72,<abba6=vec128#92,<aacbbbda23=vec128#66
# asm 2: mpya >aacbbbda23=$68,<acbd17=$74,<abba6=$94,<aacbbbda23=$68
mpya $68,$74,$94,$68

# qhasm: int32323232 aacbbbda24 += (acbd18 & 0xffff) * (abba6  & 0xffff)
# asm 1: mpya >aacbbbda24=vec128#78,<acbd18=vec128#73,<abba6=vec128#92,<aacbbbda24=vec128#78
# asm 2: mpya >aacbbbda24=$80,<acbd18=$75,<abba6=$94,<aacbbbda24=$80
mpya $80,$75,$94,$80

# qhasm: int32323232 aacbbbda25 <<= 1
# asm 1: shli >aacbbbda25=vec128#65,<aacbbbda25=vec128#65,1
# asm 2: shli >aacbbbda25=$67,<aacbbbda25=$67,1
shli $67,$67,1

# qhasm: int32323232 aacbbbda26 += (acbd8  & 0xffff) * (abba18 & 0xffff)
# asm 1: mpya >aacbbbda26=vec128#55,<acbd8=vec128#61,<abba18=vec128#87,<aacbbbda26=vec128#55
# asm 2: mpya >aacbbbda26=$57,<acbd8=$63,<abba18=$89,<aacbbbda26=$57
mpya $57,$63,$89,$57

# qhasm: int32323232 aacbbbda27 += (acbd10 & 0xffff) * (abba17 & 0xffff)
# asm 1: mpya >aacbbbda27=vec128#79,<acbd10=vec128#67,<abba17=vec128#83,<aacbbbda27=vec128#79
# asm 2: mpya >aacbbbda27=$81,<acbd10=$69,<abba17=$85,<aacbbbda27=$81
mpya $81,$69,$85,$81

# qhasm: int32323232 aacbbbda28 += (acbd11 & 0xffff) * (abba17 & 0xffff)
# asm 1: mpya >aacbbbda28=vec128#80,<acbd11=vec128#56,<abba17=vec128#83,<aacbbbda28=vec128#80
# asm 2: mpya >aacbbbda28=$82,<acbd11=$58,<abba17=$85,<aacbbbda28=$82
mpya $82,$58,$85,$82

# qhasm: int32323232 aacbbbda29  = (acbd10 & 0xffff) * (abba19 & 0xffff)
# asm 1: mpy >aacbbbda29=vec128#91,<acbd10=vec128#67,<abba19=vec128#99
# asm 2: mpy >aacbbbda29=$93,<acbd10=$69,<abba19=$101
mpy $93,$69,$101

# qhasm: int32323232 aacbbbda23 += (acbd18 & 0xffff) * (abba5  & 0xffff)
# asm 1: mpya >aacbbbda23=vec128#66,<acbd18=vec128#73,<abba5=vec128#88,<aacbbbda23=vec128#66
# asm 2: mpya >aacbbbda23=$68,<acbd18=$75,<abba5=$90,<aacbbbda23=$68
mpya $68,$75,$90,$68

# qhasm: int32323232 aacbbbda24 += (acbd19 & 0xffff) * (abba5  & 0xffff)
# asm 1: mpya >aacbbbda24=vec128#78,<acbd19=vec128#59,<abba5=vec128#88,<aacbbbda24=vec128#78
# asm 2: mpya >aacbbbda24=$80,<acbd19=$61,<abba5=$90,<aacbbbda24=$80
mpya $80,$61,$90,$80

# qhasm: int32323232 aacbbbda25 += (acbd8  & 0xffff) * (abba17 & 0xffff)
# asm 1: mpya >aacbbbda25=vec128#65,<acbd8=vec128#61,<abba17=vec128#83,<aacbbbda25=vec128#65
# asm 2: mpya >aacbbbda25=$67,<acbd8=$63,<abba17=$85,<aacbbbda25=$67
mpya $67,$63,$85,$67

# qhasm: int32323232 aacbbbda26 += (acbd9  & 0xffff) * (abba17 & 0xffff)
# asm 1: mpya >aacbbbda26=vec128#55,<acbd9=vec128#68,<abba17=vec128#83,<aacbbbda26=vec128#55
# asm 2: mpya >aacbbbda26=$57,<acbd9=$70,<abba17=$85,<aacbbbda26=$57
mpya $57,$70,$85,$57

# qhasm: int32323232 aacbbbda27 += (acbd11 & 0xffff) * (abba16 & 0xffff)
# asm 1: mpya >aacbbbda27=vec128#79,<acbd11=vec128#56,<abba16=vec128#81,<aacbbbda27=vec128#79
# asm 2: mpya >aacbbbda27=$81,<acbd11=$58,<abba16=$83,<aacbbbda27=$81
mpya $81,$58,$83,$81

# qhasm: int32323232 aacbbbda28 += (acbd13 & 0xffff) * (abba15 & 0xffff)
# asm 1: mpya >aacbbbda28=vec128#80,<acbd13=vec128#64,<abba15=vec128#97,<aacbbbda28=vec128#80
# asm 2: mpya >aacbbbda28=$82,<acbd13=$66,<abba15=$99,<aacbbbda28=$82
mpya $82,$66,$99,$82

# qhasm: int32323232 aacbbbda29 += (acbd11 & 0xffff) * (abba18 & 0xffff)
# asm 1: mpya >aacbbbda29=vec128#88,<acbd11=vec128#56,<abba18=vec128#87,<aacbbbda29=vec128#91
# asm 2: mpya >aacbbbda29=$90,<acbd11=$58,<abba18=$89,<aacbbbda29=$93
mpya $90,$58,$89,$93

# qhasm: int32323232 aacbbbda23 += (acbd19 & 0xffff) * (abba4  & 0xffff)
# asm 1: mpya >aacbbbda23=vec128#66,<acbd19=vec128#59,<abba4=vec128#84,<aacbbbda23=vec128#66
# asm 2: mpya >aacbbbda23=$68,<acbd19=$61,<abba4=$86,<aacbbbda23=$68
mpya $68,$61,$86,$68

# qhasm: int32323232 aacbbbda24 <<= 1
# asm 1: shli >aacbbbda24=vec128#78,<aacbbbda24=vec128#78,1
# asm 2: shli >aacbbbda24=$80,<aacbbbda24=$80,1
shli $80,$80,1

# qhasm: int32323232 aacbbbda25 += (acbd9  & 0xffff) * (abba16 & 0xffff)
# asm 1: mpya >aacbbbda25=vec128#65,<acbd9=vec128#68,<abba16=vec128#81,<aacbbbda25=vec128#65
# asm 2: mpya >aacbbbda25=$67,<acbd9=$70,<abba16=$83,<aacbbbda25=$67
mpya $67,$70,$83,$67

# qhasm: int32323232 aacbbbda26 += (acbd10 & 0xffff) * (abba16 & 0xffff)
# asm 1: mpya >aacbbbda26=vec128#55,<acbd10=vec128#67,<abba16=vec128#81,<aacbbbda26=vec128#55
# asm 2: mpya >aacbbbda26=$57,<acbd10=$69,<abba16=$83,<aacbbbda26=$57
mpya $57,$69,$83,$57

# qhasm: int32323232 aacbbbda27 += (acbd12 & 0xffff) * (abba15 & 0xffff)
# asm 1: mpya >aacbbbda27=vec128#67,<acbd12=vec128#62,<abba15=vec128#97,<aacbbbda27=vec128#79
# asm 2: mpya >aacbbbda27=$69,<acbd12=$64,<abba15=$99,<aacbbbda27=$81
mpya $69,$64,$99,$81

# qhasm: int32323232 aacbbbda28 += (acbd14 & 0xffff) * (abba14 & 0xffff)
# asm 1: mpya >aacbbbda28=vec128#68,<acbd14=vec128#60,<abba14=vec128#86,<aacbbbda28=vec128#80
# asm 2: mpya >aacbbbda28=$70,<acbd14=$62,<abba14=$88,<aacbbbda28=$82
mpya $70,$62,$88,$82

# qhasm: int32323232 aacbbbda29 += (acbd14 & 0xffff) * (abba15 & 0xffff)
# asm 1: mpya >aacbbbda29=vec128#79,<acbd14=vec128#60,<abba15=vec128#97,<aacbbbda29=vec128#88
# asm 2: mpya >aacbbbda29=$81,<acbd14=$62,<abba15=$99,<aacbbbda29=$90
mpya $81,$62,$99,$90

# qhasm: int32323232 aacbbbda30  = (acbd11 & 0xffff) * (abba19 & 0xffff)
# asm 1: mpy >aacbbbda30=vec128#56,<acbd11=vec128#56,<abba19=vec128#99
# asm 2: mpy >aacbbbda30=$58,<acbd11=$58,<abba19=$101
mpy $58,$58,$101

# qhasm: int32323232 aacbbbda24 += (acbd8  & 0xffff) * (abba16 & 0xffff)
# asm 1: mpya >aacbbbda24=vec128#61,<acbd8=vec128#61,<abba16=vec128#81,<aacbbbda24=vec128#78
# asm 2: mpya >aacbbbda24=$63,<acbd8=$63,<abba16=$83,<aacbbbda24=$80
mpya $63,$63,$83,$80

# qhasm: int32323232 aacbbbda25 += (acbd12 & 0xffff) * (abba13 & 0xffff)
# asm 1: mpya >aacbbbda25=vec128#65,<acbd12=vec128#62,<abba13=vec128#82,<aacbbbda25=vec128#65
# asm 2: mpya >aacbbbda25=$67,<acbd12=$64,<abba13=$84,<aacbbbda25=$67
mpya $67,$64,$84,$67

# qhasm: int32323232 aacbbbda26 += (acbd12 & 0xffff) * (abba14 & 0xffff)
# asm 1: mpya >aacbbbda26=vec128#55,<acbd12=vec128#62,<abba14=vec128#86,<aacbbbda26=vec128#55
# asm 2: mpya >aacbbbda26=$57,<acbd12=$64,<abba14=$88,<aacbbbda26=$57
mpya $57,$64,$88,$57

# qhasm: int32323232 aacbbbda27 += (acbd13 & 0xffff) * (abba14 & 0xffff)
# asm 1: mpya >aacbbbda27=vec128#67,<acbd13=vec128#64,<abba14=vec128#86,<aacbbbda27=vec128#67
# asm 2: mpya >aacbbbda27=$69,<acbd13=$66,<abba14=$88,<aacbbbda27=$69
mpya $69,$66,$88,$69

# qhasm: int32323232 aacbbbda28 += (acbd15 & 0xffff) * (abba13 & 0xffff)
# asm 1: mpya >aacbbbda28=vec128#68,<acbd15=vec128#57,<abba13=vec128#82,<aacbbbda28=vec128#68
# asm 2: mpya >aacbbbda28=$70,<acbd15=$59,<abba13=$84,<aacbbbda28=$70
mpya $70,$59,$84,$70

# qhasm: int32323232 aacbbbda29 += (acbd15 & 0xffff) * (abba14 & 0xffff)
# asm 1: mpya >aacbbbda29=vec128#78,<acbd15=vec128#57,<abba14=vec128#86,<aacbbbda29=vec128#79
# asm 2: mpya >aacbbbda29=$80,<acbd15=$59,<abba14=$88,<aacbbbda29=$81
mpya $80,$59,$88,$81

# qhasm: int32323232 aacbbbda30 += (acbd15 & 0xffff) * (abba15 & 0xffff)
# asm 1: mpya >aacbbbda30=vec128#56,<acbd15=vec128#57,<abba15=vec128#97,<aacbbbda30=vec128#56
# asm 2: mpya >aacbbbda30=$58,<acbd15=$59,<abba15=$99,<aacbbbda30=$58
mpya $58,$59,$99,$58

# qhasm: int32323232 aacbbbda24 += (acbd12 & 0xffff) * (abba12 & 0xffff)
# asm 1: mpya >aacbbbda24=vec128#61,<acbd12=vec128#62,<abba12=vec128#71,<aacbbbda24=vec128#61
# asm 2: mpya >aacbbbda24=$63,<acbd12=$64,<abba12=$73,<aacbbbda24=$63
mpya $63,$64,$73,$63

# qhasm: int32323232 aacbbbda25 += (acbd13 & 0xffff) * (abba12 & 0xffff)
# asm 1: mpya >aacbbbda25=vec128#65,<acbd13=vec128#64,<abba12=vec128#71,<aacbbbda25=vec128#65
# asm 2: mpya >aacbbbda25=$67,<acbd13=$66,<abba12=$73,<aacbbbda25=$67
mpya $67,$66,$73,$67

# qhasm: int32323232 aacbbbda26 += (acbd13 & 0xffff) * (abba13 & 0xffff)
# asm 1: mpya >aacbbbda26=vec128#55,<acbd13=vec128#64,<abba13=vec128#82,<aacbbbda26=vec128#55
# asm 2: mpya >aacbbbda26=$57,<acbd13=$66,<abba13=$84,<aacbbbda26=$57
mpya $57,$66,$84,$57

# qhasm: int32323232 aacbbbda27 += (acbd14 & 0xffff) * (abba13 & 0xffff)
# asm 1: mpya >aacbbbda27=vec128#67,<acbd14=vec128#60,<abba13=vec128#82,<aacbbbda27=vec128#67
# asm 2: mpya >aacbbbda27=$69,<acbd14=$62,<abba13=$84,<aacbbbda27=$69
mpya $69,$62,$84,$69

# qhasm: int32323232 aacbbbda28 += (acbd17 & 0xffff) * (abba11 & 0xffff)
# asm 1: mpya >aacbbbda28=vec128#68,<acbd17=vec128#72,<abba11=vec128#95,<aacbbbda28=vec128#68
# asm 2: mpya >aacbbbda28=$70,<acbd17=$74,<abba11=$97,<aacbbbda28=$70
mpya $70,$74,$97,$70

# qhasm: int32323232 aacbbbda29 += (acbd18 & 0xffff) * (abba11 & 0xffff)
# asm 1: mpya >aacbbbda29=vec128#78,<acbd18=vec128#73,<abba11=vec128#95,<aacbbbda29=vec128#78
# asm 2: mpya >aacbbbda29=$80,<acbd18=$75,<abba11=$97,<aacbbbda29=$80
mpya $80,$75,$97,$80

# qhasm: int32323232 aacbbbda30 += (acbd19 & 0xffff) * (abba11 & 0xffff)
# asm 1: mpya >aacbbbda30=vec128#56,<acbd19=vec128#59,<abba11=vec128#95,<aacbbbda30=vec128#56
# asm 2: mpya >aacbbbda30=$58,<acbd19=$61,<abba11=$97,<aacbbbda30=$58
mpya $58,$61,$97,$58

# qhasm: int32323232 aacbbbda24 += (acbd16 & 0xffff) * (abba8  & 0xffff)
# asm 1: mpya >aacbbbda24=vec128#61,<acbd16=vec128#69,<abba8=vec128#85,<aacbbbda24=vec128#61
# asm 2: mpya >aacbbbda24=$63,<acbd16=$71,<abba8=$87,<aacbbbda24=$63
mpya $63,$71,$87,$63

# qhasm: int32323232 aacbbbda25 += (acbd16 & 0xffff) * (abba9  & 0xffff)
# asm 1: mpya >aacbbbda25=vec128#65,<acbd16=vec128#69,<abba9=vec128#89,<aacbbbda25=vec128#65
# asm 2: mpya >aacbbbda25=$67,<acbd16=$71,<abba9=$91,<aacbbbda25=$67
mpya $67,$71,$91,$67

# qhasm: int32323232 aacbbbda26 += (acbd14 & 0xffff) * (abba12 & 0xffff)
# asm 1: mpya >aacbbbda26=vec128#55,<acbd14=vec128#60,<abba12=vec128#71,<aacbbbda26=vec128#55
# asm 2: mpya >aacbbbda26=$57,<acbd14=$62,<abba12=$73,<aacbbbda26=$57
mpya $57,$62,$73,$57

# qhasm: int32323232 aacbbbda27 += (acbd15 & 0xffff) * (abba12 & 0xffff)
# asm 1: mpya >aacbbbda27=vec128#67,<acbd15=vec128#57,<abba12=vec128#71,<aacbbbda27=vec128#67
# asm 2: mpya >aacbbbda27=$69,<acbd15=$59,<abba12=$73,<aacbbbda27=$69
mpya $69,$59,$73,$69

# qhasm: int32323232 aacbbbda28 += (acbd18 & 0xffff) * (abba10 & 0xffff)
# asm 1: mpya >aacbbbda28=vec128#68,<acbd18=vec128#73,<abba10=vec128#93,<aacbbbda28=vec128#68
# asm 2: mpya >aacbbbda28=$70,<acbd18=$75,<abba10=$95,<aacbbbda28=$70
mpya $70,$75,$95,$70

# qhasm: int32323232 aacbbbda29 += (acbd19 & 0xffff) * (abba10 & 0xffff)
# asm 1: mpya >aacbbbda29=vec128#78,<acbd19=vec128#59,<abba10=vec128#93,<aacbbbda29=vec128#78
# asm 2: mpya >aacbbbda29=$80,<acbd19=$61,<abba10=$95,<aacbbbda29=$80
mpya $80,$61,$95,$80

# qhasm: int32323232 aacbbbda30 <<= 1
# asm 1: shli >aacbbbda30=vec128#56,<aacbbbda30=vec128#56,1
# asm 2: shli >aacbbbda30=$58,<aacbbbda30=$58,1
shli $58,$58,1

# qhasm: int32323232 aacbbbda31  = (acbd12 & 0xffff) * (abba19 & 0xffff)
# asm 1: mpy >aacbbbda31=vec128#79,<acbd12=vec128#62,<abba19=vec128#99
# asm 2: mpy >aacbbbda31=$81,<acbd12=$64,<abba19=$101
mpy $81,$64,$101

# qhasm: int32323232 aacbbbda25 += (acbd17 & 0xffff) * (abba8  & 0xffff)
# asm 1: mpya >aacbbbda25=vec128#65,<acbd17=vec128#72,<abba8=vec128#85,<aacbbbda25=vec128#65
# asm 2: mpya >aacbbbda25=$67,<acbd17=$74,<abba8=$87,<aacbbbda25=$67
mpya $67,$74,$87,$67

# qhasm: int32323232 aacbbbda26 += (acbd16 & 0xffff) * (abba10 & 0xffff)
# asm 1: mpya >aacbbbda26=vec128#55,<acbd16=vec128#69,<abba10=vec128#93,<aacbbbda26=vec128#55
# asm 2: mpya >aacbbbda26=$57,<acbd16=$71,<abba10=$95,<aacbbbda26=$57
mpya $57,$71,$95,$57

# qhasm: int32323232 aacbbbda27 += (acbd16 & 0xffff) * (abba11 & 0xffff)
# asm 1: mpya >aacbbbda27=vec128#67,<acbd16=vec128#69,<abba11=vec128#95,<aacbbbda27=vec128#67
# asm 2: mpya >aacbbbda27=$69,<acbd16=$71,<abba11=$97,<aacbbbda27=$69
mpya $69,$71,$97,$69

# qhasm: int32323232 aacbbbda28 += (acbd19 & 0xffff) * (abba9  & 0xffff)
# asm 1: mpya >aacbbbda28=vec128#68,<acbd19=vec128#59,<abba9=vec128#89,<aacbbbda28=vec128#68
# asm 2: mpya >aacbbbda28=$70,<acbd19=$61,<abba9=$91,<aacbbbda28=$70
mpya $70,$61,$91,$70

# qhasm: int32323232 aacbbbda29 <<= 1
# asm 1: shli >aacbbbda29=vec128#78,<aacbbbda29=vec128#78,1
# asm 2: shli >aacbbbda29=$80,<aacbbbda29=$80,1
shli $80,$80,1

# qhasm: int32323232 aacbbbda30 += (acbd12 & 0xffff) * (abba18 & 0xffff)
# asm 1: mpya >aacbbbda30=vec128#56,<acbd12=vec128#62,<abba18=vec128#87,<aacbbbda30=vec128#56
# asm 2: mpya >aacbbbda30=$58,<acbd12=$64,<abba18=$89,<aacbbbda30=$58
mpya $58,$64,$89,$58

# qhasm: int32323232 aacbbbda31 += (acbd13 & 0xffff) * (abba18 & 0xffff)
# asm 1: mpya >aacbbbda31=vec128#79,<acbd13=vec128#64,<abba18=vec128#87,<aacbbbda31=vec128#79
# asm 2: mpya >aacbbbda31=$81,<acbd13=$66,<abba18=$89,<aacbbbda31=$81
mpya $81,$66,$89,$81

# qhasm: int32323232 aacbbbda32  = (acbd13 & 0xffff) * (abba19 & 0xffff)
# asm 1: mpy >aacbbbda32=vec128#80,<acbd13=vec128#64,<abba19=vec128#99
# asm 2: mpy >aacbbbda32=$82,<acbd13=$66,<abba19=$101
mpy $82,$66,$101

# qhasm: int32323232 aacbbbda26 += (acbd17 & 0xffff) * (abba9  & 0xffff)
# asm 1: mpya >aacbbbda26=vec128#55,<acbd17=vec128#72,<abba9=vec128#89,<aacbbbda26=vec128#55
# asm 2: mpya >aacbbbda26=$57,<acbd17=$74,<abba9=$91,<aacbbbda26=$57
mpya $57,$74,$91,$57

# qhasm: int32323232 aacbbbda27 += (acbd17 & 0xffff) * (abba10 & 0xffff)
# asm 1: mpya >aacbbbda27=vec128#67,<acbd17=vec128#72,<abba10=vec128#93,<aacbbbda27=vec128#67
# asm 2: mpya >aacbbbda27=$69,<acbd17=$74,<abba10=$95,<aacbbbda27=$69
mpya $69,$74,$95,$69

# qhasm: int32323232 aacbbbda28 <<= 1
# asm 1: shli >aacbbbda28=vec128#68,<aacbbbda28=vec128#68,1
# asm 2: shli >aacbbbda28=$70,<aacbbbda28=$70,1
shli $70,$70,1

# qhasm: int32323232 aacbbbda29 += (acbd12 & 0xffff) * (abba17 & 0xffff)
# asm 1: mpya >aacbbbda29=vec128#78,<acbd12=vec128#62,<abba17=vec128#83,<aacbbbda29=vec128#78
# asm 2: mpya >aacbbbda29=$80,<acbd12=$64,<abba17=$85,<aacbbbda29=$80
mpya $80,$64,$85,$80

# qhasm: int32323232 aacbbbda30 += (acbd13 & 0xffff) * (abba17 & 0xffff)
# asm 1: mpya >aacbbbda30=vec128#56,<acbd13=vec128#64,<abba17=vec128#83,<aacbbbda30=vec128#56
# asm 2: mpya >aacbbbda30=$58,<acbd13=$66,<abba17=$85,<aacbbbda30=$58
mpya $58,$66,$85,$58

# qhasm: int32323232 aacbbbda31 += (acbd14 & 0xffff) * (abba17 & 0xffff)
# asm 1: mpya >aacbbbda31=vec128#79,<acbd14=vec128#60,<abba17=vec128#83,<aacbbbda31=vec128#79
# asm 2: mpya >aacbbbda31=$81,<acbd14=$62,<abba17=$85,<aacbbbda31=$81
mpya $81,$62,$85,$81

# qhasm: int32323232 aacbbbda32 += (acbd14 & 0xffff) * (abba18 & 0xffff)
# asm 1: mpya >aacbbbda32=vec128#80,<acbd14=vec128#60,<abba18=vec128#87,<aacbbbda32=vec128#80
# asm 2: mpya >aacbbbda32=$82,<acbd14=$62,<abba18=$89,<aacbbbda32=$82
mpya $82,$62,$89,$82

# qhasm: int32323232 aacbbbda26 += (acbd18 & 0xffff) * (abba8  & 0xffff)
# asm 1: mpya >aacbbbda26=vec128#55,<acbd18=vec128#73,<abba8=vec128#85,<aacbbbda26=vec128#55
# asm 2: mpya >aacbbbda26=$57,<acbd18=$75,<abba8=$87,<aacbbbda26=$57
mpya $57,$75,$87,$57

# qhasm: int32323232 aacbbbda27 += (acbd18 & 0xffff) * (abba9  & 0xffff)
# asm 1: mpya >aacbbbda27=vec128#67,<acbd18=vec128#73,<abba9=vec128#89,<aacbbbda27=vec128#67
# asm 2: mpya >aacbbbda27=$69,<acbd18=$75,<abba9=$91,<aacbbbda27=$69
mpya $69,$75,$91,$69

# qhasm: int32323232 aacbbbda28 += (acbd12 & 0xffff) * (abba16 & 0xffff)
# asm 1: mpya >aacbbbda28=vec128#62,<acbd12=vec128#62,<abba16=vec128#81,<aacbbbda28=vec128#68
# asm 2: mpya >aacbbbda28=$64,<acbd12=$64,<abba16=$83,<aacbbbda28=$70
mpya $64,$64,$83,$70

# qhasm: int32323232 aacbbbda29 += (acbd13 & 0xffff) * (abba16 & 0xffff)
# asm 1: mpya >aacbbbda29=vec128#64,<acbd13=vec128#64,<abba16=vec128#81,<aacbbbda29=vec128#78
# asm 2: mpya >aacbbbda29=$66,<acbd13=$66,<abba16=$83,<aacbbbda29=$80
mpya $66,$66,$83,$80

# qhasm: int32323232 aacbbbda30 += (acbd14 & 0xffff) * (abba16 & 0xffff)
# asm 1: mpya >aacbbbda30=vec128#56,<acbd14=vec128#60,<abba16=vec128#81,<aacbbbda30=vec128#56
# asm 2: mpya >aacbbbda30=$58,<acbd14=$62,<abba16=$83,<aacbbbda30=$58
mpya $58,$62,$83,$58

# qhasm: int32323232 aacbbbda31 += (acbd15 & 0xffff) * (abba16 & 0xffff)
# asm 1: mpya >aacbbbda31=vec128#68,<acbd15=vec128#57,<abba16=vec128#81,<aacbbbda31=vec128#79
# asm 2: mpya >aacbbbda31=$70,<acbd15=$59,<abba16=$83,<aacbbbda31=$81
mpya $70,$59,$83,$81

# qhasm: int32323232 aacbbbda32 += (acbd15 & 0xffff) * (abba17 & 0xffff)
# asm 1: mpya >aacbbbda32=vec128#78,<acbd15=vec128#57,<abba17=vec128#83,<aacbbbda32=vec128#80
# asm 2: mpya >aacbbbda32=$80,<acbd15=$59,<abba17=$85,<aacbbbda32=$82
mpya $80,$59,$85,$82

# qhasm: int32323232 aacbbbda33  = (acbd14 & 0xffff) * (abba19 & 0xffff)
# asm 1: mpy >aacbbbda33=vec128#60,<acbd14=vec128#60,<abba19=vec128#99
# asm 2: mpy >aacbbbda33=$62,<acbd14=$62,<abba19=$101
mpy $62,$62,$101

# qhasm: int32323232 aacbbbda27 += (acbd19 & 0xffff) * (abba8  & 0xffff)
# asm 1: mpya >aacbbbda27=vec128#67,<acbd19=vec128#59,<abba8=vec128#85,<aacbbbda27=vec128#67
# asm 2: mpya >aacbbbda27=$69,<acbd19=$61,<abba8=$87,<aacbbbda27=$69
mpya $69,$61,$87,$69

# qhasm: int32323232 aacbbbda28 += (acbd16 & 0xffff) * (abba12 & 0xffff)
# asm 1: mpya >aacbbbda28=vec128#62,<acbd16=vec128#69,<abba12=vec128#71,<aacbbbda28=vec128#62
# asm 2: mpya >aacbbbda28=$64,<acbd16=$71,<abba12=$73,<aacbbbda28=$64
mpya $64,$71,$73,$64

# qhasm: int32323232 aacbbbda29 += (acbd16 & 0xffff) * (abba13 & 0xffff)
# asm 1: mpya >aacbbbda29=vec128#64,<acbd16=vec128#69,<abba13=vec128#82,<aacbbbda29=vec128#64
# asm 2: mpya >aacbbbda29=$66,<acbd16=$71,<abba13=$84,<aacbbbda29=$66
mpya $66,$71,$84,$66

# qhasm: int32323232 aacbbbda30 += (acbd16 & 0xffff) * (abba14 & 0xffff)
# asm 1: mpya >aacbbbda30=vec128#56,<acbd16=vec128#69,<abba14=vec128#86,<aacbbbda30=vec128#56
# asm 2: mpya >aacbbbda30=$58,<acbd16=$71,<abba14=$88,<aacbbbda30=$58
mpya $58,$71,$88,$58

# qhasm: int32323232 aacbbbda31 += (acbd16 & 0xffff) * (abba15 & 0xffff)
# asm 1: mpya >aacbbbda31=vec128#68,<acbd16=vec128#69,<abba15=vec128#97,<aacbbbda31=vec128#68
# asm 2: mpya >aacbbbda31=$70,<acbd16=$71,<abba15=$99,<aacbbbda31=$70
mpya $70,$71,$99,$70

# qhasm: int32323232 aacbbbda32 += (acbd17 & 0xffff) * (abba15 & 0xffff)
# asm 1: mpya >aacbbbda32=vec128#78,<acbd17=vec128#72,<abba15=vec128#97,<aacbbbda32=vec128#78
# asm 2: mpya >aacbbbda32=$80,<acbd17=$74,<abba15=$99,<aacbbbda32=$80
mpya $80,$74,$99,$80

# qhasm: int32323232 aacbbbda33 += (acbd15 & 0xffff) * (abba18 & 0xffff)
# asm 1: mpya >aacbbbda33=vec128#60,<acbd15=vec128#57,<abba18=vec128#87,<aacbbbda33=vec128#60
# asm 2: mpya >aacbbbda33=$62,<acbd15=$59,<abba18=$89,<aacbbbda33=$62
mpya $62,$59,$89,$62

# qhasm: int32323232 aacbbbda34 = (acbd15 & 0xffff) * (abba19 & 0xffff)
# asm 1: mpy >aacbbbda34=vec128#57,<acbd15=vec128#57,<abba19=vec128#99
# asm 2: mpy >aacbbbda34=$59,<acbd15=$59,<abba19=$101
mpy $59,$59,$101

# qhasm: int32323232 aacbbbda35 = (acbd16 & 0xffff) * (abba19 & 0xffff)
# asm 1: mpy >aacbbbda35=vec128#79,<acbd16=vec128#69,<abba19=vec128#99
# asm 2: mpy >aacbbbda35=$81,<acbd16=$71,<abba19=$101
mpy $81,$71,$101

# qhasm: int32323232 aacbbbda29 += (acbd17 & 0xffff) * (abba12 & 0xffff)
# asm 1: mpya >aacbbbda29=vec128#64,<acbd17=vec128#72,<abba12=vec128#71,<aacbbbda29=vec128#64
# asm 2: mpya >aacbbbda29=$66,<acbd17=$74,<abba12=$73,<aacbbbda29=$66
mpya $66,$74,$73,$66

# qhasm: int32323232 aacbbbda30 += (acbd17 & 0xffff) * (abba13 & 0xffff)
# asm 1: mpya >aacbbbda30=vec128#56,<acbd17=vec128#72,<abba13=vec128#82,<aacbbbda30=vec128#56
# asm 2: mpya >aacbbbda30=$58,<acbd17=$74,<abba13=$84,<aacbbbda30=$58
mpya $58,$74,$84,$58

# qhasm: int32323232 aacbbbda31 += (acbd17 & 0xffff) * (abba14 & 0xffff)
# asm 1: mpya >aacbbbda31=vec128#68,<acbd17=vec128#72,<abba14=vec128#86,<aacbbbda31=vec128#68
# asm 2: mpya >aacbbbda31=$70,<acbd17=$74,<abba14=$88,<aacbbbda31=$70
mpya $70,$74,$88,$70

# qhasm: int32323232 aacbbbda32 += (acbd18 & 0xffff) * (abba14 & 0xffff)
# asm 1: mpya >aacbbbda32=vec128#78,<acbd18=vec128#73,<abba14=vec128#86,<aacbbbda32=vec128#78
# asm 2: mpya >aacbbbda32=$80,<acbd18=$75,<abba14=$88,<aacbbbda32=$80
mpya $80,$75,$88,$80

# qhasm: int32323232 aacbbbda33 += (acbd18 & 0xffff) * (abba15 & 0xffff)
# asm 1: mpya >aacbbbda33=vec128#60,<acbd18=vec128#73,<abba15=vec128#97,<aacbbbda33=vec128#60
# asm 2: mpya >aacbbbda33=$62,<acbd18=$75,<abba15=$99,<aacbbbda33=$62
mpya $62,$75,$99,$62

# qhasm: int32323232 aacbbbda34 += (acbd19 & 0xffff) * (abba15 & 0xffff)
# asm 1: mpya >aacbbbda34=vec128#57,<acbd19=vec128#59,<abba15=vec128#97,<aacbbbda34=vec128#57
# asm 2: mpya >aacbbbda34=$59,<acbd19=$61,<abba15=$99,<aacbbbda34=$59
mpya $59,$61,$99,$59

# qhasm: int32323232 aacbbbda35 += (acbd17 & 0xffff) * (abba18 & 0xffff)
# asm 1: mpya >aacbbbda35=vec128#79,<acbd17=vec128#72,<abba18=vec128#87,<aacbbbda35=vec128#79
# asm 2: mpya >aacbbbda35=$81,<acbd17=$74,<abba18=$89,<aacbbbda35=$81
mpya $81,$74,$89,$81

# qhasm: int32323232 aacbbbda36 = (acbd17 & 0xffff) * (abba19 & 0xffff)
# asm 1: mpy >aacbbbda36=vec128#80,<acbd17=vec128#72,<abba19=vec128#99
# asm 2: mpy >aacbbbda36=$82,<acbd17=$74,<abba19=$101
mpy $82,$74,$101

# qhasm: int32323232 aacbbbda30 += (acbd18 & 0xffff) * (abba12 & 0xffff)
# asm 1: mpya >aacbbbda30=vec128#56,<acbd18=vec128#73,<abba12=vec128#71,<aacbbbda30=vec128#56
# asm 2: mpya >aacbbbda30=$58,<acbd18=$75,<abba12=$73,<aacbbbda30=$58
mpya $58,$75,$73,$58

# qhasm: int32323232 aacbbbda31 += (acbd18 & 0xffff) * (abba13 & 0xffff)
# asm 1: mpya >aacbbbda31=vec128#68,<acbd18=vec128#73,<abba13=vec128#82,<aacbbbda31=vec128#68
# asm 2: mpya >aacbbbda31=$70,<acbd18=$75,<abba13=$84,<aacbbbda31=$70
mpya $70,$75,$84,$70

# qhasm: int32323232 aacbbbda32 += (acbd19 & 0xffff) * (abba13 & 0xffff)
# asm 1: mpya >aacbbbda32=vec128#78,<acbd19=vec128#59,<abba13=vec128#82,<aacbbbda32=vec128#78
# asm 2: mpya >aacbbbda32=$80,<acbd19=$61,<abba13=$84,<aacbbbda32=$80
mpya $80,$61,$84,$80

# qhasm: int32323232 aacbbbda33 += (acbd19 & 0xffff) * (abba14 & 0xffff)
# asm 1: mpya >aacbbbda33=vec128#60,<acbd19=vec128#59,<abba14=vec128#86,<aacbbbda33=vec128#60
# asm 2: mpya >aacbbbda33=$62,<acbd19=$61,<abba14=$88,<aacbbbda33=$62
mpya $62,$61,$88,$62

# qhasm: int32323232 aacbbbda34 <<= 1
# asm 1: shli >aacbbbda34=vec128#57,<aacbbbda34=vec128#57,1
# asm 2: shli >aacbbbda34=$59,<aacbbbda34=$59,1
shli $59,$59,1

# qhasm: int32323232 aacbbbda35 += (acbd18 & 0xffff) * (abba17 & 0xffff)
# asm 1: mpya >aacbbbda35=vec128#79,<acbd18=vec128#73,<abba17=vec128#83,<aacbbbda35=vec128#79
# asm 2: mpya >aacbbbda35=$81,<acbd18=$75,<abba17=$85,<aacbbbda35=$81
mpya $81,$75,$85,$81

# qhasm: int32323232 aacbbbda36 += (acbd18 & 0xffff) * (abba18 & 0xffff)
# asm 1: mpya >aacbbbda36=vec128#80,<acbd18=vec128#73,<abba18=vec128#87,<aacbbbda36=vec128#80
# asm 2: mpya >aacbbbda36=$82,<acbd18=$75,<abba18=$89,<aacbbbda36=$82
mpya $82,$75,$89,$82

# qhasm: int32323232 aacbbbda37 = (acbd18 & 0xffff) * (abba19 & 0xffff)
# asm 1: mpy >aacbbbda37=vec128#82,<acbd18=vec128#73,<abba19=vec128#99
# asm 2: mpy >aacbbbda37=$84,<acbd18=$75,<abba19=$101
mpy $84,$75,$101

# qhasm: int32323232 aacbbbda31 += (acbd19 & 0xffff) * (abba12 & 0xffff)
# asm 1: mpya >aacbbbda31=vec128#68,<acbd19=vec128#59,<abba12=vec128#71,<aacbbbda31=vec128#68
# asm 2: mpya >aacbbbda31=$70,<acbd19=$61,<abba12=$73,<aacbbbda31=$70
mpya $70,$61,$73,$70

# qhasm: int32323232 aacbbbda32 <<= 1
# asm 1: shli >aacbbbda32=vec128#71,<aacbbbda32=vec128#78,1
# asm 2: shli >aacbbbda32=$73,<aacbbbda32=$80,1
shli $73,$80,1

# qhasm: int32323232 aacbbbda33 <<= 1
# asm 1: shli >aacbbbda33=vec128#60,<aacbbbda33=vec128#60,1
# asm 2: shli >aacbbbda33=$62,<aacbbbda33=$62,1
shli $62,$62,1

# qhasm: int32323232 aacbbbda34 += (acbd16 & 0xffff) * (abba18 & 0xffff)
# asm 1: mpya >aacbbbda34=vec128#57,<acbd16=vec128#69,<abba18=vec128#87,<aacbbbda34=vec128#57
# asm 2: mpya >aacbbbda34=$59,<acbd16=$71,<abba18=$89,<aacbbbda34=$59
mpya $59,$71,$89,$59

# qhasm: int32323232 aacbbbda35 += (acbd19 & 0xffff) * (abba16 & 0xffff)
# asm 1: mpya >aacbbbda35=vec128#78,<acbd19=vec128#59,<abba16=vec128#81,<aacbbbda35=vec128#79
# asm 2: mpya >aacbbbda35=$80,<acbd19=$61,<abba16=$83,<aacbbbda35=$81
mpya $80,$61,$83,$81

# qhasm: int32323232 aacbbbda36 += (acbd19 & 0xffff) * (abba17 & 0xffff)
# asm 1: mpya >aacbbbda36=vec128#79,<acbd19=vec128#59,<abba17=vec128#83,<aacbbbda36=vec128#80
# asm 2: mpya >aacbbbda36=$81,<acbd19=$61,<abba17=$85,<aacbbbda36=$82
mpya $81,$61,$85,$82

# qhasm: int32323232 aacbbbda37 += (acbd19 & 0xffff) * (abba18 & 0xffff)
# asm 1: mpya >aacbbbda37=vec128#80,<acbd19=vec128#59,<abba18=vec128#87,<aacbbbda37=vec128#82
# asm 2: mpya >aacbbbda37=$82,<acbd19=$61,<abba18=$89,<aacbbbda37=$84
mpya $82,$61,$89,$84

# qhasm: int32323232 aacbbbda38 = (acbd19 & 0xffff) * (abba19 & 0xffff)
# asm 1: mpy >aacbbbda38=vec128#59,<acbd19=vec128#59,<abba19=vec128#99
# asm 2: mpy >aacbbbda38=$61,<acbd19=$61,<abba19=$101
mpy $61,$61,$101

# qhasm: int32323232 aacbbbda32 += (acbd16 & 0xffff) * (abba16 & 0xffff)
# asm 1: mpya >aacbbbda32=vec128#71,<acbd16=vec128#69,<abba16=vec128#81,<aacbbbda32=vec128#71
# asm 2: mpya >aacbbbda32=$73,<acbd16=$71,<abba16=$83,<aacbbbda32=$73
mpya $73,$71,$83,$73

# qhasm: int32323232 aacbbbda33 += (acbd16 & 0xffff) * (abba17 & 0xffff)
# asm 1: mpya >aacbbbda33=vec128#60,<acbd16=vec128#69,<abba17=vec128#83,<aacbbbda33=vec128#60
# asm 2: mpya >aacbbbda33=$62,<acbd16=$71,<abba17=$85,<aacbbbda33=$62
mpya $62,$71,$85,$62

# qhasm: int32323232 aacbbbda34 += (acbd17 & 0xffff) * (abba17 & 0xffff)
# asm 1: mpya >aacbbbda34=vec128#57,<acbd17=vec128#72,<abba17=vec128#83,<aacbbbda34=vec128#57
# asm 2: mpya >aacbbbda34=$59,<acbd17=$74,<abba17=$85,<aacbbbda34=$59
mpya $59,$74,$85,$59

# qhasm: int32323232 aacbbbda36 <<= 1
# asm 1: shli >aacbbbda36=vec128#69,<aacbbbda36=vec128#79,1
# asm 2: shli >aacbbbda36=$71,<aacbbbda36=$81,1
shli $71,$81,1

# qhasm: int32323232 aacbbbda37 <<= 1
# asm 1: shli >aacbbbda37=vec128#79,<aacbbbda37=vec128#80,1
# asm 2: shli >aacbbbda37=$81,<aacbbbda37=$82,1
shli $81,$82,1

# qhasm: int32323232 aacbbbda38 <<= 1
# asm 1: shli >aacbbbda38=vec128#59,<aacbbbda38=vec128#59,1
# asm 2: shli >aacbbbda38=$61,<aacbbbda38=$61,1
shli $61,$61,1

# qhasm: int32323232 aacbbbda33 += (acbd17 & 0xffff) * (abba16 & 0xffff)
# asm 1: mpya >aacbbbda33=vec128#60,<acbd17=vec128#72,<abba16=vec128#81,<aacbbbda33=vec128#60
# asm 2: mpya >aacbbbda33=$62,<acbd17=$74,<abba16=$83,<aacbbbda33=$62
mpya $62,$74,$83,$62

# qhasm: int32323232 aacbbbda34 += (acbd18 & 0xffff) * (abba16 & 0xffff)
# asm 1: mpya >aacbbbda34=vec128#57,<acbd18=vec128#73,<abba16=vec128#81,<aacbbbda34=vec128#57
# asm 2: mpya >aacbbbda34=$59,<acbd18=$75,<abba16=$83,<aacbbbda34=$59
mpya $59,$75,$83,$59

# qhasm: uint32323232 carry0 = aacbbbda20 >> 13
# asm 1: rotmi >carry0=vec128#72,<aacbbbda20=vec128#58,-13
# asm 2: rotmi >carry0=$74,<aacbbbda20=$60,-13
rotmi $74,$60,-13

# qhasm: uint32323232 carry1 = aacbbbda24 >> 13
# asm 1: rotmi >carry1=vec128#73,<aacbbbda24=vec128#61,-13
# asm 2: rotmi >carry1=$75,<aacbbbda24=$63,-13
rotmi $75,$63,-13

# qhasm: uint32323232 carry2 = aacbbbda28 >> 13
# asm 1: rotmi >carry2=vec128#80,<aacbbbda28=vec128#62,-13
# asm 2: rotmi >carry2=$82,<aacbbbda28=$64,-13
rotmi $82,$64,-13

# qhasm: uint32323232 carry3 = aacbbbda32 >> 13
# asm 1: rotmi >carry3=vec128#81,<aacbbbda32=vec128#71,-13
# asm 2: rotmi >carry3=$83,<aacbbbda32=$73,-13
rotmi $83,$73,-13

# qhasm: int32323232 aacbbbda21 += carry0
# asm 1: a >aacbbbda21=vec128#63,<aacbbbda21=vec128#63,<carry0=vec128#72
# asm 2: a >aacbbbda21=$65,<aacbbbda21=$65,<carry0=$74
a $65,$65,$74

# qhasm: aacbbbda20 &= mask13
# asm 1: and >aacbbbda20=vec128#58,<aacbbbda20=vec128#58,<mask13=vec128#22
# asm 2: and >aacbbbda20=$60,<aacbbbda20=$60,<mask13=$24
and $60,$60,$24

# qhasm: int32323232 aacbbbda25 += carry1
# asm 1: a >aacbbbda25=vec128#65,<aacbbbda25=vec128#65,<carry1=vec128#73
# asm 2: a >aacbbbda25=$67,<aacbbbda25=$67,<carry1=$75
a $67,$67,$75

# qhasm: aacbbbda24 &= mask13
# asm 1: and >aacbbbda24=vec128#61,<aacbbbda24=vec128#61,<mask13=vec128#22
# asm 2: and >aacbbbda24=$63,<aacbbbda24=$63,<mask13=$24
and $63,$63,$24

# qhasm: int32323232 aacbbbda29 += carry2
# asm 1: a >aacbbbda29=vec128#64,<aacbbbda29=vec128#64,<carry2=vec128#80
# asm 2: a >aacbbbda29=$66,<aacbbbda29=$66,<carry2=$82
a $66,$66,$82

# qhasm: aacbbbda28 &= mask13
# asm 1: and >aacbbbda28=vec128#62,<aacbbbda28=vec128#62,<mask13=vec128#22
# asm 2: and >aacbbbda28=$64,<aacbbbda28=$64,<mask13=$24
and $64,$64,$24

# qhasm: int32323232 aacbbbda33 += carry3
# asm 1: a >aacbbbda33=vec128#60,<aacbbbda33=vec128#60,<carry3=vec128#81
# asm 2: a >aacbbbda33=$62,<aacbbbda33=$62,<carry3=$83
a $62,$62,$83

# qhasm: aacbbbda32 &= mask13
# asm 1: and >aacbbbda32=vec128#71,<aacbbbda32=vec128#71,<mask13=vec128#22
# asm 2: and >aacbbbda32=$73,<aacbbbda32=$73,<mask13=$24
and $73,$73,$24

# qhasm: uint32323232 carry0 = aacbbbda21 >> 13
# asm 1: rotmi >carry0=vec128#72,<aacbbbda21=vec128#63,-13
# asm 2: rotmi >carry0=$74,<aacbbbda21=$65,-13
rotmi $74,$65,-13

# qhasm: uint32323232 carry1 = aacbbbda25 >> 13
# asm 1: rotmi >carry1=vec128#73,<aacbbbda25=vec128#65,-13
# asm 2: rotmi >carry1=$75,<aacbbbda25=$67,-13
rotmi $75,$67,-13

# qhasm: uint32323232 carry2 = aacbbbda29 >> 13
# asm 1: rotmi >carry2=vec128#80,<aacbbbda29=vec128#64,-13
# asm 2: rotmi >carry2=$82,<aacbbbda29=$66,-13
rotmi $82,$66,-13

# qhasm: uint32323232 carry3 = aacbbbda33 >> 13
# asm 1: rotmi >carry3=vec128#81,<aacbbbda33=vec128#60,-13
# asm 2: rotmi >carry3=$83,<aacbbbda33=$62,-13
rotmi $83,$62,-13

# qhasm: int32323232 aacbbbda22 += carry0
# asm 1: a >aacbbbda22=vec128#5,<aacbbbda22=vec128#5,<carry0=vec128#72
# asm 2: a >aacbbbda22=$7,<aacbbbda22=$7,<carry0=$74
a $7,$7,$74

# qhasm: aacbbbda21 &= mask13
# asm 1: and >aacbbbda21=vec128#63,<aacbbbda21=vec128#63,<mask13=vec128#22
# asm 2: and >aacbbbda21=$65,<aacbbbda21=$65,<mask13=$24
and $65,$65,$24

# qhasm: int32323232 aacbbbda26 += carry1
# asm 1: a >aacbbbda26=vec128#55,<aacbbbda26=vec128#55,<carry1=vec128#73
# asm 2: a >aacbbbda26=$57,<aacbbbda26=$57,<carry1=$75
a $57,$57,$75

# qhasm: aacbbbda25 &= mask13
# asm 1: and >aacbbbda25=vec128#65,<aacbbbda25=vec128#65,<mask13=vec128#22
# asm 2: and >aacbbbda25=$67,<aacbbbda25=$67,<mask13=$24
and $67,$67,$24

# qhasm: int32323232 aacbbbda30 += carry2
# asm 1: a >aacbbbda30=vec128#56,<aacbbbda30=vec128#56,<carry2=vec128#80
# asm 2: a >aacbbbda30=$58,<aacbbbda30=$58,<carry2=$82
a $58,$58,$82

# qhasm: aacbbbda29 &= mask13
# asm 1: and >aacbbbda29=vec128#64,<aacbbbda29=vec128#64,<mask13=vec128#22
# asm 2: and >aacbbbda29=$66,<aacbbbda29=$66,<mask13=$24
and $66,$66,$24

# qhasm: int32323232 aacbbbda34 += carry3
# asm 1: a >aacbbbda34=vec128#57,<aacbbbda34=vec128#57,<carry3=vec128#81
# asm 2: a >aacbbbda34=$59,<aacbbbda34=$59,<carry3=$83
a $59,$59,$83

# qhasm: aacbbbda33 &= mask13
# asm 1: and >aacbbbda33=vec128#60,<aacbbbda33=vec128#60,<mask13=vec128#22
# asm 2: and >aacbbbda33=$62,<aacbbbda33=$62,<mask13=$24
and $62,$62,$24

# qhasm: uint32323232 carry0 = aacbbbda22 >> 13
# asm 1: rotmi >carry0=vec128#72,<aacbbbda22=vec128#5,-13
# asm 2: rotmi >carry0=$74,<aacbbbda22=$7,-13
rotmi $74,$7,-13

# qhasm: uint32323232 carry1 = aacbbbda26 >> 13
# asm 1: rotmi >carry1=vec128#73,<aacbbbda26=vec128#55,-13
# asm 2: rotmi >carry1=$75,<aacbbbda26=$57,-13
rotmi $75,$57,-13

# qhasm: uint32323232 carry2 = aacbbbda30 >> 13
# asm 1: rotmi >carry2=vec128#80,<aacbbbda30=vec128#56,-13
# asm 2: rotmi >carry2=$82,<aacbbbda30=$58,-13
rotmi $82,$58,-13

# qhasm: uint32323232 carry3 = aacbbbda34 >> 13
# asm 1: rotmi >carry3=vec128#81,<aacbbbda34=vec128#57,-13
# asm 2: rotmi >carry3=$83,<aacbbbda34=$59,-13
rotmi $83,$59,-13

# qhasm: int32323232 aacbbbda23 += carry0
# asm 1: a >aacbbbda23=vec128#66,<aacbbbda23=vec128#66,<carry0=vec128#72
# asm 2: a >aacbbbda23=$68,<aacbbbda23=$68,<carry0=$74
a $68,$68,$74

# qhasm: aacbbbda22 &= mask13
# asm 1: and >aacbbbda22=vec128#5,<aacbbbda22=vec128#5,<mask13=vec128#22
# asm 2: and >aacbbbda22=$7,<aacbbbda22=$7,<mask13=$24
and $7,$7,$24

# qhasm: int32323232 aacbbbda27 += carry1
# asm 1: a >aacbbbda27=vec128#67,<aacbbbda27=vec128#67,<carry1=vec128#73
# asm 2: a >aacbbbda27=$69,<aacbbbda27=$69,<carry1=$75
a $69,$69,$75

# qhasm: aacbbbda26 &= mask13
# asm 1: and >aacbbbda26=vec128#55,<aacbbbda26=vec128#55,<mask13=vec128#22
# asm 2: and >aacbbbda26=$57,<aacbbbda26=$57,<mask13=$24
and $57,$57,$24

# qhasm: int32323232 aacbbbda31 += carry2
# asm 1: a >aacbbbda31=vec128#68,<aacbbbda31=vec128#68,<carry2=vec128#80
# asm 2: a >aacbbbda31=$70,<aacbbbda31=$70,<carry2=$82
a $70,$70,$82

# qhasm: aacbbbda30 &= mask13
# asm 1: and >aacbbbda30=vec128#56,<aacbbbda30=vec128#56,<mask13=vec128#22
# asm 2: and >aacbbbda30=$58,<aacbbbda30=$58,<mask13=$24
and $58,$58,$24

# qhasm: int32323232 aacbbbda35 += carry3
# asm 1: a >aacbbbda35=vec128#72,<aacbbbda35=vec128#78,<carry3=vec128#81
# asm 2: a >aacbbbda35=$74,<aacbbbda35=$80,<carry3=$83
a $74,$80,$83

# qhasm: aacbbbda34 &= mask13
# asm 1: and >aacbbbda34=vec128#57,<aacbbbda34=vec128#57,<mask13=vec128#22
# asm 2: and >aacbbbda34=$59,<aacbbbda34=$59,<mask13=$24
and $59,$59,$24

# qhasm: uint32323232 carry0 = aacbbbda23 >> 12
# asm 1: rotmi >carry0=vec128#73,<aacbbbda23=vec128#66,-12
# asm 2: rotmi >carry0=$75,<aacbbbda23=$68,-12
rotmi $75,$68,-12

# qhasm: uint32323232 carry1 = aacbbbda27 >> 12
# asm 1: rotmi >carry1=vec128#78,<aacbbbda27=vec128#67,-12
# asm 2: rotmi >carry1=$80,<aacbbbda27=$69,-12
rotmi $80,$69,-12

# qhasm: uint32323232 carry2 = aacbbbda31 >> 12
# asm 1: rotmi >carry2=vec128#80,<aacbbbda31=vec128#68,-12
# asm 2: rotmi >carry2=$82,<aacbbbda31=$70,-12
rotmi $82,$70,-12

# qhasm: uint32323232 carry3 = aacbbbda35 >> 12
# asm 1: rotmi >carry3=vec128#81,<aacbbbda35=vec128#72,-12
# asm 2: rotmi >carry3=$83,<aacbbbda35=$74,-12
rotmi $83,$74,-12

# qhasm: int32323232 aacbbbda24 += carry0
# asm 1: a >aacbbbda24=vec128#61,<aacbbbda24=vec128#61,<carry0=vec128#73
# asm 2: a >aacbbbda24=$63,<aacbbbda24=$63,<carry0=$75
a $63,$63,$75

# qhasm: aacbbbda23 &= mask12
# asm 1: and >aacbbbda23=vec128#66,<aacbbbda23=vec128#66,<mask12=vec128#21
# asm 2: and >aacbbbda23=$68,<aacbbbda23=$68,<mask12=$23
and $68,$68,$23

# qhasm: int32323232 aacbbbda28 += carry1
# asm 1: a >aacbbbda28=vec128#62,<aacbbbda28=vec128#62,<carry1=vec128#78
# asm 2: a >aacbbbda28=$64,<aacbbbda28=$64,<carry1=$80
a $64,$64,$80

# qhasm: aacbbbda27 &= mask12
# asm 1: and >aacbbbda27=vec128#67,<aacbbbda27=vec128#67,<mask12=vec128#21
# asm 2: and >aacbbbda27=$69,<aacbbbda27=$69,<mask12=$23
and $69,$69,$23

# qhasm: int32323232 aacbbbda32 += carry2
# asm 1: a >aacbbbda32=vec128#71,<aacbbbda32=vec128#71,<carry2=vec128#80
# asm 2: a >aacbbbda32=$73,<aacbbbda32=$73,<carry2=$82
a $73,$73,$82

# qhasm: aacbbbda31 &= mask12
# asm 1: and >aacbbbda31=vec128#68,<aacbbbda31=vec128#68,<mask12=vec128#21
# asm 2: and >aacbbbda31=$70,<aacbbbda31=$70,<mask12=$23
and $70,$70,$23

# qhasm: int32323232 aacbbbda36 += carry3
# asm 1: a >aacbbbda36=vec128#69,<aacbbbda36=vec128#69,<carry3=vec128#81
# asm 2: a >aacbbbda36=$71,<aacbbbda36=$71,<carry3=$83
a $71,$71,$83

# qhasm: aacbbbda35 &= mask12
# asm 1: and >aacbbbda35=vec128#72,<aacbbbda35=vec128#72,<mask12=vec128#21
# asm 2: and >aacbbbda35=$74,<aacbbbda35=$74,<mask12=$23
and $74,$74,$23

# qhasm: uint32323232 carry1 = aacbbbda24 >> 13
# asm 1: rotmi >carry1=vec128#73,<aacbbbda24=vec128#61,-13
# asm 2: rotmi >carry1=$75,<aacbbbda24=$63,-13
rotmi $75,$63,-13

# qhasm: uint32323232 carry2 = aacbbbda28 >> 13
# asm 1: rotmi >carry2=vec128#78,<aacbbbda28=vec128#62,-13
# asm 2: rotmi >carry2=$80,<aacbbbda28=$64,-13
rotmi $80,$64,-13

# qhasm: uint32323232 carry3 = aacbbbda32 >> 13
# asm 1: rotmi >carry3=vec128#80,<aacbbbda32=vec128#71,-13
# asm 2: rotmi >carry3=$82,<aacbbbda32=$73,-13
rotmi $82,$73,-13

# qhasm: uint32323232 carry4 = aacbbbda36 >> 13
# asm 1: rotmi >carry4=vec128#81,<aacbbbda36=vec128#69,-13
# asm 2: rotmi >carry4=$83,<aacbbbda36=$71,-13
rotmi $83,$71,-13

# qhasm: int32323232 aacbbbda25 += carry1
# asm 1: a >aacbbbda25=vec128#65,<aacbbbda25=vec128#65,<carry1=vec128#73
# asm 2: a >aacbbbda25=$67,<aacbbbda25=$67,<carry1=$75
a $67,$67,$75

# qhasm: aacbbbda24 &= mask13
# asm 1: and >aacbbbda24=vec128#61,<aacbbbda24=vec128#61,<mask13=vec128#22
# asm 2: and >aacbbbda24=$63,<aacbbbda24=$63,<mask13=$24
and $63,$63,$24

# qhasm: int32323232 aacbbbda29 += carry2
# asm 1: a >aacbbbda29=vec128#64,<aacbbbda29=vec128#64,<carry2=vec128#78
# asm 2: a >aacbbbda29=$66,<aacbbbda29=$66,<carry2=$80
a $66,$66,$80

# qhasm: aacbbbda28 &= mask13
# asm 1: and >aacbbbda28=vec128#62,<aacbbbda28=vec128#62,<mask13=vec128#22
# asm 2: and >aacbbbda28=$64,<aacbbbda28=$64,<mask13=$24
and $64,$64,$24

# qhasm: int32323232 aacbbbda33 += carry3
# asm 1: a >aacbbbda33=vec128#60,<aacbbbda33=vec128#60,<carry3=vec128#80
# asm 2: a >aacbbbda33=$62,<aacbbbda33=$62,<carry3=$82
a $62,$62,$82

# qhasm: aacbbbda32 &= mask13
# asm 1: and >aacbbbda32=vec128#71,<aacbbbda32=vec128#71,<mask13=vec128#22
# asm 2: and >aacbbbda32=$73,<aacbbbda32=$73,<mask13=$24
and $73,$73,$24

# qhasm: int32323232 aacbbbda37 += carry4
# asm 1: a >aacbbbda37=vec128#73,<aacbbbda37=vec128#79,<carry4=vec128#81
# asm 2: a >aacbbbda37=$75,<aacbbbda37=$81,<carry4=$83
a $75,$81,$83

# qhasm: aacbbbda36 &= mask13
# asm 1: and >aacbbbda36=vec128#69,<aacbbbda36=vec128#69,<mask13=vec128#22
# asm 2: and >aacbbbda36=$71,<aacbbbda36=$71,<mask13=$24
and $71,$71,$24

# qhasm: uint32323232 carry1 = aacbbbda25 >> 13
# asm 1: rotmi >carry1=vec128#78,<aacbbbda25=vec128#65,-13
# asm 2: rotmi >carry1=$80,<aacbbbda25=$67,-13
rotmi $80,$67,-13

# qhasm: uint32323232 carry2 = aacbbbda29 >> 13
# asm 1: rotmi >carry2=vec128#79,<aacbbbda29=vec128#64,-13
# asm 2: rotmi >carry2=$81,<aacbbbda29=$66,-13
rotmi $81,$66,-13

# qhasm: uint32323232 carry3 = aacbbbda33 >> 13
# asm 1: rotmi >carry3=vec128#80,<aacbbbda33=vec128#60,-13
# asm 2: rotmi >carry3=$82,<aacbbbda33=$62,-13
rotmi $82,$62,-13

# qhasm: uint32323232 carry4 = aacbbbda37 >> 13
# asm 1: rotmi >carry4=vec128#81,<aacbbbda37=vec128#73,-13
# asm 2: rotmi >carry4=$83,<aacbbbda37=$75,-13
rotmi $83,$75,-13

# qhasm: int32323232 aacbbbda26 += carry1
# asm 1: a >aacbbbda26=vec128#55,<aacbbbda26=vec128#55,<carry1=vec128#78
# asm 2: a >aacbbbda26=$57,<aacbbbda26=$57,<carry1=$80
a $57,$57,$80

# qhasm: aacbbbda25 &= mask13
# asm 1: and >aacbbbda25=vec128#65,<aacbbbda25=vec128#65,<mask13=vec128#22
# asm 2: and >aacbbbda25=$67,<aacbbbda25=$67,<mask13=$24
and $67,$67,$24

# qhasm: int32323232 aacbbbda30 += carry2
# asm 1: a >aacbbbda30=vec128#56,<aacbbbda30=vec128#56,<carry2=vec128#79
# asm 2: a >aacbbbda30=$58,<aacbbbda30=$58,<carry2=$81
a $58,$58,$81

# qhasm: aacbbbda29 &= mask13
# asm 1: and >aacbbbda29=vec128#64,<aacbbbda29=vec128#64,<mask13=vec128#22
# asm 2: and >aacbbbda29=$66,<aacbbbda29=$66,<mask13=$24
and $66,$66,$24

# qhasm: int32323232 aacbbbda34 += carry3
# asm 1: a >aacbbbda34=vec128#57,<aacbbbda34=vec128#57,<carry3=vec128#80
# asm 2: a >aacbbbda34=$59,<aacbbbda34=$59,<carry3=$82
a $59,$59,$82

# qhasm: aacbbbda33 &= mask13
# asm 1: and >aacbbbda33=vec128#60,<aacbbbda33=vec128#60,<mask13=vec128#22
# asm 2: and >aacbbbda33=$62,<aacbbbda33=$62,<mask13=$24
and $62,$62,$24

# qhasm: int32323232 aacbbbda38 += carry4
# asm 1: a >aacbbbda38=vec128#59,<aacbbbda38=vec128#59,<carry4=vec128#81
# asm 2: a >aacbbbda38=$61,<aacbbbda38=$61,<carry4=$83
a $61,$61,$83

# qhasm: aacbbbda37 &= mask13
# asm 1: and >aacbbbda37=vec128#73,<aacbbbda37=vec128#73,<mask13=vec128#22
# asm 2: and >aacbbbda37=$75,<aacbbbda37=$75,<mask13=$24
and $75,$75,$24

# qhasm: uint32323232 carry1 = aacbbbda26 >> 13
# asm 1: rotmi >carry1=vec128#78,<aacbbbda26=vec128#55,-13
# asm 2: rotmi >carry1=$80,<aacbbbda26=$57,-13
rotmi $80,$57,-13

# qhasm: uint32323232 carry2 = aacbbbda30 >> 13
# asm 1: rotmi >carry2=vec128#79,<aacbbbda30=vec128#56,-13
# asm 2: rotmi >carry2=$81,<aacbbbda30=$58,-13
rotmi $81,$58,-13

# qhasm: uint32323232 carry3 = aacbbbda34 >> 13
# asm 1: rotmi >carry3=vec128#80,<aacbbbda34=vec128#57,-13
# asm 2: rotmi >carry3=$82,<aacbbbda34=$59,-13
rotmi $82,$59,-13

# qhasm: uint32323232 aacbbbda39 = aacbbbda38 >> 13
# asm 1: rotmi >aacbbbda39=vec128#81,<aacbbbda38=vec128#59,-13
# asm 2: rotmi >aacbbbda39=$83,<aacbbbda38=$61,-13
rotmi $83,$61,-13

# qhasm: int32323232 aacbbbda27 += carry1
# asm 1: a >aacbbbda27=vec128#67,<aacbbbda27=vec128#67,<carry1=vec128#78
# asm 2: a >aacbbbda27=$69,<aacbbbda27=$69,<carry1=$80
a $69,$69,$80

# qhasm: aacbbbda26 &= mask13
# asm 1: and >aacbbbda26=vec128#55,<aacbbbda26=vec128#55,<mask13=vec128#22
# asm 2: and >aacbbbda26=$57,<aacbbbda26=$57,<mask13=$24
and $57,$57,$24

# qhasm: int32323232 aacbbbda31 += carry2
# asm 1: a >aacbbbda31=vec128#68,<aacbbbda31=vec128#68,<carry2=vec128#79
# asm 2: a >aacbbbda31=$70,<aacbbbda31=$70,<carry2=$81
a $70,$70,$81

# qhasm: aacbbbda30 &= mask13
# asm 1: and >aacbbbda30=vec128#56,<aacbbbda30=vec128#56,<mask13=vec128#22
# asm 2: and >aacbbbda30=$58,<aacbbbda30=$58,<mask13=$24
and $58,$58,$24

# qhasm: int32323232 aacbbbda35 += carry3
# asm 1: a >aacbbbda35=vec128#72,<aacbbbda35=vec128#72,<carry3=vec128#80
# asm 2: a >aacbbbda35=$74,<aacbbbda35=$74,<carry3=$82
a $74,$74,$82

# qhasm: aacbbbda34 &= mask13
# asm 1: and >aacbbbda34=vec128#57,<aacbbbda34=vec128#57,<mask13=vec128#22
# asm 2: and >aacbbbda34=$59,<aacbbbda34=$59,<mask13=$24
and $59,$59,$24

# qhasm: uint32323232 carry1 = aacbbbda27 >> 12
# asm 1: rotmi >carry1=vec128#78,<aacbbbda27=vec128#67,-12
# asm 2: rotmi >carry1=$80,<aacbbbda27=$69,-12
rotmi $80,$69,-12

# qhasm: aacbbbda38 &= mask13
# asm 1: and >aacbbbda38=vec128#59,<aacbbbda38=vec128#59,<mask13=vec128#22
# asm 2: and >aacbbbda38=$61,<aacbbbda38=$61,<mask13=$24
and $61,$61,$24

# qhasm: uint32323232 carry2 = aacbbbda31 >> 12
# asm 1: rotmi >carry2=vec128#79,<aacbbbda31=vec128#68,-12
# asm 2: rotmi >carry2=$81,<aacbbbda31=$70,-12
rotmi $81,$70,-12

# qhasm: uint32323232 carry3 = aacbbbda35 >> 12
# asm 1: rotmi >carry3=vec128#80,<aacbbbda35=vec128#72,-12
# asm 2: rotmi >carry3=$82,<aacbbbda35=$74,-12
rotmi $82,$74,-12

# qhasm: int32323232 aacbbbda28 += carry1
# asm 1: a >aacbbbda28=vec128#62,<aacbbbda28=vec128#62,<carry1=vec128#78
# asm 2: a >aacbbbda28=$64,<aacbbbda28=$64,<carry1=$80
a $64,$64,$80

# qhasm: aacbbbda27 &= mask12
# asm 1: and >aacbbbda27=vec128#67,<aacbbbda27=vec128#67,<mask12=vec128#21
# asm 2: and >aacbbbda27=$69,<aacbbbda27=$69,<mask12=$23
and $69,$69,$23

# qhasm: int32323232 aacbbbda32 += carry2
# asm 1: a >aacbbbda32=vec128#71,<aacbbbda32=vec128#71,<carry2=vec128#79
# asm 2: a >aacbbbda32=$73,<aacbbbda32=$73,<carry2=$81
a $73,$73,$81

# qhasm: aacbbbda31 &= mask12
# asm 1: and >aacbbbda31=vec128#68,<aacbbbda31=vec128#68,<mask12=vec128#21
# asm 2: and >aacbbbda31=$70,<aacbbbda31=$70,<mask12=$23
and $70,$70,$23

# qhasm: int32323232 aacbbbda36 += carry3
# asm 1: a >aacbbbda36=vec128#69,<aacbbbda36=vec128#69,<carry3=vec128#80
# asm 2: a >aacbbbda36=$71,<aacbbbda36=$71,<carry3=$82
a $71,$71,$82

# qhasm: aacbbbda35 &= mask12
# asm 1: and >aacbbbda35=vec128#72,<aacbbbda35=vec128#72,<mask12=vec128#21
# asm 2: and >aacbbbda35=$74,<aacbbbda35=$74,<mask12=$23
and $74,$74,$23

# qhasm: int32323232 aacbbbda0  += (aacbbbda20 & 0xffff) * (vec19 & 0xffff)
# asm 1: mpya >aacbbbda0=vec128#58,<aacbbbda20=vec128#58,<vec19=vec128#74,<aacbbbda0=vec128#90
# asm 2: mpya >aacbbbda0=$60,<aacbbbda20=$60,<vec19=$76,<aacbbbda0=$92
mpya $60,$60,$76,$92

# qhasm: int32323232 aacbbbda1  += (aacbbbda21 & 0xffff) * (vec19 & 0xffff)
# asm 1: mpya >aacbbbda1=vec128#63,<aacbbbda21=vec128#63,<vec19=vec128#74,<aacbbbda1=vec128#94
# asm 2: mpya >aacbbbda1=$65,<aacbbbda21=$65,<vec19=$76,<aacbbbda1=$96
mpya $65,$65,$76,$96

# qhasm: int32323232 aacbbbda2  += (aacbbbda22 & 0xffff) * (vec19 & 0xffff)
# asm 1: mpya >aacbbbda2=vec128#5,<aacbbbda22=vec128#5,<vec19=vec128#74,<aacbbbda2=vec128#96
# asm 2: mpya >aacbbbda2=$7,<aacbbbda22=$7,<vec19=$76,<aacbbbda2=$98
mpya $7,$7,$76,$98

# qhasm: int32323232 aacbbbda3  += (aacbbbda23 & 0xffff) * (vec19 & 0xffff)
# asm 1: mpya >aacbbbda3=vec128#66,<aacbbbda23=vec128#66,<vec19=vec128#74,<aacbbbda3=vec128#98
# asm 2: mpya >aacbbbda3=$68,<aacbbbda23=$68,<vec19=$76,<aacbbbda3=$100
mpya $68,$68,$76,$100

# qhasm: int32323232 aacbbbda4  += (aacbbbda24 & 0xffff) * (vec19 & 0xffff)
# asm 1: mpya >aacbbbda4=vec128#61,<aacbbbda24=vec128#61,<vec19=vec128#74,<aacbbbda4=vec128#100
# asm 2: mpya >aacbbbda4=$63,<aacbbbda24=$63,<vec19=$76,<aacbbbda4=$102
mpya $63,$63,$76,$102

# qhasm: int32323232 aacbbbda5  += (aacbbbda25 & 0xffff) * (vec19 & 0xffff)
# asm 1: mpya >aacbbbda5=vec128#65,<aacbbbda25=vec128#65,<vec19=vec128#74,<aacbbbda5=vec128#101
# asm 2: mpya >aacbbbda5=$67,<aacbbbda25=$67,<vec19=$76,<aacbbbda5=$103
mpya $67,$67,$76,$103

# qhasm: int32323232 aacbbbda6  += (aacbbbda26 & 0xffff) * (vec19 & 0xffff)
# asm 1: mpya >aacbbbda6=vec128#55,<aacbbbda26=vec128#55,<vec19=vec128#74,<aacbbbda6=vec128#102
# asm 2: mpya >aacbbbda6=$57,<aacbbbda26=$57,<vec19=$76,<aacbbbda6=$104
mpya $57,$57,$76,$104

# qhasm: int32323232 aacbbbda7  += (aacbbbda27 & 0xffff) * (vec19 & 0xffff)
# asm 1: mpya >aacbbbda7=vec128#67,<aacbbbda27=vec128#67,<vec19=vec128#74,<aacbbbda7=vec128#103
# asm 2: mpya >aacbbbda7=$69,<aacbbbda27=$69,<vec19=$76,<aacbbbda7=$105
mpya $69,$69,$76,$105

# qhasm: int32323232 aacbbbda8  += (aacbbbda28 & 0xffff) * (vec19 & 0xffff)
# asm 1: mpya >aacbbbda8=vec128#62,<aacbbbda28=vec128#62,<vec19=vec128#74,<aacbbbda8=vec128#104
# asm 2: mpya >aacbbbda8=$64,<aacbbbda28=$64,<vec19=$76,<aacbbbda8=$106
mpya $64,$64,$76,$106

# qhasm: int32323232 aacbbbda9  += (aacbbbda29 & 0xffff) * (vec19 & 0xffff)
# asm 1: mpya >aacbbbda9=vec128#64,<aacbbbda29=vec128#64,<vec19=vec128#74,<aacbbbda9=vec128#105
# asm 2: mpya >aacbbbda9=$66,<aacbbbda29=$66,<vec19=$76,<aacbbbda9=$107
mpya $66,$66,$76,$107

# qhasm: int32323232 aacbbbda10 += (aacbbbda30 & 0xffff) * (vec19 & 0xffff)
# asm 1: mpya >aacbbbda10=vec128#56,<aacbbbda30=vec128#56,<vec19=vec128#74,<aacbbbda10=vec128#106
# asm 2: mpya >aacbbbda10=$58,<aacbbbda30=$58,<vec19=$76,<aacbbbda10=$108
mpya $58,$58,$76,$108

# qhasm: int32323232 aacbbbda11 += (aacbbbda31 & 0xffff) * (vec19 & 0xffff)
# asm 1: mpya >aacbbbda11=vec128#68,<aacbbbda31=vec128#68,<vec19=vec128#74,<aacbbbda11=vec128#107
# asm 2: mpya >aacbbbda11=$70,<aacbbbda31=$70,<vec19=$76,<aacbbbda11=$109
mpya $70,$70,$76,$109

# qhasm: int32323232 aacbbbda12 += (aacbbbda32 & 0xffff) * (vec19 & 0xffff)
# asm 1: mpya >aacbbbda12=vec128#71,<aacbbbda32=vec128#71,<vec19=vec128#74,<aacbbbda12=vec128#108
# asm 2: mpya >aacbbbda12=$73,<aacbbbda32=$73,<vec19=$76,<aacbbbda12=$110
mpya $73,$73,$76,$110

# qhasm: int32323232 aacbbbda13 += (aacbbbda33 & 0xffff) * (vec19 & 0xffff)
# asm 1: mpya >aacbbbda13=vec128#60,<aacbbbda33=vec128#60,<vec19=vec128#74,<aacbbbda13=vec128#109
# asm 2: mpya >aacbbbda13=$62,<aacbbbda33=$62,<vec19=$76,<aacbbbda13=$111
mpya $62,$62,$76,$111

# qhasm: int32323232 aacbbbda14 += (aacbbbda34 & 0xffff) * (vec19 & 0xffff)
# asm 1: mpya >aacbbbda14=vec128#57,<aacbbbda34=vec128#57,<vec19=vec128#74,<aacbbbda14=vec128#110
# asm 2: mpya >aacbbbda14=$59,<aacbbbda34=$59,<vec19=$76,<aacbbbda14=$112
mpya $59,$59,$76,$112

# qhasm: int32323232 aacbbbda15 += (aacbbbda35 & 0xffff) * (vec19 & 0xffff)
# asm 1: mpya >aacbbbda15=vec128#72,<aacbbbda35=vec128#72,<vec19=vec128#74,<aacbbbda15=vec128#111
# asm 2: mpya >aacbbbda15=$74,<aacbbbda35=$74,<vec19=$76,<aacbbbda15=$113
mpya $74,$74,$76,$113

# qhasm: int32323232 aacbbbda16 += (aacbbbda36 & 0xffff) * (vec19 & 0xffff)
# asm 1: mpya >aacbbbda16=vec128#69,<aacbbbda36=vec128#69,<vec19=vec128#74,<aacbbbda16=vec128#70
# asm 2: mpya >aacbbbda16=$71,<aacbbbda36=$71,<vec19=$76,<aacbbbda16=$72
mpya $71,$71,$76,$72

# qhasm: int32323232 aacbbbda17 += (aacbbbda37 & 0xffff) * (vec19 & 0xffff)
# asm 1: mpya >aacbbbda17=vec128#70,<aacbbbda37=vec128#73,<vec19=vec128#74,<aacbbbda17=vec128#76
# asm 2: mpya >aacbbbda17=$72,<aacbbbda37=$75,<vec19=$76,<aacbbbda17=$78
mpya $72,$75,$76,$78

# qhasm: int32323232 aacbbbda18 += (aacbbbda38 & 0xffff) * (vec19 & 0xffff)
# asm 1: mpya >aacbbbda18=vec128#59,<aacbbbda38=vec128#59,<vec19=vec128#74,<aacbbbda18=vec128#112
# asm 2: mpya >aacbbbda18=$61,<aacbbbda38=$61,<vec19=$76,<aacbbbda18=$114
mpya $61,$61,$76,$114

# qhasm: int32323232 aacbbbda19 += (aacbbbda39 & 0xffff) * (vec19 & 0xffff)
# asm 1: mpya >aacbbbda19=vec128#73,<aacbbbda39=vec128#81,<vec19=vec128#74,<aacbbbda19=vec128#77
# asm 2: mpya >aacbbbda19=$75,<aacbbbda39=$83,<vec19=$76,<aacbbbda19=$79
mpya $75,$83,$76,$79

# qhasm: uint32323232 carry = aacbbbda16 >> 13
# asm 1: rotmi >carry=vec128#76,<aacbbbda16=vec128#69,-13
# asm 2: rotmi >carry=$78,<aacbbbda16=$71,-13
rotmi $78,$71,-13

# qhasm: int32323232 aacbbbda17 += carry
# asm 1: a >aacbbbda17=vec128#70,<aacbbbda17=vec128#70,<carry=vec128#76
# asm 2: a >aacbbbda17=$72,<aacbbbda17=$72,<carry=$78
a $72,$72,$78

# qhasm: uint32323232 carry = aacbbbda17 >> 13
# asm 1: rotmi >carry=vec128#76,<aacbbbda17=vec128#70,-13
# asm 2: rotmi >carry=$78,<aacbbbda17=$72,-13
rotmi $78,$72,-13

# qhasm: int32323232 aacbbbda18 += carry
# asm 1: a >aacbbbda18=vec128#59,<aacbbbda18=vec128#59,<carry=vec128#76
# asm 2: a >aacbbbda18=$61,<aacbbbda18=$61,<carry=$78
a $61,$61,$78

# qhasm: uint32323232 carry = aacbbbda18 >> 13
# asm 1: rotmi >carry=vec128#76,<aacbbbda18=vec128#59,-13
# asm 2: rotmi >carry=$78,<aacbbbda18=$61,-13
rotmi $78,$61,-13

# qhasm: int32323232 aacbbbda19 += carry
# asm 1: a >aacbbbda19=vec128#73,<aacbbbda19=vec128#73,<carry=vec128#76
# asm 2: a >aacbbbda19=$75,<aacbbbda19=$75,<carry=$78
a $75,$75,$78

# qhasm: uint32323232 carry = aacbbbda19 >> 12
# asm 1: rotmi >carry=vec128#76,<aacbbbda19=vec128#73,-12
# asm 2: rotmi >carry=$78,<aacbbbda19=$75,-12
rotmi $78,$75,-12

# qhasm: int32323232 red = carry << 4
# asm 1: shli >red=vec128#77,<carry=vec128#76,4
# asm 2: shli >red=$79,<carry=$78,4
shli $79,$78,4

# qhasm: int32323232 red += carry
# asm 1: a >red=vec128#77,<red=vec128#77,<carry=vec128#76
# asm 2: a >red=$79,<red=$79,<carry=$78
a $79,$79,$78

# qhasm: int32323232 red += carry
# asm 1: a >red=vec128#77,<red=vec128#77,<carry=vec128#76
# asm 2: a >red=$79,<red=$79,<carry=$78
a $79,$79,$78

# qhasm: int32323232 red += carry
# asm 1: a >red=vec128#76,<red=vec128#77,<carry=vec128#76
# asm 2: a >red=$78,<red=$79,<carry=$78
a $78,$79,$78

# qhasm: int32323232 aacbbbda0 += red
# asm 1: a >aacbbbda0=vec128#58,<aacbbbda0=vec128#58,<red=vec128#76
# asm 2: a >aacbbbda0=$60,<aacbbbda0=$60,<red=$78
a $60,$60,$78

# qhasm: aacbbbda19 &= mask12
# asm 1: and >aacbbbda19=vec128#73,<aacbbbda19=vec128#73,<mask12=vec128#21
# asm 2: and >aacbbbda19=$75,<aacbbbda19=$75,<mask12=$23
and $75,$75,$23

# qhasm: aacbbbda16 &= mask13
# asm 1: and >aacbbbda16=vec128#69,<aacbbbda16=vec128#69,<mask13=vec128#22
# asm 2: and >aacbbbda16=$71,<aacbbbda16=$71,<mask13=$24
and $71,$71,$24

# qhasm: aacbbbda17 &= mask13
# asm 1: and >aacbbbda17=vec128#70,<aacbbbda17=vec128#70,<mask13=vec128#22
# asm 2: and >aacbbbda17=$72,<aacbbbda17=$72,<mask13=$24
and $72,$72,$24

# qhasm: aacbbbda18 &= mask13
# asm 1: and >aacbbbda18=vec128#59,<aacbbbda18=vec128#59,<mask13=vec128#22
# asm 2: and >aacbbbda18=$61,<aacbbbda18=$61,<mask13=$24
and $61,$61,$24

# qhasm: aacbbbda19 &= mask12
# asm 1: and >aacbbbda19=vec128#73,<aacbbbda19=vec128#73,<mask12=vec128#21
# asm 2: and >aacbbbda19=$75,<aacbbbda19=$75,<mask12=$23
and $75,$75,$23

# qhasm: uint32323232 carry0 = aacbbbda0  >> 13
# asm 1: rotmi >carry0=vec128#76,<aacbbbda0=vec128#58,-13
# asm 2: rotmi >carry0=$78,<aacbbbda0=$60,-13
rotmi $78,$60,-13

# qhasm: uint32323232 carry1 = aacbbbda4  >> 13
# asm 1: rotmi >carry1=vec128#77,<aacbbbda4=vec128#61,-13
# asm 2: rotmi >carry1=$79,<aacbbbda4=$63,-13
rotmi $79,$63,-13

# qhasm: uint32323232 carry2 = aacbbbda8  >> 13
# asm 1: rotmi >carry2=vec128#78,<aacbbbda8=vec128#62,-13
# asm 2: rotmi >carry2=$80,<aacbbbda8=$64,-13
rotmi $80,$64,-13

# qhasm: uint32323232 carry3 = aacbbbda12 >> 13
# asm 1: rotmi >carry3=vec128#79,<aacbbbda12=vec128#71,-13
# asm 2: rotmi >carry3=$81,<aacbbbda12=$73,-13
rotmi $81,$73,-13

# qhasm: int32323232 aacbbbda1  += carry0
# asm 1: a >aacbbbda1=vec128#63,<aacbbbda1=vec128#63,<carry0=vec128#76
# asm 2: a >aacbbbda1=$65,<aacbbbda1=$65,<carry0=$78
a $65,$65,$78

# qhasm: int32323232 aacbbbda5  += carry1
# asm 1: a >aacbbbda5=vec128#65,<aacbbbda5=vec128#65,<carry1=vec128#77
# asm 2: a >aacbbbda5=$67,<aacbbbda5=$67,<carry1=$79
a $67,$67,$79

# qhasm: int32323232 aacbbbda9  += carry2
# asm 1: a >aacbbbda9=vec128#64,<aacbbbda9=vec128#64,<carry2=vec128#78
# asm 2: a >aacbbbda9=$66,<aacbbbda9=$66,<carry2=$80
a $66,$66,$80

# qhasm: int32323232 aacbbbda13 += carry3
# asm 1: a >aacbbbda13=vec128#60,<aacbbbda13=vec128#60,<carry3=vec128#79
# asm 2: a >aacbbbda13=$62,<aacbbbda13=$62,<carry3=$81
a $62,$62,$81

# qhasm: aacbbbda0  &= mask13
# asm 1: and >aacbbbda0=vec128#58,<aacbbbda0=vec128#58,<mask13=vec128#22
# asm 2: and >aacbbbda0=$60,<aacbbbda0=$60,<mask13=$24
and $60,$60,$24

# qhasm: aacbbbda4  &= mask13
# asm 1: and >aacbbbda4=vec128#61,<aacbbbda4=vec128#61,<mask13=vec128#22
# asm 2: and >aacbbbda4=$63,<aacbbbda4=$63,<mask13=$24
and $63,$63,$24

# qhasm: aacbbbda8  &= mask13
# asm 1: and >aacbbbda8=vec128#62,<aacbbbda8=vec128#62,<mask13=vec128#22
# asm 2: and >aacbbbda8=$64,<aacbbbda8=$64,<mask13=$24
and $64,$64,$24

# qhasm: aacbbbda12 &= mask13
# asm 1: and >aacbbbda12=vec128#71,<aacbbbda12=vec128#71,<mask13=vec128#22
# asm 2: and >aacbbbda12=$73,<aacbbbda12=$73,<mask13=$24
and $73,$73,$24

# qhasm: uint32323232 carry0 = aacbbbda1  >> 13
# asm 1: rotmi >carry0=vec128#76,<aacbbbda1=vec128#63,-13
# asm 2: rotmi >carry0=$78,<aacbbbda1=$65,-13
rotmi $78,$65,-13

# qhasm: uint32323232 carry1 = aacbbbda5  >> 13
# asm 1: rotmi >carry1=vec128#77,<aacbbbda5=vec128#65,-13
# asm 2: rotmi >carry1=$79,<aacbbbda5=$67,-13
rotmi $79,$67,-13

# qhasm: uint32323232 carry2 = aacbbbda9  >> 13
# asm 1: rotmi >carry2=vec128#78,<aacbbbda9=vec128#64,-13
# asm 2: rotmi >carry2=$80,<aacbbbda9=$66,-13
rotmi $80,$66,-13

# qhasm: uint32323232 carry3 = aacbbbda13 >> 13
# asm 1: rotmi >carry3=vec128#79,<aacbbbda13=vec128#60,-13
# asm 2: rotmi >carry3=$81,<aacbbbda13=$62,-13
rotmi $81,$62,-13

# qhasm: int32323232 aacbbbda2  += carry0
# asm 1: a >aacbbbda2=vec128#5,<aacbbbda2=vec128#5,<carry0=vec128#76
# asm 2: a >aacbbbda2=$7,<aacbbbda2=$7,<carry0=$78
a $7,$7,$78

# qhasm: int32323232 aacbbbda6  += carry1
# asm 1: a >aacbbbda6=vec128#55,<aacbbbda6=vec128#55,<carry1=vec128#77
# asm 2: a >aacbbbda6=$57,<aacbbbda6=$57,<carry1=$79
a $57,$57,$79

# qhasm: int32323232 aacbbbda10 += carry2
# asm 1: a >aacbbbda10=vec128#56,<aacbbbda10=vec128#56,<carry2=vec128#78
# asm 2: a >aacbbbda10=$58,<aacbbbda10=$58,<carry2=$80
a $58,$58,$80

# qhasm: int32323232 aacbbbda14 += carry3
# asm 1: a >aacbbbda14=vec128#57,<aacbbbda14=vec128#57,<carry3=vec128#79
# asm 2: a >aacbbbda14=$59,<aacbbbda14=$59,<carry3=$81
a $59,$59,$81

# qhasm: aacbbbda1  &= mask13
# asm 1: and >aacbbbda1=vec128#63,<aacbbbda1=vec128#63,<mask13=vec128#22
# asm 2: and >aacbbbda1=$65,<aacbbbda1=$65,<mask13=$24
and $65,$65,$24

# qhasm: aacbbbda5  &= mask13
# asm 1: and >aacbbbda5=vec128#65,<aacbbbda5=vec128#65,<mask13=vec128#22
# asm 2: and >aacbbbda5=$67,<aacbbbda5=$67,<mask13=$24
and $67,$67,$24

# qhasm: aacbbbda9  &= mask13
# asm 1: and >aacbbbda9=vec128#64,<aacbbbda9=vec128#64,<mask13=vec128#22
# asm 2: and >aacbbbda9=$66,<aacbbbda9=$66,<mask13=$24
and $66,$66,$24

# qhasm: aacbbbda13 &= mask13
# asm 1: and >aacbbbda13=vec128#60,<aacbbbda13=vec128#60,<mask13=vec128#22
# asm 2: and >aacbbbda13=$62,<aacbbbda13=$62,<mask13=$24
and $62,$62,$24

# qhasm: uint32323232 carry0 = aacbbbda2  >> 13
# asm 1: rotmi >carry0=vec128#76,<aacbbbda2=vec128#5,-13
# asm 2: rotmi >carry0=$78,<aacbbbda2=$7,-13
rotmi $78,$7,-13

# qhasm: uint32323232 carry1 = aacbbbda6  >> 13
# asm 1: rotmi >carry1=vec128#77,<aacbbbda6=vec128#55,-13
# asm 2: rotmi >carry1=$79,<aacbbbda6=$57,-13
rotmi $79,$57,-13

# qhasm: uint32323232 carry2 = aacbbbda10 >> 13
# asm 1: rotmi >carry2=vec128#78,<aacbbbda10=vec128#56,-13
# asm 2: rotmi >carry2=$80,<aacbbbda10=$58,-13
rotmi $80,$58,-13

# qhasm: uint32323232 carry3 = aacbbbda14 >> 13
# asm 1: rotmi >carry3=vec128#79,<aacbbbda14=vec128#57,-13
# asm 2: rotmi >carry3=$81,<aacbbbda14=$59,-13
rotmi $81,$59,-13

# qhasm: int32323232 aacbbbda3  += carry0
# asm 1: a >aacbbbda3=vec128#66,<aacbbbda3=vec128#66,<carry0=vec128#76
# asm 2: a >aacbbbda3=$68,<aacbbbda3=$68,<carry0=$78
a $68,$68,$78

# qhasm: int32323232 aacbbbda7  += carry1
# asm 1: a >aacbbbda7=vec128#67,<aacbbbda7=vec128#67,<carry1=vec128#77
# asm 2: a >aacbbbda7=$69,<aacbbbda7=$69,<carry1=$79
a $69,$69,$79

# qhasm: int32323232 aacbbbda11 += carry2
# asm 1: a >aacbbbda11=vec128#68,<aacbbbda11=vec128#68,<carry2=vec128#78
# asm 2: a >aacbbbda11=$70,<aacbbbda11=$70,<carry2=$80
a $70,$70,$80

# qhasm: int32323232 aacbbbda15 += carry3
# asm 1: a >aacbbbda15=vec128#72,<aacbbbda15=vec128#72,<carry3=vec128#79
# asm 2: a >aacbbbda15=$74,<aacbbbda15=$74,<carry3=$81
a $74,$74,$81

# qhasm: aacbbbda2  &= mask13
# asm 1: and >aacbbbda2=vec128#5,<aacbbbda2=vec128#5,<mask13=vec128#22
# asm 2: and >aacbbbda2=$7,<aacbbbda2=$7,<mask13=$24
and $7,$7,$24

# qhasm: aacbbbda6  &= mask13
# asm 1: and >aacbbbda6=vec128#55,<aacbbbda6=vec128#55,<mask13=vec128#22
# asm 2: and >aacbbbda6=$57,<aacbbbda6=$57,<mask13=$24
and $57,$57,$24

# qhasm: aacbbbda10 &= mask13
# asm 1: and >aacbbbda10=vec128#56,<aacbbbda10=vec128#56,<mask13=vec128#22
# asm 2: and >aacbbbda10=$58,<aacbbbda10=$58,<mask13=$24
and $58,$58,$24

# qhasm: aacbbbda14 &= mask13
# asm 1: and >aacbbbda14=vec128#57,<aacbbbda14=vec128#57,<mask13=vec128#22
# asm 2: and >aacbbbda14=$59,<aacbbbda14=$59,<mask13=$24
and $59,$59,$24

# qhasm: uint32323232 carry0 = aacbbbda3  >> 12
# asm 1: rotmi >carry0=vec128#76,<aacbbbda3=vec128#66,-12
# asm 2: rotmi >carry0=$78,<aacbbbda3=$68,-12
rotmi $78,$68,-12

# qhasm: uint32323232 carry1 = aacbbbda7  >> 12
# asm 1: rotmi >carry1=vec128#77,<aacbbbda7=vec128#67,-12
# asm 2: rotmi >carry1=$79,<aacbbbda7=$69,-12
rotmi $79,$69,-12

# qhasm: uint32323232 carry2 = aacbbbda11 >> 12
# asm 1: rotmi >carry2=vec128#78,<aacbbbda11=vec128#68,-12
# asm 2: rotmi >carry2=$80,<aacbbbda11=$70,-12
rotmi $80,$70,-12

# qhasm: uint32323232 carry3 = aacbbbda15 >> 12
# asm 1: rotmi >carry3=vec128#79,<aacbbbda15=vec128#72,-12
# asm 2: rotmi >carry3=$81,<aacbbbda15=$74,-12
rotmi $81,$74,-12

# qhasm: int32323232 aacbbbda4  += carry0
# asm 1: a >aacbbbda4=vec128#61,<aacbbbda4=vec128#61,<carry0=vec128#76
# asm 2: a >aacbbbda4=$63,<aacbbbda4=$63,<carry0=$78
a $63,$63,$78

# qhasm: int32323232 aacbbbda8  += carry1
# asm 1: a >aacbbbda8=vec128#62,<aacbbbda8=vec128#62,<carry1=vec128#77
# asm 2: a >aacbbbda8=$64,<aacbbbda8=$64,<carry1=$79
a $64,$64,$79

# qhasm: int32323232 aacbbbda12 += carry2
# asm 1: a >aacbbbda12=vec128#71,<aacbbbda12=vec128#71,<carry2=vec128#78
# asm 2: a >aacbbbda12=$73,<aacbbbda12=$73,<carry2=$80
a $73,$73,$80

# qhasm: int32323232 aacbbbda16 += carry3
# asm 1: a >aacbbbda16=vec128#69,<aacbbbda16=vec128#69,<carry3=vec128#79
# asm 2: a >aacbbbda16=$71,<aacbbbda16=$71,<carry3=$81
a $71,$71,$81

# qhasm: aacbbbda3  &= mask12
# asm 1: and >aacbbbda3=vec128#66,<aacbbbda3=vec128#66,<mask12=vec128#21
# asm 2: and >aacbbbda3=$68,<aacbbbda3=$68,<mask12=$23
and $68,$68,$23

# qhasm: aacbbbda7  &= mask12
# asm 1: and >aacbbbda7=vec128#67,<aacbbbda7=vec128#67,<mask12=vec128#21
# asm 2: and >aacbbbda7=$69,<aacbbbda7=$69,<mask12=$23
and $69,$69,$23

# qhasm: aacbbbda11 &= mask12
# asm 1: and >aacbbbda11=vec128#68,<aacbbbda11=vec128#68,<mask12=vec128#21
# asm 2: and >aacbbbda11=$70,<aacbbbda11=$70,<mask12=$23
and $70,$70,$23

# qhasm: aacbbbda15 &= mask12
# asm 1: and >aacbbbda15=vec128#72,<aacbbbda15=vec128#72,<mask12=vec128#21
# asm 2: and >aacbbbda15=$74,<aacbbbda15=$74,<mask12=$23
and $74,$74,$23

# qhasm: uint32323232 carry1 = aacbbbda4  >> 13
# asm 1: rotmi >carry1=vec128#76,<aacbbbda4=vec128#61,-13
# asm 2: rotmi >carry1=$78,<aacbbbda4=$63,-13
rotmi $78,$63,-13

# qhasm: uint32323232 carry2 = aacbbbda8  >> 13
# asm 1: rotmi >carry2=vec128#77,<aacbbbda8=vec128#62,-13
# asm 2: rotmi >carry2=$79,<aacbbbda8=$64,-13
rotmi $79,$64,-13

# qhasm: uint32323232 carry3 = aacbbbda12 >> 13
# asm 1: rotmi >carry3=vec128#78,<aacbbbda12=vec128#71,-13
# asm 2: rotmi >carry3=$80,<aacbbbda12=$73,-13
rotmi $80,$73,-13

# qhasm: uint32323232 carry4 = aacbbbda16 >> 13
# asm 1: rotmi >carry4=vec128#79,<aacbbbda16=vec128#69,-13
# asm 2: rotmi >carry4=$81,<aacbbbda16=$71,-13
rotmi $81,$71,-13

# qhasm: int32323232 aacbbbda5  += carry1
# asm 1: a >aacbbbda5=vec128#65,<aacbbbda5=vec128#65,<carry1=vec128#76
# asm 2: a >aacbbbda5=$67,<aacbbbda5=$67,<carry1=$78
a $67,$67,$78

# qhasm: int32323232 aacbbbda9  += carry2
# asm 1: a >aacbbbda9=vec128#64,<aacbbbda9=vec128#64,<carry2=vec128#77
# asm 2: a >aacbbbda9=$66,<aacbbbda9=$66,<carry2=$79
a $66,$66,$79

# qhasm: int32323232 aacbbbda13 += carry3
# asm 1: a >aacbbbda13=vec128#60,<aacbbbda13=vec128#60,<carry3=vec128#78
# asm 2: a >aacbbbda13=$62,<aacbbbda13=$62,<carry3=$80
a $62,$62,$80

# qhasm: int32323232 aacbbbda17  += carry4
# asm 1: a >aacbbbda17=vec128#70,<aacbbbda17=vec128#70,<carry4=vec128#79
# asm 2: a >aacbbbda17=$72,<aacbbbda17=$72,<carry4=$81
a $72,$72,$81

# qhasm: aacbbbda4  &= mask13
# asm 1: and >aacbbbda4=vec128#61,<aacbbbda4=vec128#61,<mask13=vec128#22
# asm 2: and >aacbbbda4=$63,<aacbbbda4=$63,<mask13=$24
and $63,$63,$24

# qhasm: aacbbbda8  &= mask13
# asm 1: and >aacbbbda8=vec128#62,<aacbbbda8=vec128#62,<mask13=vec128#22
# asm 2: and >aacbbbda8=$64,<aacbbbda8=$64,<mask13=$24
and $64,$64,$24

# qhasm: aacbbbda12 &= mask13
# asm 1: and >aacbbbda12=vec128#71,<aacbbbda12=vec128#71,<mask13=vec128#22
# asm 2: and >aacbbbda12=$73,<aacbbbda12=$73,<mask13=$24
and $73,$73,$24

# qhasm: aacbbbda16 &= mask13
# asm 1: and >aacbbbda16=vec128#69,<aacbbbda16=vec128#69,<mask13=vec128#22
# asm 2: and >aacbbbda16=$71,<aacbbbda16=$71,<mask13=$24
and $71,$71,$24

# qhasm: uint32323232 carry1 = aacbbbda5  >> 13
# asm 1: rotmi >carry1=vec128#76,<aacbbbda5=vec128#65,-13
# asm 2: rotmi >carry1=$78,<aacbbbda5=$67,-13
rotmi $78,$67,-13

# qhasm: uint32323232 carry2 = aacbbbda9  >> 13
# asm 1: rotmi >carry2=vec128#77,<aacbbbda9=vec128#64,-13
# asm 2: rotmi >carry2=$79,<aacbbbda9=$66,-13
rotmi $79,$66,-13

# qhasm: uint32323232 carry3 = aacbbbda13 >> 13
# asm 1: rotmi >carry3=vec128#78,<aacbbbda13=vec128#60,-13
# asm 2: rotmi >carry3=$80,<aacbbbda13=$62,-13
rotmi $80,$62,-13

# qhasm: uint32323232 carry4 = aacbbbda17 >> 13
# asm 1: rotmi >carry4=vec128#79,<aacbbbda17=vec128#70,-13
# asm 2: rotmi >carry4=$81,<aacbbbda17=$72,-13
rotmi $81,$72,-13

# qhasm: int32323232 aacbbbda6  += carry1
# asm 1: a >aacbbbda6=vec128#55,<aacbbbda6=vec128#55,<carry1=vec128#76
# asm 2: a >aacbbbda6=$57,<aacbbbda6=$57,<carry1=$78
a $57,$57,$78

# qhasm: int32323232 aacbbbda10 += carry2
# asm 1: a >aacbbbda10=vec128#56,<aacbbbda10=vec128#56,<carry2=vec128#77
# asm 2: a >aacbbbda10=$58,<aacbbbda10=$58,<carry2=$79
a $58,$58,$79

# qhasm: int32323232 aacbbbda14 += carry3
# asm 1: a >aacbbbda14=vec128#57,<aacbbbda14=vec128#57,<carry3=vec128#78
# asm 2: a >aacbbbda14=$59,<aacbbbda14=$59,<carry3=$80
a $59,$59,$80

# qhasm: int32323232 aacbbbda18 += carry4
# asm 1: a >aacbbbda18=vec128#59,<aacbbbda18=vec128#59,<carry4=vec128#79
# asm 2: a >aacbbbda18=$61,<aacbbbda18=$61,<carry4=$81
a $61,$61,$81

# qhasm: aacbbbda5  &= mask13
# asm 1: and >aacbbbda5=vec128#65,<aacbbbda5=vec128#65,<mask13=vec128#22
# asm 2: and >aacbbbda5=$67,<aacbbbda5=$67,<mask13=$24
and $67,$67,$24

# qhasm: aacbbbda9  &= mask13
# asm 1: and >aacbbbda9=vec128#64,<aacbbbda9=vec128#64,<mask13=vec128#22
# asm 2: and >aacbbbda9=$66,<aacbbbda9=$66,<mask13=$24
and $66,$66,$24

# qhasm: aacbbbda13 &= mask13
# asm 1: and >aacbbbda13=vec128#60,<aacbbbda13=vec128#60,<mask13=vec128#22
# asm 2: and >aacbbbda13=$62,<aacbbbda13=$62,<mask13=$24
and $62,$62,$24

# qhasm: aacbbbda17 &= mask13
# asm 1: and >aacbbbda17=vec128#70,<aacbbbda17=vec128#70,<mask13=vec128#22
# asm 2: and >aacbbbda17=$72,<aacbbbda17=$72,<mask13=$24
and $72,$72,$24

# qhasm: uint32323232 carry1 = aacbbbda6  >> 13
# asm 1: rotmi >carry1=vec128#76,<aacbbbda6=vec128#55,-13
# asm 2: rotmi >carry1=$78,<aacbbbda6=$57,-13
rotmi $78,$57,-13

# qhasm: uint32323232 carry2 = aacbbbda10 >> 13
# asm 1: rotmi >carry2=vec128#77,<aacbbbda10=vec128#56,-13
# asm 2: rotmi >carry2=$79,<aacbbbda10=$58,-13
rotmi $79,$58,-13

# qhasm: uint32323232 carry3 = aacbbbda14 >> 13
# asm 1: rotmi >carry3=vec128#78,<aacbbbda14=vec128#57,-13
# asm 2: rotmi >carry3=$80,<aacbbbda14=$59,-13
rotmi $80,$59,-13

# qhasm: uint32323232 carry4 = aacbbbda18  >> 13
# asm 1: rotmi >carry4=vec128#79,<aacbbbda18=vec128#59,-13
# asm 2: rotmi >carry4=$81,<aacbbbda18=$61,-13
rotmi $81,$61,-13

# qhasm: int32323232 aacbbbda7  += carry1
# asm 1: a >aacbbbda7=vec128#67,<aacbbbda7=vec128#67,<carry1=vec128#76
# asm 2: a >aacbbbda7=$69,<aacbbbda7=$69,<carry1=$78
a $69,$69,$78

# qhasm: int32323232 aacbbbda11 += carry2
# asm 1: a >aacbbbda11=vec128#68,<aacbbbda11=vec128#68,<carry2=vec128#77
# asm 2: a >aacbbbda11=$70,<aacbbbda11=$70,<carry2=$79
a $70,$70,$79

# qhasm: int32323232 aacbbbda15 += carry3
# asm 1: a >aacbbbda15=vec128#72,<aacbbbda15=vec128#72,<carry3=vec128#78
# asm 2: a >aacbbbda15=$74,<aacbbbda15=$74,<carry3=$80
a $74,$74,$80

# qhasm: int32323232 aacbbbda19  += carry4
# asm 1: a >aacbbbda19=vec128#73,<aacbbbda19=vec128#73,<carry4=vec128#79
# asm 2: a >aacbbbda19=$75,<aacbbbda19=$75,<carry4=$81
a $75,$75,$81

# qhasm: aacbbbda6  &= mask13
# asm 1: and >aacbbbda6=vec128#55,<aacbbbda6=vec128#55,<mask13=vec128#22
# asm 2: and >aacbbbda6=$57,<aacbbbda6=$57,<mask13=$24
and $57,$57,$24

# qhasm: aacbbbda10 &= mask13
# asm 1: and >aacbbbda10=vec128#56,<aacbbbda10=vec128#56,<mask13=vec128#22
# asm 2: and >aacbbbda10=$58,<aacbbbda10=$58,<mask13=$24
and $58,$58,$24

# qhasm: aacbbbda14 &= mask13
# asm 1: and >aacbbbda14=vec128#57,<aacbbbda14=vec128#57,<mask13=vec128#22
# asm 2: and >aacbbbda14=$59,<aacbbbda14=$59,<mask13=$24
and $59,$59,$24

# qhasm: aacbbbda18  &= mask13
# asm 1: and >aacbbbda18=vec128#59,<aacbbbda18=vec128#59,<mask13=vec128#22
# asm 2: and >aacbbbda18=$61,<aacbbbda18=$61,<mask13=$24
and $61,$61,$24

# qhasm: int32323232 tmp0 =  aacbbbda0 << 16
# asm 1: shli >tmp0=vec128#76,<aacbbbda0=vec128#58,16
# asm 2: shli >tmp0=$78,<aacbbbda0=$60,16
shli $78,$60,16

# qhasm: 2p2p2pcb0 = combine 2pconsts0 and aacbbbda0 by selw0105
# asm 1: shufb >2p2p2pcb0=vec128#77,<2pconsts0=vec128#35,<aacbbbda0=vec128#58,<selw0105=vec128#24
# asm 2: shufb >2p2p2pcb0=$79,<2pconsts0=$37,<aacbbbda0=$60,<selw0105=$26
shufb $79,$37,$60,$26

# qhasm: int32323232 tmp1 =  aacbbbda1 << 16
# asm 1: shli >tmp1=vec128#78,<aacbbbda1=vec128#63,16
# asm 2: shli >tmp1=$80,<aacbbbda1=$65,16
shli $80,$65,16

# qhasm: 2p2p2pcb1 = combine 2pconsts and aacbbbda1 by selw0105
# asm 1: shufb >2p2p2pcb1=vec128#79,<2pconsts=vec128#36,<aacbbbda1=vec128#63,<selw0105=vec128#24
# asm 2: shufb >2p2p2pcb1=$81,<2pconsts=$38,<aacbbbda1=$65,<selw0105=$26
shufb $81,$38,$65,$26

# qhasm: int32323232 tmp2 =  aacbbbda2 << 16
# asm 1: shli >tmp2=vec128#80,<aacbbbda2=vec128#5,16
# asm 2: shli >tmp2=$82,<aacbbbda2=$7,16
shli $82,$7,16

# qhasm: 2p2p2pcb2 = combine 2pconsts and aacbbbda2 by selw0105
# asm 1: shufb >2p2p2pcb2=vec128#81,<2pconsts=vec128#36,<aacbbbda2=vec128#5,<selw0105=vec128#24
# asm 2: shufb >2p2p2pcb2=$83,<2pconsts=$38,<aacbbbda2=$7,<selw0105=$26
shufb $83,$38,$7,$26

# qhasm: int32323232 tmp3 =  aacbbbda3 << 16
# asm 1: shli >tmp3=vec128#82,<aacbbbda3=vec128#66,16
# asm 2: shli >tmp3=$84,<aacbbbda3=$68,16
shli $84,$68,16

# qhasm: 2p2p2pcb3 = combine 2pconsts and aacbbbda3 by selw2325
# asm 1: shufb >2p2p2pcb3=vec128#83,<2pconsts=vec128#36,<aacbbbda3=vec128#66,<selw2325=vec128#25
# asm 2: shufb >2p2p2pcb3=$85,<2pconsts=$38,<aacbbbda3=$68,<selw2325=$27
shufb $85,$38,$68,$27

# qhasm: uint32323232 tmp0  += (tmp0  >> 16) * (a24vec >> 16)
# asm 1: mpyhhau <tmp0=vec128#76,<tmp0=vec128#76,<a24vec=vec128#48
# asm 2: mpyhhau <tmp0=$78,<tmp0=$78,<a24vec=$50
mpyhhau $78,$78,$50

# qhasm: 2p2p2pcb4 = combine 2pconsts and aacbbbda4 by selw0105
# asm 1: shufb >2p2p2pcb4=vec128#84,<2pconsts=vec128#36,<aacbbbda4=vec128#61,<selw0105=vec128#24
# asm 2: shufb >2p2p2pcb4=$86,<2pconsts=$38,<aacbbbda4=$63,<selw0105=$26
shufb $86,$38,$63,$26

# qhasm: uint32323232 tmp1  += (tmp1  >> 16) * (a24vec >> 16)
# asm 1: mpyhhau <tmp1=vec128#78,<tmp1=vec128#78,<a24vec=vec128#48
# asm 2: mpyhhau <tmp1=$80,<tmp1=$80,<a24vec=$50
mpyhhau $80,$80,$50

# qhasm: 2p2p2pcb5 = combine 2pconsts and aacbbbda5 by selw0105
# asm 1: shufb >2p2p2pcb5=vec128#85,<2pconsts=vec128#36,<aacbbbda5=vec128#65,<selw0105=vec128#24
# asm 2: shufb >2p2p2pcb5=$87,<2pconsts=$38,<aacbbbda5=$67,<selw0105=$26
shufb $87,$38,$67,$26

# qhasm: uint32323232 tmp2  += (tmp2  >> 16) * (a24vec >> 16)
# asm 1: mpyhhau <tmp2=vec128#80,<tmp2=vec128#80,<a24vec=vec128#48
# asm 2: mpyhhau <tmp2=$82,<tmp2=$82,<a24vec=$50
mpyhhau $82,$82,$50

# qhasm: 2p2p2pcb6 = combine 2pconsts and aacbbbda6 by selw0105
# asm 1: shufb >2p2p2pcb6=vec128#86,<2pconsts=vec128#36,<aacbbbda6=vec128#55,<selw0105=vec128#24
# asm 2: shufb >2p2p2pcb6=$88,<2pconsts=$38,<aacbbbda6=$57,<selw0105=$26
shufb $88,$38,$57,$26

# qhasm: uint32323232 tmp3  += (tmp3  >> 16) * (a24vec >> 16)
# asm 1: mpyhhau <tmp3=vec128#82,<tmp3=vec128#82,<a24vec=vec128#48
# asm 2: mpyhhau <tmp3=$84,<tmp3=$84,<a24vec=$50
mpyhhau $84,$84,$50

# qhasm: 2p2p2pcb7 = combine 2pconsts and aacbbbda7 by selw2325
# asm 1: shufb >2p2p2pcb7=vec128#87,<2pconsts=vec128#36,<aacbbbda7=vec128#67,<selw2325=vec128#25
# asm 2: shufb >2p2p2pcb7=$89,<2pconsts=$38,<aacbbbda7=$69,<selw2325=$27
shufb $89,$38,$69,$27

# qhasm: int32323232 tmp4 =  aacbbbda4 << 16
# asm 1: shli >tmp4=vec128#88,<aacbbbda4=vec128#61,16
# asm 2: shli >tmp4=$90,<aacbbbda4=$63,16
shli $90,$63,16

# qhasm: 2p2p2pcb8 = combine 2pconsts and aacbbbda8 by selw0105
# asm 1: shufb >2p2p2pcb8=vec128#89,<2pconsts=vec128#36,<aacbbbda8=vec128#62,<selw0105=vec128#24
# asm 2: shufb >2p2p2pcb8=$91,<2pconsts=$38,<aacbbbda8=$64,<selw0105=$26
shufb $91,$38,$64,$26

# qhasm: int32323232 tmp5 =  aacbbbda5 << 16
# asm 1: shli >tmp5=vec128#90,<aacbbbda5=vec128#65,16
# asm 2: shli >tmp5=$92,<aacbbbda5=$67,16
shli $92,$67,16

# qhasm: 2p2p2pcb9 = combine 2pconsts and aacbbbda9 by selw0105
# asm 1: shufb >2p2p2pcb9=vec128#91,<2pconsts=vec128#36,<aacbbbda9=vec128#64,<selw0105=vec128#24
# asm 2: shufb >2p2p2pcb9=$93,<2pconsts=$38,<aacbbbda9=$66,<selw0105=$26
shufb $93,$38,$66,$26

# qhasm: int32323232 tmp6 =  aacbbbda6 << 16
# asm 1: shli >tmp6=vec128#92,<aacbbbda6=vec128#55,16
# asm 2: shli >tmp6=$94,<aacbbbda6=$57,16
shli $94,$57,16

# qhasm: 2p2p2pcb10 = combine 2pconsts and aacbbbda10 by selw0105
# asm 1: shufb >2p2p2pcb10=vec128#93,<2pconsts=vec128#36,<aacbbbda10=vec128#56,<selw0105=vec128#24
# asm 2: shufb >2p2p2pcb10=$95,<2pconsts=$38,<aacbbbda10=$58,<selw0105=$26
shufb $95,$38,$58,$26

# qhasm: int32323232 tmp7 =  aacbbbda7 << 16
# asm 1: shli >tmp7=vec128#94,<aacbbbda7=vec128#67,16
# asm 2: shli >tmp7=$96,<aacbbbda7=$69,16
shli $96,$69,16

# qhasm: 2p2p2pcb11 = combine 2pconsts and aacbbbda11 by selw2325
# asm 1: shufb >2p2p2pcb11=vec128#95,<2pconsts=vec128#36,<aacbbbda11=vec128#68,<selw2325=vec128#25
# asm 2: shufb >2p2p2pcb11=$97,<2pconsts=$38,<aacbbbda11=$70,<selw2325=$27
shufb $97,$38,$70,$27

# qhasm: uint32323232 tmp4  += (tmp4  >> 16) * (a24vec >> 16)
# asm 1: mpyhhau <tmp4=vec128#88,<tmp4=vec128#88,<a24vec=vec128#48
# asm 2: mpyhhau <tmp4=$90,<tmp4=$90,<a24vec=$50
mpyhhau $90,$90,$50

# qhasm: 2p2p2pcb12 = combine 2pconsts and aacbbbda12 by selw0105
# asm 1: shufb >2p2p2pcb12=vec128#96,<2pconsts=vec128#36,<aacbbbda12=vec128#71,<selw0105=vec128#24
# asm 2: shufb >2p2p2pcb12=$98,<2pconsts=$38,<aacbbbda12=$73,<selw0105=$26
shufb $98,$38,$73,$26

# qhasm: uint32323232 tmp5  += (tmp5  >> 16) * (a24vec >> 16)
# asm 1: mpyhhau <tmp5=vec128#90,<tmp5=vec128#90,<a24vec=vec128#48
# asm 2: mpyhhau <tmp5=$92,<tmp5=$92,<a24vec=$50
mpyhhau $92,$92,$50

# qhasm: 2p2p2pcb13 = combine 2pconsts and aacbbbda13 by selw0105
# asm 1: shufb >2p2p2pcb13=vec128#97,<2pconsts=vec128#36,<aacbbbda13=vec128#60,<selw0105=vec128#24
# asm 2: shufb >2p2p2pcb13=$99,<2pconsts=$38,<aacbbbda13=$62,<selw0105=$26
shufb $99,$38,$62,$26

# qhasm: uint32323232 tmp6  += (tmp6  >> 16) * (a24vec >> 16)
# asm 1: mpyhhau <tmp6=vec128#92,<tmp6=vec128#92,<a24vec=vec128#48
# asm 2: mpyhhau <tmp6=$94,<tmp6=$94,<a24vec=$50
mpyhhau $94,$94,$50

# qhasm: 2p2p2pcb14 = combine 2pconsts and aacbbbda14 by selw0105
# asm 1: shufb >2p2p2pcb14=vec128#98,<2pconsts=vec128#36,<aacbbbda14=vec128#57,<selw0105=vec128#24
# asm 2: shufb >2p2p2pcb14=$100,<2pconsts=$38,<aacbbbda14=$59,<selw0105=$26
shufb $100,$38,$59,$26

# qhasm: uint32323232 tmp7  += (tmp7  >> 16) * (a24vec >> 16)
# asm 1: mpyhhau <tmp7=vec128#94,<tmp7=vec128#94,<a24vec=vec128#48
# asm 2: mpyhhau <tmp7=$96,<tmp7=$96,<a24vec=$50
mpyhhau $96,$96,$50

# qhasm: 2p2p2pcb15 = combine 2pconsts and aacbbbda15 by selw2325
# asm 1: shufb >2p2p2pcb15=vec128#99,<2pconsts=vec128#36,<aacbbbda15=vec128#72,<selw2325=vec128#25
# asm 2: shufb >2p2p2pcb15=$101,<2pconsts=$38,<aacbbbda15=$74,<selw2325=$27
shufb $101,$38,$74,$27

# qhasm: int32323232 tmp8 =  aacbbbda8 << 16
# asm 1: shli >tmp8=vec128#100,<aacbbbda8=vec128#62,16
# asm 2: shli >tmp8=$102,<aacbbbda8=$64,16
shli $102,$64,16

# qhasm: 2p2p2pcb16 = combine 2pconsts and aacbbbda16 by selw0105
# asm 1: shufb >2p2p2pcb16=vec128#101,<2pconsts=vec128#36,<aacbbbda16=vec128#69,<selw0105=vec128#24
# asm 2: shufb >2p2p2pcb16=$103,<2pconsts=$38,<aacbbbda16=$71,<selw0105=$26
shufb $103,$38,$71,$26

# qhasm: int32323232 tmp9 =  aacbbbda9 << 16
# asm 1: shli >tmp9=vec128#102,<aacbbbda9=vec128#64,16
# asm 2: shli >tmp9=$104,<aacbbbda9=$66,16
shli $104,$66,16

# qhasm: 2p2p2pcb17 = combine 2pconsts and aacbbbda17 by selw0105
# asm 1: shufb >2p2p2pcb17=vec128#103,<2pconsts=vec128#36,<aacbbbda17=vec128#70,<selw0105=vec128#24
# asm 2: shufb >2p2p2pcb17=$105,<2pconsts=$38,<aacbbbda17=$72,<selw0105=$26
shufb $105,$38,$72,$26

# qhasm: int32323232 tmp10 = aacbbbda10 << 16
# asm 1: shli >tmp10=vec128#104,<aacbbbda10=vec128#56,16
# asm 2: shli >tmp10=$106,<aacbbbda10=$58,16
shli $106,$58,16

# qhasm: 2p2p2pcb18 = combine 2pconsts and aacbbbda18 by selw0105
# asm 1: shufb >2p2p2pcb18=vec128#105,<2pconsts=vec128#36,<aacbbbda18=vec128#59,<selw0105=vec128#24
# asm 2: shufb >2p2p2pcb18=$107,<2pconsts=$38,<aacbbbda18=$61,<selw0105=$26
shufb $107,$38,$61,$26

# qhasm: int32323232 tmp11 = aacbbbda11 << 16
# asm 1: shli >tmp11=vec128#106,<aacbbbda11=vec128#68,16
# asm 2: shli >tmp11=$108,<aacbbbda11=$70,16
shli $108,$70,16

# qhasm: 2p2p2pcb19 = combine 2pconsts and aacbbbda19 by selw2325
# asm 1: shufb >2p2p2pcb19=vec128#107,<2pconsts=vec128#36,<aacbbbda19=vec128#73,<selw2325=vec128#25
# asm 2: shufb >2p2p2pcb19=$109,<2pconsts=$38,<aacbbbda19=$75,<selw2325=$27
shufb $109,$38,$75,$27

# qhasm: uint32323232 tmp8  += (tmp8  >> 16) * (a24vec >> 16)
# asm 1: mpyhhau <tmp8=vec128#100,<tmp8=vec128#100,<a24vec=vec128#48
# asm 2: mpyhhau <tmp8=$102,<tmp8=$102,<a24vec=$50
mpyhhau $102,$102,$50

# qhasm: aa_a24aadada0 = combine aacbbbda0 and tmp0 by selw0433
# asm 1: shufb >aa_a24aadada0=vec128#108,<aacbbbda0=vec128#58,<tmp0=vec128#76,<selw0433=vec128#26
# asm 2: shufb >aa_a24aadada0=$110,<aacbbbda0=$60,<tmp0=$78,<selw0433=$28
shufb $110,$60,$78,$28

# qhasm: uint32323232 tmp9  += (tmp9  >> 16) * (a24vec >> 16)
# asm 1: mpyhhau <tmp9=vec128#102,<tmp9=vec128#102,<a24vec=vec128#48
# asm 2: mpyhhau <tmp9=$104,<tmp9=$104,<a24vec=$50
mpyhhau $104,$104,$50

# qhasm: aa_a24aadada1 = combine aacbbbda1 and tmp1 by selw0433
# asm 1: shufb >aa_a24aadada1=vec128#109,<aacbbbda1=vec128#63,<tmp1=vec128#78,<selw0433=vec128#26
# asm 2: shufb >aa_a24aadada1=$111,<aacbbbda1=$65,<tmp1=$80,<selw0433=$28
shufb $111,$65,$80,$28

# qhasm: uint32323232 tmp10 += (tmp10 >> 16) * (a24vec >> 16)
# asm 1: mpyhhau <tmp10=vec128#104,<tmp10=vec128#104,<a24vec=vec128#48
# asm 2: mpyhhau <tmp10=$106,<tmp10=$106,<a24vec=$50
mpyhhau $106,$106,$50

# qhasm: aa_a24aadada2 = combine aacbbbda2 and tmp2 by selw0433
# asm 1: shufb >aa_a24aadada2=vec128#110,<aacbbbda2=vec128#5,<tmp2=vec128#80,<selw0433=vec128#26
# asm 2: shufb >aa_a24aadada2=$112,<aacbbbda2=$7,<tmp2=$82,<selw0433=$28
shufb $112,$7,$82,$28

# qhasm: uint32323232 tmp11 += (tmp11 >> 16) * (a24vec >> 16)
# asm 1: mpyhhau <tmp11=vec128#106,<tmp11=vec128#106,<a24vec=vec128#48
# asm 2: mpyhhau <tmp11=$108,<tmp11=$108,<a24vec=$50
mpyhhau $108,$108,$50

# qhasm: aa_a24aadada3 = combine aacbbbda3 and tmp3 by selw0433
# asm 1: shufb >aa_a24aadada3=vec128#111,<aacbbbda3=vec128#66,<tmp3=vec128#82,<selw0433=vec128#26
# asm 2: shufb >aa_a24aadada3=$113,<aacbbbda3=$68,<tmp3=$84,<selw0433=$28
shufb $113,$68,$84,$28

# qhasm: int32323232 tmp12 = aacbbbda12 << 16
# asm 1: shli >tmp12=vec128#112,<aacbbbda12=vec128#71,16
# asm 2: shli >tmp12=$114,<aacbbbda12=$73,16
shli $114,$73,16

# qhasm: aa_a24aadada4 = combine aacbbbda4 and tmp4 by selw0433
# asm 1: shufb >aa_a24aadada4=vec128#113,<aacbbbda4=vec128#61,<tmp4=vec128#88,<selw0433=vec128#26
# asm 2: shufb >aa_a24aadada4=$115,<aacbbbda4=$63,<tmp4=$90,<selw0433=$28
shufb $115,$63,$90,$28

# qhasm: int32323232 tmp13 = aacbbbda13 << 16
# asm 1: shli >tmp13=vec128#114,<aacbbbda13=vec128#60,16
# asm 2: shli >tmp13=$116,<aacbbbda13=$62,16
shli $116,$62,16

# qhasm: aa_a24aadada5 = combine aacbbbda5 and tmp5 by selw0433
# asm 1: shufb >aa_a24aadada5=vec128#115,<aacbbbda5=vec128#65,<tmp5=vec128#90,<selw0433=vec128#26
# asm 2: shufb >aa_a24aadada5=$117,<aacbbbda5=$67,<tmp5=$92,<selw0433=$28
shufb $117,$67,$92,$28

# qhasm: int32323232 tmp14 = aacbbbda14 << 16
# asm 1: shli >tmp14=vec128#116,<aacbbbda14=vec128#57,16
# asm 2: shli >tmp14=$118,<aacbbbda14=$59,16
shli $118,$59,16

# qhasm: aa_a24aadada6 = combine aacbbbda6 and tmp6 by selw0433
# asm 1: shufb >aa_a24aadada6=vec128#117,<aacbbbda6=vec128#55,<tmp6=vec128#92,<selw0433=vec128#26
# asm 2: shufb >aa_a24aadada6=$119,<aacbbbda6=$57,<tmp6=$94,<selw0433=$28
shufb $119,$57,$94,$28

# qhasm: int32323232 tmp15 = aacbbbda15 << 16
# asm 1: shli >tmp15=vec128#118,<aacbbbda15=vec128#72,16
# asm 2: shli >tmp15=$120,<aacbbbda15=$74,16
shli $120,$74,16

# qhasm: aa_a24aadada7 = combine aacbbbda7 and tmp7 by selw0433
# asm 1: shufb >aa_a24aadada7=vec128#119,<aacbbbda7=vec128#67,<tmp7=vec128#94,<selw0433=vec128#26
# asm 2: shufb >aa_a24aadada7=$121,<aacbbbda7=$69,<tmp7=$96,<selw0433=$28
shufb $121,$69,$96,$28

# qhasm: uint32323232 tmp12 += (tmp12 >> 16) * (a24vec >> 16)
# asm 1: mpyhhau <tmp12=vec128#112,<tmp12=vec128#112,<a24vec=vec128#48
# asm 2: mpyhhau <tmp12=$114,<tmp12=$114,<a24vec=$50
mpyhhau $114,$114,$50

# qhasm: aa_a24aadada8 = combine aacbbbda8 and tmp8 by selw0433
# asm 1: shufb >aa_a24aadada8=vec128#120,<aacbbbda8=vec128#62,<tmp8=vec128#100,<selw0433=vec128#26
# asm 2: shufb >aa_a24aadada8=$122,<aacbbbda8=$64,<tmp8=$102,<selw0433=$28
shufb $122,$64,$102,$28

# qhasm: uint32323232 tmp13 += (tmp13 >> 16) * (a24vec >> 16)
# asm 1: mpyhhau <tmp13=vec128#114,<tmp13=vec128#114,<a24vec=vec128#48
# asm 2: mpyhhau <tmp13=$116,<tmp13=$116,<a24vec=$50
mpyhhau $116,$116,$50

# qhasm: aa_a24aadada9 = combine aacbbbda9 and tmp9 by selw0433
# asm 1: shufb >aa_a24aadada9=vec128#121,<aacbbbda9=vec128#64,<tmp9=vec128#102,<selw0433=vec128#26
# asm 2: shufb >aa_a24aadada9=$123,<aacbbbda9=$66,<tmp9=$104,<selw0433=$28
shufb $123,$66,$104,$28

# qhasm: uint32323232 tmp14 += (tmp14 >> 16) * (a24vec >> 16)
# asm 1: mpyhhau <tmp14=vec128#116,<tmp14=vec128#116,<a24vec=vec128#48
# asm 2: mpyhhau <tmp14=$118,<tmp14=$118,<a24vec=$50
mpyhhau $118,$118,$50

# qhasm: bb_a24m1bbcb0 = combine aacbbbda0 and tmp0 by selw261c0
# asm 1: shufb >bb_a24m1bbcb0=vec128#76,<aacbbbda0=vec128#58,<tmp0=vec128#76,<selw261c0=vec128#27
# asm 2: shufb >bb_a24m1bbcb0=$78,<aacbbbda0=$60,<tmp0=$78,<selw261c0=$29
shufb $78,$60,$78,$29

# qhasm: uint32323232 tmp15 += (tmp15 >> 16) * (a24vec >> 16)
# asm 1: mpyhhau <tmp15=vec128#118,<tmp15=vec128#118,<a24vec=vec128#48
# asm 2: mpyhhau <tmp15=$120,<tmp15=$120,<a24vec=$50
mpyhhau $120,$120,$50

# qhasm: bb_a24m1bbcb1 = combine aacbbbda1 and tmp1 by selw261c0
# asm 1: shufb >bb_a24m1bbcb1=vec128#78,<aacbbbda1=vec128#63,<tmp1=vec128#78,<selw261c0=vec128#27
# asm 2: shufb >bb_a24m1bbcb1=$80,<aacbbbda1=$65,<tmp1=$80,<selw261c0=$29
shufb $80,$65,$80,$29

# qhasm: int32323232 tmp16 = aacbbbda16 << 16
# asm 1: shli >tmp16=vec128#122,<aacbbbda16=vec128#69,16
# asm 2: shli >tmp16=$124,<aacbbbda16=$71,16
shli $124,$71,16

# qhasm: bb_a24m1bbcb2 = combine aacbbbda2 and tmp2 by selw261c0
# asm 1: shufb >bb_a24m1bbcb2=vec128#80,<aacbbbda2=vec128#5,<tmp2=vec128#80,<selw261c0=vec128#27
# asm 2: shufb >bb_a24m1bbcb2=$82,<aacbbbda2=$7,<tmp2=$82,<selw261c0=$29
shufb $82,$7,$82,$29

# qhasm: int32323232 tmp17 = aacbbbda17 << 16
# asm 1: shli >tmp17=vec128#123,<aacbbbda17=vec128#70,16
# asm 2: shli >tmp17=$125,<aacbbbda17=$72,16
shli $125,$72,16

# qhasm: bb_a24m1bbcb3 = combine aacbbbda3 and tmp3 by selw261c0
# asm 1: shufb >bb_a24m1bbcb3=vec128#82,<aacbbbda3=vec128#66,<tmp3=vec128#82,<selw261c0=vec128#27
# asm 2: shufb >bb_a24m1bbcb3=$84,<aacbbbda3=$68,<tmp3=$84,<selw261c0=$29
shufb $84,$68,$84,$29

# qhasm: int32323232 tmp18 = aacbbbda18 << 16
# asm 1: shli >tmp18=vec128#124,<aacbbbda18=vec128#59,16
# asm 2: shli >tmp18=$126,<aacbbbda18=$61,16
shli $126,$61,16

# qhasm: bb_a24m1bbcb4 = combine aacbbbda4 and tmp4 by selw261c0
# asm 1: shufb >bb_a24m1bbcb4=vec128#88,<aacbbbda4=vec128#61,<tmp4=vec128#88,<selw261c0=vec128#27
# asm 2: shufb >bb_a24m1bbcb4=$90,<aacbbbda4=$63,<tmp4=$90,<selw261c0=$29
shufb $90,$63,$90,$29

# qhasm: int32323232 tmp19 = aacbbbda19 << 16
# asm 1: shli >tmp19=vec128#125,<aacbbbda19=vec128#73,16
# asm 2: shli >tmp19=$127,<aacbbbda19=$75,16
shli $127,$75,16

# qhasm: bb_a24m1bbcb5 = combine aacbbbda5 and tmp5 by selw261c0
# asm 1: shufb >bb_a24m1bbcb5=vec128#90,<aacbbbda5=vec128#65,<tmp5=vec128#90,<selw261c0=vec128#27
# asm 2: shufb >bb_a24m1bbcb5=$92,<aacbbbda5=$67,<tmp5=$92,<selw261c0=$29
shufb $92,$67,$92,$29

# qhasm: uint32323232 tmp16 += (tmp16 >> 16) * (a24vec >> 16)
# asm 1: mpyhhau <tmp16=vec128#122,<tmp16=vec128#122,<a24vec=vec128#48
# asm 2: mpyhhau <tmp16=$124,<tmp16=$124,<a24vec=$50
mpyhhau $124,$124,$50

# qhasm: bb_a24m1bbcb6 = combine aacbbbda6 and tmp6 by selw261c0
# asm 1: shufb >bb_a24m1bbcb6=vec128#92,<aacbbbda6=vec128#55,<tmp6=vec128#92,<selw261c0=vec128#27
# asm 2: shufb >bb_a24m1bbcb6=$94,<aacbbbda6=$57,<tmp6=$94,<selw261c0=$29
shufb $94,$57,$94,$29

# qhasm: uint32323232 tmp17 += (tmp17 >> 16) * (a24vec >> 16)
# asm 1: mpyhhau <tmp17=vec128#123,<tmp17=vec128#123,<a24vec=vec128#48
# asm 2: mpyhhau <tmp17=$125,<tmp17=$125,<a24vec=$50
mpyhhau $125,$125,$50

# qhasm: bb_a24m1bbcb7 = combine aacbbbda7 and tmp7 by selw261c0
# asm 1: shufb >bb_a24m1bbcb7=vec128#94,<aacbbbda7=vec128#67,<tmp7=vec128#94,<selw261c0=vec128#27
# asm 2: shufb >bb_a24m1bbcb7=$96,<aacbbbda7=$69,<tmp7=$96,<selw261c0=$29
shufb $96,$69,$96,$29

# qhasm: uint32323232 tmp18 += (tmp18 >> 16) * (a24vec >> 16)
# asm 1: mpyhhau <tmp18=vec128#124,<tmp18=vec128#124,<a24vec=vec128#48
# asm 2: mpyhhau <tmp18=$126,<tmp18=$126,<a24vec=$50
mpyhhau $126,$126,$50

# qhasm: bb_a24m1bbcb8 = combine aacbbbda8 and tmp8 by selw261c0
# asm 1: shufb >bb_a24m1bbcb8=vec128#100,<aacbbbda8=vec128#62,<tmp8=vec128#100,<selw261c0=vec128#27
# asm 2: shufb >bb_a24m1bbcb8=$102,<aacbbbda8=$64,<tmp8=$102,<selw261c0=$29
shufb $102,$64,$102,$29

# qhasm: uint32323232 tmp19 += (tmp19 >> 16) * (a24vec >> 16)
# asm 1: mpyhhau <tmp19=vec128#125,<tmp19=vec128#125,<a24vec=vec128#48
# asm 2: mpyhhau <tmp19=$127,<tmp19=$127,<a24vec=$50
mpyhhau $127,$127,$50

# qhasm: bb_a24m1bbcb9 = combine aacbbbda9 and tmp9 by selw261c0
# asm 1: shufb >bb_a24m1bbcb9=vec128#102,<aacbbbda9=vec128#64,<tmp9=vec128#102,<selw261c0=vec128#27
# asm 2: shufb >bb_a24m1bbcb9=$104,<aacbbbda9=$66,<tmp9=$104,<selw261c0=$29
shufb $104,$66,$104,$29

# qhasm: int32323232 et4t1t00 = aa_a24aadada0 + 2p2p2pcb0
# asm 1: a >et4t1t00=vec128#77,<aa_a24aadada0=vec128#108,<2p2p2pcb0=vec128#77
# asm 2: a >et4t1t00=$79,<aa_a24aadada0=$110,<2p2p2pcb0=$79
a $79,$110,$79

# qhasm: aa_a24aadada10 = combine aacbbbda10 and tmp10 by selw0433
# asm 1: shufb >aa_a24aadada10=vec128#108,<aacbbbda10=vec128#56,<tmp10=vec128#104,<selw0433=vec128#26
# asm 2: shufb >aa_a24aadada10=$110,<aacbbbda10=$58,<tmp10=$106,<selw0433=$28
shufb $110,$58,$106,$28

# qhasm: int32323232 et4t1t01 = aa_a24aadada1 + 2p2p2pcb1
# asm 1: a >et4t1t01=vec128#79,<aa_a24aadada1=vec128#109,<2p2p2pcb1=vec128#79
# asm 2: a >et4t1t01=$81,<aa_a24aadada1=$111,<2p2p2pcb1=$81
a $81,$111,$81

# qhasm: aa_a24aadada11 = combine aacbbbda11 and tmp11 by selw0433
# asm 1: shufb >aa_a24aadada11=vec128#109,<aacbbbda11=vec128#68,<tmp11=vec128#106,<selw0433=vec128#26
# asm 2: shufb >aa_a24aadada11=$111,<aacbbbda11=$70,<tmp11=$108,<selw0433=$28
shufb $111,$70,$108,$28

# qhasm: int32323232 et4t1t02 = aa_a24aadada2 + 2p2p2pcb2
# asm 1: a >et4t1t02=vec128#81,<aa_a24aadada2=vec128#110,<2p2p2pcb2=vec128#81
# asm 2: a >et4t1t02=$83,<aa_a24aadada2=$112,<2p2p2pcb2=$83
a $83,$112,$83

# qhasm: aa_a24aadada12 = combine aacbbbda12 and tmp12 by selw0433
# asm 1: shufb >aa_a24aadada12=vec128#110,<aacbbbda12=vec128#71,<tmp12=vec128#112,<selw0433=vec128#26
# asm 2: shufb >aa_a24aadada12=$112,<aacbbbda12=$73,<tmp12=$114,<selw0433=$28
shufb $112,$73,$114,$28

# qhasm: int32323232 et4t1t03 = aa_a24aadada3 + 2p2p2pcb3
# asm 1: a >et4t1t03=vec128#83,<aa_a24aadada3=vec128#111,<2p2p2pcb3=vec128#83
# asm 2: a >et4t1t03=$85,<aa_a24aadada3=$113,<2p2p2pcb3=$85
a $85,$113,$85

# qhasm: aa_a24aadada13 = combine aacbbbda13 and tmp13 by selw0433
# asm 1: shufb >aa_a24aadada13=vec128#111,<aacbbbda13=vec128#60,<tmp13=vec128#114,<selw0433=vec128#26
# asm 2: shufb >aa_a24aadada13=$113,<aacbbbda13=$62,<tmp13=$116,<selw0433=$28
shufb $113,$62,$116,$28

# qhasm: int32323232 et4t1t04 = aa_a24aadada4 + 2p2p2pcb4
# asm 1: a >et4t1t04=vec128#84,<aa_a24aadada4=vec128#113,<2p2p2pcb4=vec128#84
# asm 2: a >et4t1t04=$86,<aa_a24aadada4=$115,<2p2p2pcb4=$86
a $86,$115,$86

# qhasm: aa_a24aadada14 = combine aacbbbda14 and tmp14 by selw0433
# asm 1: shufb >aa_a24aadada14=vec128#113,<aacbbbda14=vec128#57,<tmp14=vec128#116,<selw0433=vec128#26
# asm 2: shufb >aa_a24aadada14=$115,<aacbbbda14=$59,<tmp14=$118,<selw0433=$28
shufb $115,$59,$118,$28

# qhasm: int32323232 et4t1t05 = aa_a24aadada5 + 2p2p2pcb5
# asm 1: a >et4t1t05=vec128#85,<aa_a24aadada5=vec128#115,<2p2p2pcb5=vec128#85
# asm 2: a >et4t1t05=$87,<aa_a24aadada5=$117,<2p2p2pcb5=$87
a $87,$117,$87

# qhasm: aa_a24aadada15 = combine aacbbbda15 and tmp15 by selw0433
# asm 1: shufb >aa_a24aadada15=vec128#115,<aacbbbda15=vec128#72,<tmp15=vec128#118,<selw0433=vec128#26
# asm 2: shufb >aa_a24aadada15=$117,<aacbbbda15=$74,<tmp15=$120,<selw0433=$28
shufb $117,$74,$120,$28

# qhasm: int32323232 et4t1t06 = aa_a24aadada6 + 2p2p2pcb6
# asm 1: a >et4t1t06=vec128#86,<aa_a24aadada6=vec128#117,<2p2p2pcb6=vec128#86
# asm 2: a >et4t1t06=$88,<aa_a24aadada6=$119,<2p2p2pcb6=$88
a $88,$119,$88

# qhasm: aa_a24aadada16 = combine aacbbbda16 and tmp16 by selw0433
# asm 1: shufb >aa_a24aadada16=vec128#117,<aacbbbda16=vec128#69,<tmp16=vec128#122,<selw0433=vec128#26
# asm 2: shufb >aa_a24aadada16=$119,<aacbbbda16=$71,<tmp16=$124,<selw0433=$28
shufb $119,$71,$124,$28

# qhasm: int32323232 et4t1t07 = aa_a24aadada7 + 2p2p2pcb7
# asm 1: a >et4t1t07=vec128#87,<aa_a24aadada7=vec128#119,<2p2p2pcb7=vec128#87
# asm 2: a >et4t1t07=$89,<aa_a24aadada7=$121,<2p2p2pcb7=$89
a $89,$121,$89

# qhasm: aa_a24aadada17 = combine aacbbbda17 and tmp17 by selw0433
# asm 1: shufb >aa_a24aadada17=vec128#119,<aacbbbda17=vec128#70,<tmp17=vec128#123,<selw0433=vec128#26
# asm 2: shufb >aa_a24aadada17=$121,<aacbbbda17=$72,<tmp17=$125,<selw0433=$28
shufb $121,$72,$125,$28

# qhasm: int32323232 et4t1t08 = aa_a24aadada8 + 2p2p2pcb8
# asm 1: a >et4t1t08=vec128#89,<aa_a24aadada8=vec128#120,<2p2p2pcb8=vec128#89
# asm 2: a >et4t1t08=$91,<aa_a24aadada8=$122,<2p2p2pcb8=$91
a $91,$122,$91

# qhasm: aa_a24aadada18 = combine aacbbbda18 and tmp18 by selw0433
# asm 1: shufb >aa_a24aadada18=vec128#120,<aacbbbda18=vec128#59,<tmp18=vec128#124,<selw0433=vec128#26
# asm 2: shufb >aa_a24aadada18=$122,<aacbbbda18=$61,<tmp18=$126,<selw0433=$28
shufb $122,$61,$126,$28

# qhasm: int32323232 et4t1t09 = aa_a24aadada9 + 2p2p2pcb9
# asm 1: a >et4t1t09=vec128#91,<aa_a24aadada9=vec128#121,<2p2p2pcb9=vec128#91
# asm 2: a >et4t1t09=$93,<aa_a24aadada9=$123,<2p2p2pcb9=$93
a $93,$123,$93

# qhasm: aa_a24aadada19 = combine aacbbbda19 and tmp19 by selw0433
# asm 1: shufb >aa_a24aadada19=vec128#121,<aacbbbda19=vec128#73,<tmp19=vec128#125,<selw0433=vec128#26
# asm 2: shufb >aa_a24aadada19=$123,<aacbbbda19=$75,<tmp19=$127,<selw0433=$28
shufb $123,$75,$127,$28

# qhasm: int32323232 et4t1t010 = aa_a24aadada10 + 2p2p2pcb10
# asm 1: a >et4t1t010=vec128#93,<aa_a24aadada10=vec128#108,<2p2p2pcb10=vec128#93
# asm 2: a >et4t1t010=$95,<aa_a24aadada10=$110,<2p2p2pcb10=$95
a $95,$110,$95

# qhasm: bb_a24m1bbcb10 = combine aacbbbda10 and tmp10 by selw261c0
# asm 1: shufb >bb_a24m1bbcb10=vec128#104,<aacbbbda10=vec128#56,<tmp10=vec128#104,<selw261c0=vec128#27
# asm 2: shufb >bb_a24m1bbcb10=$106,<aacbbbda10=$58,<tmp10=$106,<selw261c0=$29
shufb $106,$58,$106,$29

# qhasm: int32323232 et4t1t011 = aa_a24aadada11 + 2p2p2pcb11
# asm 1: a >et4t1t011=vec128#95,<aa_a24aadada11=vec128#109,<2p2p2pcb11=vec128#95
# asm 2: a >et4t1t011=$97,<aa_a24aadada11=$111,<2p2p2pcb11=$97
a $97,$111,$97

# qhasm: bb_a24m1bbcb11 = combine aacbbbda11 and tmp11 by selw261c0
# asm 1: shufb >bb_a24m1bbcb11=vec128#106,<aacbbbda11=vec128#68,<tmp11=vec128#106,<selw261c0=vec128#27
# asm 2: shufb >bb_a24m1bbcb11=$108,<aacbbbda11=$70,<tmp11=$108,<selw261c0=$29
shufb $108,$70,$108,$29

# qhasm: int32323232 et4t1t012 = aa_a24aadada12 + 2p2p2pcb12
# asm 1: a >et4t1t012=vec128#96,<aa_a24aadada12=vec128#110,<2p2p2pcb12=vec128#96
# asm 2: a >et4t1t012=$98,<aa_a24aadada12=$112,<2p2p2pcb12=$98
a $98,$112,$98

# qhasm: bb_a24m1bbcb12 = combine aacbbbda12 and tmp12 by selw261c0
# asm 1: shufb >bb_a24m1bbcb12=vec128#108,<aacbbbda12=vec128#71,<tmp12=vec128#112,<selw261c0=vec128#27
# asm 2: shufb >bb_a24m1bbcb12=$110,<aacbbbda12=$73,<tmp12=$114,<selw261c0=$29
shufb $110,$73,$114,$29

# qhasm: int32323232 et4t1t013 = aa_a24aadada13 + 2p2p2pcb13
# asm 1: a >et4t1t013=vec128#97,<aa_a24aadada13=vec128#111,<2p2p2pcb13=vec128#97
# asm 2: a >et4t1t013=$99,<aa_a24aadada13=$113,<2p2p2pcb13=$99
a $99,$113,$99

# qhasm: bb_a24m1bbcb13 = combine aacbbbda13 and tmp13 by selw261c0
# asm 1: shufb >bb_a24m1bbcb13=vec128#109,<aacbbbda13=vec128#60,<tmp13=vec128#114,<selw261c0=vec128#27
# asm 2: shufb >bb_a24m1bbcb13=$111,<aacbbbda13=$62,<tmp13=$116,<selw261c0=$29
shufb $111,$62,$116,$29

# qhasm: int32323232 et4t1t014 = aa_a24aadada14 + 2p2p2pcb14
# asm 1: a >et4t1t014=vec128#98,<aa_a24aadada14=vec128#113,<2p2p2pcb14=vec128#98
# asm 2: a >et4t1t014=$100,<aa_a24aadada14=$115,<2p2p2pcb14=$100
a $100,$115,$100

# qhasm: bb_a24m1bbcb14 = combine aacbbbda14 and tmp14 by selw261c0
# asm 1: shufb >bb_a24m1bbcb14=vec128#110,<aacbbbda14=vec128#57,<tmp14=vec128#116,<selw261c0=vec128#27
# asm 2: shufb >bb_a24m1bbcb14=$112,<aacbbbda14=$59,<tmp14=$118,<selw261c0=$29
shufb $112,$59,$118,$29

# qhasm: int32323232 et4t1t015 = aa_a24aadada15 + 2p2p2pcb15
# asm 1: a >et4t1t015=vec128#99,<aa_a24aadada15=vec128#115,<2p2p2pcb15=vec128#99
# asm 2: a >et4t1t015=$101,<aa_a24aadada15=$117,<2p2p2pcb15=$101
a $101,$117,$101

# qhasm: bb_a24m1bbcb15 = combine aacbbbda15 and tmp15 by selw261c0
# asm 1: shufb >bb_a24m1bbcb15=vec128#111,<aacbbbda15=vec128#72,<tmp15=vec128#118,<selw261c0=vec128#27
# asm 2: shufb >bb_a24m1bbcb15=$113,<aacbbbda15=$74,<tmp15=$120,<selw261c0=$29
shufb $113,$74,$120,$29

# qhasm: int32323232 et4t1t016 = aa_a24aadada16 + 2p2p2pcb16
# asm 1: a >et4t1t016=vec128#101,<aa_a24aadada16=vec128#117,<2p2p2pcb16=vec128#101
# asm 2: a >et4t1t016=$103,<aa_a24aadada16=$119,<2p2p2pcb16=$103
a $103,$119,$103

# qhasm: bb_a24m1bbcb16 = combine aacbbbda16 and tmp16 by selw261c0
# asm 1: shufb >bb_a24m1bbcb16=vec128#112,<aacbbbda16=vec128#69,<tmp16=vec128#122,<selw261c0=vec128#27
# asm 2: shufb >bb_a24m1bbcb16=$114,<aacbbbda16=$71,<tmp16=$124,<selw261c0=$29
shufb $114,$71,$124,$29

# qhasm: int32323232 et4t1t017 = aa_a24aadada17 + 2p2p2pcb17
# asm 1: a >et4t1t017=vec128#103,<aa_a24aadada17=vec128#119,<2p2p2pcb17=vec128#103
# asm 2: a >et4t1t017=$105,<aa_a24aadada17=$121,<2p2p2pcb17=$105
a $105,$121,$105

# qhasm: bb_a24m1bbcb17 = combine aacbbbda17 and tmp17 by selw261c0
# asm 1: shufb >bb_a24m1bbcb17=vec128#113,<aacbbbda17=vec128#70,<tmp17=vec128#123,<selw261c0=vec128#27
# asm 2: shufb >bb_a24m1bbcb17=$115,<aacbbbda17=$72,<tmp17=$125,<selw261c0=$29
shufb $115,$72,$125,$29

# qhasm: int32323232 et4t1t018 = aa_a24aadada18 + 2p2p2pcb18
# asm 1: a >et4t1t018=vec128#105,<aa_a24aadada18=vec128#120,<2p2p2pcb18=vec128#105
# asm 2: a >et4t1t018=$107,<aa_a24aadada18=$122,<2p2p2pcb18=$107
a $107,$122,$107

# qhasm: bb_a24m1bbcb18 = combine aacbbbda18 and tmp18 by selw261c0
# asm 1: shufb >bb_a24m1bbcb18=vec128#114,<aacbbbda18=vec128#59,<tmp18=vec128#124,<selw261c0=vec128#27
# asm 2: shufb >bb_a24m1bbcb18=$116,<aacbbbda18=$61,<tmp18=$126,<selw261c0=$29
shufb $116,$61,$126,$29

# qhasm: int32323232 et4t1t019 = aa_a24aadada19 + 2p2p2pcb19
# asm 1: a >et4t1t019=vec128#107,<aa_a24aadada19=vec128#121,<2p2p2pcb19=vec128#107
# asm 2: a >et4t1t019=$109,<aa_a24aadada19=$123,<2p2p2pcb19=$109
a $109,$123,$109

# qhasm: bb_a24m1bbcb19 = combine aacbbbda19 and tmp19 by selw261c0
# asm 1: shufb >bb_a24m1bbcb19=vec128#115,<aacbbbda19=vec128#73,<tmp19=vec128#125,<selw261c0=vec128#27
# asm 2: shufb >bb_a24m1bbcb19=$117,<aacbbbda19=$75,<tmp19=$127,<selw261c0=$29
shufb $117,$75,$127,$29

# qhasm: int32323232 et4t1t00 = et4t1t00 - bb_a24m1bbcb0 
# asm 1: sf >et4t1t00=vec128#76,<bb_a24m1bbcb0=vec128#76,<et4t1t00=vec128#77
# asm 2: sf >et4t1t00=$78,<bb_a24m1bbcb0=$78,<et4t1t00=$79
sf $78,$78,$79

# qhasm: int32323232 et4t1t01 = et4t1t01 - bb_a24m1bbcb1
# asm 1: sf >et4t1t01=vec128#77,<bb_a24m1bbcb1=vec128#78,<et4t1t01=vec128#79
# asm 2: sf >et4t1t01=$79,<bb_a24m1bbcb1=$80,<et4t1t01=$81
sf $79,$80,$81

# qhasm: int32323232 et4t1t02 = et4t1t02 - bb_a24m1bbcb2
# asm 1: sf >et4t1t02=vec128#78,<bb_a24m1bbcb2=vec128#80,<et4t1t02=vec128#81
# asm 2: sf >et4t1t02=$80,<bb_a24m1bbcb2=$82,<et4t1t02=$83
sf $80,$82,$83

# qhasm: int32323232 et4t1t03 = et4t1t03 - bb_a24m1bbcb3
# asm 1: sf >et4t1t03=vec128#79,<bb_a24m1bbcb3=vec128#82,<et4t1t03=vec128#83
# asm 2: sf >et4t1t03=$81,<bb_a24m1bbcb3=$84,<et4t1t03=$85
sf $81,$84,$85

# qhasm: int32323232 et4t1t04 = et4t1t04 - bb_a24m1bbcb4
# asm 1: sf >et4t1t04=vec128#80,<bb_a24m1bbcb4=vec128#88,<et4t1t04=vec128#84
# asm 2: sf >et4t1t04=$82,<bb_a24m1bbcb4=$90,<et4t1t04=$86
sf $82,$90,$86

# qhasm: int32323232 et4t1t05 = et4t1t05 - bb_a24m1bbcb5
# asm 1: sf >et4t1t05=vec128#81,<bb_a24m1bbcb5=vec128#90,<et4t1t05=vec128#85
# asm 2: sf >et4t1t05=$83,<bb_a24m1bbcb5=$92,<et4t1t05=$87
sf $83,$92,$87

# qhasm: int32323232 et4t1t06 = et4t1t06 - bb_a24m1bbcb6
# asm 1: sf >et4t1t06=vec128#82,<bb_a24m1bbcb6=vec128#92,<et4t1t06=vec128#86
# asm 2: sf >et4t1t06=$84,<bb_a24m1bbcb6=$94,<et4t1t06=$88
sf $84,$94,$88

# qhasm: int32323232 et4t1t07 = et4t1t07 - bb_a24m1bbcb7
# asm 1: sf >et4t1t07=vec128#83,<bb_a24m1bbcb7=vec128#94,<et4t1t07=vec128#87
# asm 2: sf >et4t1t07=$85,<bb_a24m1bbcb7=$96,<et4t1t07=$89
sf $85,$96,$89

# qhasm: int32323232 et4t1t08 = et4t1t08 - bb_a24m1bbcb8
# asm 1: sf >et4t1t08=vec128#84,<bb_a24m1bbcb8=vec128#100,<et4t1t08=vec128#89
# asm 2: sf >et4t1t08=$86,<bb_a24m1bbcb8=$102,<et4t1t08=$91
sf $86,$102,$91

# qhasm: int32323232 et4t1t09 = et4t1t09 - bb_a24m1bbcb9
# asm 1: sf >et4t1t09=vec128#85,<bb_a24m1bbcb9=vec128#102,<et4t1t09=vec128#91
# asm 2: sf >et4t1t09=$87,<bb_a24m1bbcb9=$104,<et4t1t09=$93
sf $87,$104,$93

# qhasm: int32323232 et4t1t010 = et4t1t010 - bb_a24m1bbcb10
# asm 1: sf >et4t1t010=vec128#86,<bb_a24m1bbcb10=vec128#104,<et4t1t010=vec128#93
# asm 2: sf >et4t1t010=$88,<bb_a24m1bbcb10=$106,<et4t1t010=$95
sf $88,$106,$95

# qhasm: int32323232 et4t1t011 = et4t1t011 - bb_a24m1bbcb11
# asm 1: sf >et4t1t011=vec128#87,<bb_a24m1bbcb11=vec128#106,<et4t1t011=vec128#95
# asm 2: sf >et4t1t011=$89,<bb_a24m1bbcb11=$108,<et4t1t011=$97
sf $89,$108,$97

# qhasm: int32323232 et4t1t012 = et4t1t012 - bb_a24m1bbcb12
# asm 1: sf >et4t1t012=vec128#88,<bb_a24m1bbcb12=vec128#108,<et4t1t012=vec128#96
# asm 2: sf >et4t1t012=$90,<bb_a24m1bbcb12=$110,<et4t1t012=$98
sf $90,$110,$98

# qhasm: int32323232 et4t1t013 = et4t1t013 - bb_a24m1bbcb13
# asm 1: sf >et4t1t013=vec128#89,<bb_a24m1bbcb13=vec128#109,<et4t1t013=vec128#97
# asm 2: sf >et4t1t013=$91,<bb_a24m1bbcb13=$111,<et4t1t013=$99
sf $91,$111,$99

# qhasm: int32323232 et4t1t014 = et4t1t014 - bb_a24m1bbcb14
# asm 1: sf >et4t1t014=vec128#90,<bb_a24m1bbcb14=vec128#110,<et4t1t014=vec128#98
# asm 2: sf >et4t1t014=$92,<bb_a24m1bbcb14=$112,<et4t1t014=$100
sf $92,$112,$100

# qhasm: int32323232 et4t1t015 = et4t1t015 - bb_a24m1bbcb15
# asm 1: sf >et4t1t015=vec128#91,<bb_a24m1bbcb15=vec128#111,<et4t1t015=vec128#99
# asm 2: sf >et4t1t015=$93,<bb_a24m1bbcb15=$113,<et4t1t015=$101
sf $93,$113,$101

# qhasm: int32323232 et4t1t016 = et4t1t016 - bb_a24m1bbcb16
# asm 1: sf >et4t1t016=vec128#92,<bb_a24m1bbcb16=vec128#112,<et4t1t016=vec128#101
# asm 2: sf >et4t1t016=$94,<bb_a24m1bbcb16=$114,<et4t1t016=$103
sf $94,$114,$103

# qhasm: int32323232 et4t1t017 = et4t1t017 - bb_a24m1bbcb17
# asm 1: sf >et4t1t017=vec128#93,<bb_a24m1bbcb17=vec128#113,<et4t1t017=vec128#103
# asm 2: sf >et4t1t017=$95,<bb_a24m1bbcb17=$115,<et4t1t017=$105
sf $95,$115,$105

# qhasm: int32323232 et4t1t018 = et4t1t018 - bb_a24m1bbcb18
# asm 1: sf >et4t1t018=vec128#94,<bb_a24m1bbcb18=vec128#114,<et4t1t018=vec128#105
# asm 2: sf >et4t1t018=$96,<bb_a24m1bbcb18=$116,<et4t1t018=$107
sf $96,$116,$107

# qhasm: int32323232 et4t1t019 = et4t1t019 - bb_a24m1bbcb19
# asm 1: sf >et4t1t019=vec128#95,<bb_a24m1bbcb19=vec128#115,<et4t1t019=vec128#107
# asm 2: sf >et4t1t019=$97,<bb_a24m1bbcb19=$117,<et4t1t019=$109
sf $97,$117,$109

# qhasm: uint32323232 carry0 = et4t1t016 >> 13
# asm 1: rotmi >carry0=vec128#96,<et4t1t016=vec128#92,-13
# asm 2: rotmi >carry0=$98,<et4t1t016=$94,-13
rotmi $98,$94,-13

# qhasm: int32323232 et4t1t017 += carry0
# asm 1: a >et4t1t017=vec128#93,<et4t1t017=vec128#93,<carry0=vec128#96
# asm 2: a >et4t1t017=$95,<et4t1t017=$95,<carry0=$98
a $95,$95,$98

# qhasm: uint32323232 carry1 = et4t1t017 >> 13
# asm 1: rotmi >carry1=vec128#96,<et4t1t017=vec128#93,-13
# asm 2: rotmi >carry1=$98,<et4t1t017=$95,-13
rotmi $98,$95,-13

# qhasm: int32323232 et4t1t018 += carry1
# asm 1: a >et4t1t018=vec128#94,<et4t1t018=vec128#94,<carry1=vec128#96
# asm 2: a >et4t1t018=$96,<et4t1t018=$96,<carry1=$98
a $96,$96,$98

# qhasm: uint32323232 carry = et4t1t018 >> 13
# asm 1: rotmi >carry=vec128#96,<et4t1t018=vec128#94,-13
# asm 2: rotmi >carry=$98,<et4t1t018=$96,-13
rotmi $98,$96,-13

# qhasm: int32323232 et4t1t019 += carry
# asm 1: a >et4t1t019=vec128#95,<et4t1t019=vec128#95,<carry=vec128#96
# asm 2: a >et4t1t019=$97,<et4t1t019=$97,<carry=$98
a $97,$97,$98

# qhasm: uint32323232 carry = et4t1t019 >> 12
# asm 1: rotmi >carry=vec128#96,<et4t1t019=vec128#95,-12
# asm 2: rotmi >carry=$98,<et4t1t019=$97,-12
rotmi $98,$97,-12

# qhasm: int32323232 red = carry << 4
# asm 1: shli >red=vec128#97,<carry=vec128#96,4
# asm 2: shli >red=$99,<carry=$98,4
shli $99,$98,4

# qhasm: int32323232 red += carry
# asm 1: a >red=vec128#97,<red=vec128#97,<carry=vec128#96
# asm 2: a >red=$99,<red=$99,<carry=$98
a $99,$99,$98

# qhasm: int32323232 red += carry
# asm 1: a >red=vec128#97,<red=vec128#97,<carry=vec128#96
# asm 2: a >red=$99,<red=$99,<carry=$98
a $99,$99,$98

# qhasm: int32323232 red += carry
# asm 1: a >red=vec128#96,<red=vec128#97,<carry=vec128#96
# asm 2: a >red=$98,<red=$99,<carry=$98
a $98,$99,$98

# qhasm: int32323232 et4t1t00 += red
# asm 1: a >et4t1t00=vec128#76,<et4t1t00=vec128#76,<red=vec128#96
# asm 2: a >et4t1t00=$78,<et4t1t00=$78,<red=$98
a $78,$78,$98

# qhasm: et4t1t016 &= mask13
# asm 1: and >et4t1t016=vec128#92,<et4t1t016=vec128#92,<mask13=vec128#22
# asm 2: and >et4t1t016=$94,<et4t1t016=$94,<mask13=$24
and $94,$94,$24

# qhasm: et4t1t017 &= mask13
# asm 1: and >et4t1t017=vec128#93,<et4t1t017=vec128#93,<mask13=vec128#22
# asm 2: and >et4t1t017=$95,<et4t1t017=$95,<mask13=$24
and $95,$95,$24

# qhasm: et4t1t018 &= mask13
# asm 1: and >et4t1t018=vec128#94,<et4t1t018=vec128#94,<mask13=vec128#22
# asm 2: and >et4t1t018=$96,<et4t1t018=$96,<mask13=$24
and $96,$96,$24

# qhasm: et4t1t019 &= mask12
# asm 1: and >et4t1t019=vec128#95,<et4t1t019=vec128#95,<mask12=vec128#21
# asm 2: and >et4t1t019=$97,<et4t1t019=$97,<mask12=$23
and $97,$97,$23

# qhasm: uint32323232 carry0 = et4t1t00  >> 13
# asm 1: rotmi >carry0=vec128#96,<et4t1t00=vec128#76,-13
# asm 2: rotmi >carry0=$98,<et4t1t00=$78,-13
rotmi $98,$78,-13

# qhasm: uint32323232 carry1 = et4t1t04  >> 13
# asm 1: rotmi >carry1=vec128#97,<et4t1t04=vec128#80,-13
# asm 2: rotmi >carry1=$99,<et4t1t04=$82,-13
rotmi $99,$82,-13

# qhasm: uint32323232 carry2 = et4t1t08  >> 13
# asm 1: rotmi >carry2=vec128#98,<et4t1t08=vec128#84,-13
# asm 2: rotmi >carry2=$100,<et4t1t08=$86,-13
rotmi $100,$86,-13

# qhasm: uint32323232 carry3 = et4t1t012 >> 13
# asm 1: rotmi >carry3=vec128#99,<et4t1t012=vec128#88,-13
# asm 2: rotmi >carry3=$101,<et4t1t012=$90,-13
rotmi $101,$90,-13

# qhasm: et4t1t00  &= mask13
# asm 1: and >et4t1t00=vec128#76,<et4t1t00=vec128#76,<mask13=vec128#22
# asm 2: and >et4t1t00=$78,<et4t1t00=$78,<mask13=$24
and $78,$78,$24

# qhasm: et4t1t04  &= mask13
# asm 1: and >et4t1t04=vec128#80,<et4t1t04=vec128#80,<mask13=vec128#22
# asm 2: and >et4t1t04=$82,<et4t1t04=$82,<mask13=$24
and $82,$82,$24

# qhasm: et4t1t08  &= mask13
# asm 1: and >et4t1t08=vec128#84,<et4t1t08=vec128#84,<mask13=vec128#22
# asm 2: and >et4t1t08=$86,<et4t1t08=$86,<mask13=$24
and $86,$86,$24

# qhasm: et4t1t012 &= mask13
# asm 1: and >et4t1t012=vec128#88,<et4t1t012=vec128#88,<mask13=vec128#22
# asm 2: and >et4t1t012=$90,<et4t1t012=$90,<mask13=$24
and $90,$90,$24

# qhasm: int32323232 et4t1t01  += carry0
# asm 1: a >et4t1t01=vec128#77,<et4t1t01=vec128#77,<carry0=vec128#96
# asm 2: a >et4t1t01=$79,<et4t1t01=$79,<carry0=$98
a $79,$79,$98

# qhasm: int32323232 et4t1t05  += carry1
# asm 1: a >et4t1t05=vec128#81,<et4t1t05=vec128#81,<carry1=vec128#97
# asm 2: a >et4t1t05=$83,<et4t1t05=$83,<carry1=$99
a $83,$83,$99

# qhasm: int32323232 et4t1t09  += carry2
# asm 1: a >et4t1t09=vec128#85,<et4t1t09=vec128#85,<carry2=vec128#98
# asm 2: a >et4t1t09=$87,<et4t1t09=$87,<carry2=$100
a $87,$87,$100

# qhasm: int32323232 et4t1t013 += carry3
# asm 1: a >et4t1t013=vec128#89,<et4t1t013=vec128#89,<carry3=vec128#99
# asm 2: a >et4t1t013=$91,<et4t1t013=$91,<carry3=$101
a $91,$91,$101

# qhasm: uint32323232 carry0 = et4t1t01  >> 13
# asm 1: rotmi >carry0=vec128#96,<et4t1t01=vec128#77,-13
# asm 2: rotmi >carry0=$98,<et4t1t01=$79,-13
rotmi $98,$79,-13

# qhasm: uint32323232 carry1 = et4t1t05  >> 13
# asm 1: rotmi >carry1=vec128#97,<et4t1t05=vec128#81,-13
# asm 2: rotmi >carry1=$99,<et4t1t05=$83,-13
rotmi $99,$83,-13

# qhasm: uint32323232 carry2 = et4t1t09  >> 13
# asm 1: rotmi >carry2=vec128#98,<et4t1t09=vec128#85,-13
# asm 2: rotmi >carry2=$100,<et4t1t09=$87,-13
rotmi $100,$87,-13

# qhasm: uint32323232 carry3 = et4t1t013 >> 13
# asm 1: rotmi >carry3=vec128#99,<et4t1t013=vec128#89,-13
# asm 2: rotmi >carry3=$101,<et4t1t013=$91,-13
rotmi $101,$91,-13

# qhasm: et4t1t01  &= mask13
# asm 1: and >et4t1t01=vec128#77,<et4t1t01=vec128#77,<mask13=vec128#22
# asm 2: and >et4t1t01=$79,<et4t1t01=$79,<mask13=$24
and $79,$79,$24

# qhasm: et4t1t05  &= mask13
# asm 1: and >et4t1t05=vec128#81,<et4t1t05=vec128#81,<mask13=vec128#22
# asm 2: and >et4t1t05=$83,<et4t1t05=$83,<mask13=$24
and $83,$83,$24

# qhasm: et4t1t09  &= mask13
# asm 1: and >et4t1t09=vec128#85,<et4t1t09=vec128#85,<mask13=vec128#22
# asm 2: and >et4t1t09=$87,<et4t1t09=$87,<mask13=$24
and $87,$87,$24

# qhasm: et4t1t013 &= mask13
# asm 1: and >et4t1t013=vec128#89,<et4t1t013=vec128#89,<mask13=vec128#22
# asm 2: and >et4t1t013=$91,<et4t1t013=$91,<mask13=$24
and $91,$91,$24

# qhasm: int32323232 et4t1t02  += carry0
# asm 1: a >et4t1t02=vec128#78,<et4t1t02=vec128#78,<carry0=vec128#96
# asm 2: a >et4t1t02=$80,<et4t1t02=$80,<carry0=$98
a $80,$80,$98

# qhasm: int32323232 et4t1t06  += carry1
# asm 1: a >et4t1t06=vec128#82,<et4t1t06=vec128#82,<carry1=vec128#97
# asm 2: a >et4t1t06=$84,<et4t1t06=$84,<carry1=$99
a $84,$84,$99

# qhasm: int32323232 et4t1t010 += carry2
# asm 1: a >et4t1t010=vec128#86,<et4t1t010=vec128#86,<carry2=vec128#98
# asm 2: a >et4t1t010=$88,<et4t1t010=$88,<carry2=$100
a $88,$88,$100

# qhasm: int32323232 et4t1t014 += carry3
# asm 1: a >et4t1t014=vec128#90,<et4t1t014=vec128#90,<carry3=vec128#99
# asm 2: a >et4t1t014=$92,<et4t1t014=$92,<carry3=$101
a $92,$92,$101

# qhasm: uint32323232 carry0 = et4t1t02  >> 13
# asm 1: rotmi >carry0=vec128#96,<et4t1t02=vec128#78,-13
# asm 2: rotmi >carry0=$98,<et4t1t02=$80,-13
rotmi $98,$80,-13

# qhasm: uint32323232 carry1 = et4t1t06  >> 13
# asm 1: rotmi >carry1=vec128#97,<et4t1t06=vec128#82,-13
# asm 2: rotmi >carry1=$99,<et4t1t06=$84,-13
rotmi $99,$84,-13

# qhasm: uint32323232 carry2 = et4t1t010 >> 13
# asm 1: rotmi >carry2=vec128#98,<et4t1t010=vec128#86,-13
# asm 2: rotmi >carry2=$100,<et4t1t010=$88,-13
rotmi $100,$88,-13

# qhasm: uint32323232 carry3 = et4t1t014 >> 13
# asm 1: rotmi >carry3=vec128#99,<et4t1t014=vec128#90,-13
# asm 2: rotmi >carry3=$101,<et4t1t014=$92,-13
rotmi $101,$92,-13

# qhasm: et4t1t02  &= mask13
# asm 1: and >et4t1t02=vec128#78,<et4t1t02=vec128#78,<mask13=vec128#22
# asm 2: and >et4t1t02=$80,<et4t1t02=$80,<mask13=$24
and $80,$80,$24

# qhasm: et4t1t06  &= mask13
# asm 1: and >et4t1t06=vec128#82,<et4t1t06=vec128#82,<mask13=vec128#22
# asm 2: and >et4t1t06=$84,<et4t1t06=$84,<mask13=$24
and $84,$84,$24

# qhasm: et4t1t010 &= mask13
# asm 1: and >et4t1t010=vec128#86,<et4t1t010=vec128#86,<mask13=vec128#22
# asm 2: and >et4t1t010=$88,<et4t1t010=$88,<mask13=$24
and $88,$88,$24

# qhasm: et4t1t014 &= mask13
# asm 1: and >et4t1t014=vec128#90,<et4t1t014=vec128#90,<mask13=vec128#22
# asm 2: and >et4t1t014=$92,<et4t1t014=$92,<mask13=$24
and $92,$92,$24

# qhasm: int32323232 et4t1t03  += carry0
# asm 1: a >et4t1t03=vec128#79,<et4t1t03=vec128#79,<carry0=vec128#96
# asm 2: a >et4t1t03=$81,<et4t1t03=$81,<carry0=$98
a $81,$81,$98

# qhasm: int32323232 et4t1t07  += carry1
# asm 1: a >et4t1t07=vec128#83,<et4t1t07=vec128#83,<carry1=vec128#97
# asm 2: a >et4t1t07=$85,<et4t1t07=$85,<carry1=$99
a $85,$85,$99

# qhasm: int32323232 et4t1t011 += carry2
# asm 1: a >et4t1t011=vec128#87,<et4t1t011=vec128#87,<carry2=vec128#98
# asm 2: a >et4t1t011=$89,<et4t1t011=$89,<carry2=$100
a $89,$89,$100

# qhasm: int32323232 et4t1t015 += carry3
# asm 1: a >et4t1t015=vec128#91,<et4t1t015=vec128#91,<carry3=vec128#99
# asm 2: a >et4t1t015=$93,<et4t1t015=$93,<carry3=$101
a $93,$93,$101

# qhasm: uint32323232 carry0 = et4t1t03  >> 12
# asm 1: rotmi >carry0=vec128#96,<et4t1t03=vec128#79,-12
# asm 2: rotmi >carry0=$98,<et4t1t03=$81,-12
rotmi $98,$81,-12

# qhasm: uint32323232 carry1 = et4t1t07  >> 12
# asm 1: rotmi >carry1=vec128#97,<et4t1t07=vec128#83,-12
# asm 2: rotmi >carry1=$99,<et4t1t07=$85,-12
rotmi $99,$85,-12

# qhasm: uint32323232 carry2 = et4t1t011 >> 12
# asm 1: rotmi >carry2=vec128#98,<et4t1t011=vec128#87,-12
# asm 2: rotmi >carry2=$100,<et4t1t011=$89,-12
rotmi $100,$89,-12

# qhasm: uint32323232 carry3 = et4t1t015 >> 12
# asm 1: rotmi >carry3=vec128#99,<et4t1t015=vec128#91,-12
# asm 2: rotmi >carry3=$101,<et4t1t015=$93,-12
rotmi $101,$93,-12

# qhasm: et4t1t03  &= mask12
# asm 1: and >et4t1t03=vec128#79,<et4t1t03=vec128#79,<mask12=vec128#21
# asm 2: and >et4t1t03=$81,<et4t1t03=$81,<mask12=$23
and $81,$81,$23

# qhasm: et4t1t07  &= mask12
# asm 1: and >et4t1t07=vec128#83,<et4t1t07=vec128#83,<mask12=vec128#21
# asm 2: and >et4t1t07=$85,<et4t1t07=$85,<mask12=$23
and $85,$85,$23

# qhasm: et4t1t011 &= mask12
# asm 1: and >et4t1t011=vec128#87,<et4t1t011=vec128#87,<mask12=vec128#21
# asm 2: and >et4t1t011=$89,<et4t1t011=$89,<mask12=$23
and $89,$89,$23

# qhasm: et0aat10 = combine et4t1t00 and aacbbbda0 by selw0342
# asm 1: shufb >et0aat10=vec128#100,<et4t1t00=vec128#76,<aacbbbda0=vec128#58,<selw0342=vec128#28
# asm 2: shufb >et0aat10=$102,<et4t1t00=$78,<aacbbbda0=$60,<selw0342=$30
shufb $102,$78,$60,$30

# qhasm: et4t1t015 &= mask12
# asm 1: and >et4t1t015=vec128#91,<et4t1t015=vec128#91,<mask12=vec128#21
# asm 2: and >et4t1t015=$93,<et4t1t015=$93,<mask12=$23
and $93,$93,$23

# qhasm: et0aat11 = combine et4t1t01 and aacbbbda1 by selw0342
# asm 1: shufb >et0aat11=vec128#101,<et4t1t01=vec128#77,<aacbbbda1=vec128#63,<selw0342=vec128#28
# asm 2: shufb >et0aat11=$103,<et4t1t01=$79,<aacbbbda1=$65,<selw0342=$30
shufb $103,$79,$65,$30

# qhasm: int32323232 et4t1t04  += carry0
# asm 1: a >et4t1t04=vec128#80,<et4t1t04=vec128#80,<carry0=vec128#96
# asm 2: a >et4t1t04=$82,<et4t1t04=$82,<carry0=$98
a $82,$82,$98

# qhasm: et0aat12 = combine et4t1t02 and aacbbbda2 by selw0342
# asm 1: shufb >et0aat12=vec128#96,<et4t1t02=vec128#78,<aacbbbda2=vec128#5,<selw0342=vec128#28
# asm 2: shufb >et0aat12=$98,<et4t1t02=$80,<aacbbbda2=$7,<selw0342=$30
shufb $98,$80,$7,$30

# qhasm: int32323232 et4t1t08  += carry1
# asm 1: a >et4t1t08=vec128#84,<et4t1t08=vec128#84,<carry1=vec128#97
# asm 2: a >et4t1t08=$86,<et4t1t08=$86,<carry1=$99
a $86,$86,$99

# qhasm: et0aat13 = combine et4t1t03 and aacbbbda3 by selw0342
# asm 1: shufb >et0aat13=vec128#97,<et4t1t03=vec128#79,<aacbbbda3=vec128#66,<selw0342=vec128#28
# asm 2: shufb >et0aat13=$99,<et4t1t03=$81,<aacbbbda3=$68,<selw0342=$30
shufb $99,$81,$68,$30

# qhasm: int32323232 et4t1t012 += carry2
# asm 1: a >et4t1t012=vec128#88,<et4t1t012=vec128#88,<carry2=vec128#98
# asm 2: a >et4t1t012=$90,<et4t1t012=$90,<carry2=$100
a $90,$90,$100

# qhasm: t4t0bbt10 = combine et4t1t00 and aacbbbda0 by selw1362
# asm 1: shufb >t4t0bbt10=vec128#58,<et4t1t00=vec128#76,<aacbbbda0=vec128#58,<selw1362=vec128#29
# asm 2: shufb >t4t0bbt10=$60,<et4t1t00=$78,<aacbbbda0=$60,<selw1362=$31
shufb $60,$78,$60,$31

# qhasm: int32323232 et4t1t016 += carry3
# asm 1: a >et4t1t016=vec128#76,<et4t1t016=vec128#92,<carry3=vec128#99
# asm 2: a >et4t1t016=$78,<et4t1t016=$94,<carry3=$101
a $78,$94,$101

# qhasm: t4t0bbt11 = combine et4t1t01 and aacbbbda1 by selw1362
# asm 1: shufb >t4t0bbt11=vec128#63,<et4t1t01=vec128#77,<aacbbbda1=vec128#63,<selw1362=vec128#29
# asm 2: shufb >t4t0bbt11=$65,<et4t1t01=$79,<aacbbbda1=$65,<selw1362=$31
shufb $65,$79,$65,$31

# qhasm: uint32323232 carry1 = et4t1t04  >> 13
# asm 1: rotmi >carry1=vec128#77,<et4t1t04=vec128#80,-13
# asm 2: rotmi >carry1=$79,<et4t1t04=$82,-13
rotmi $79,$82,-13

# qhasm: t4t0bbt12 = combine et4t1t02 and aacbbbda2 by selw1362
# asm 1: shufb >t4t0bbt12=vec128#5,<et4t1t02=vec128#78,<aacbbbda2=vec128#5,<selw1362=vec128#29
# asm 2: shufb >t4t0bbt12=$7,<et4t1t02=$80,<aacbbbda2=$7,<selw1362=$31
shufb $7,$80,$7,$31

# qhasm: uint32323232 carry2 = et4t1t08  >> 13
# asm 1: rotmi >carry2=vec128#78,<et4t1t08=vec128#84,-13
# asm 2: rotmi >carry2=$80,<et4t1t08=$86,-13
rotmi $80,$86,-13

# qhasm: t4t0bbt13 = combine et4t1t03 and aacbbbda3 by selw1362
# asm 1: shufb >t4t0bbt13=vec128#66,<et4t1t03=vec128#79,<aacbbbda3=vec128#66,<selw1362=vec128#29
# asm 2: shufb >t4t0bbt13=$68,<et4t1t03=$81,<aacbbbda3=$68,<selw1362=$31
shufb $68,$81,$68,$31

# qhasm: uint32323232 carry3 = et4t1t012 >> 13
# asm 1: rotmi >carry3=vec128#79,<et4t1t012=vec128#88,-13
# asm 2: rotmi >carry3=$81,<et4t1t012=$90,-13
rotmi $81,$90,-13

# qhasm: uint32323232 carry4 = et4t1t016 >> 13
# asm 1: rotmi >carry4=vec128#92,<et4t1t016=vec128#76,-13
# asm 2: rotmi >carry4=$94,<et4t1t016=$78,-13
rotmi $94,$78,-13

# qhasm: et4t1t04  &= mask13
# asm 1: and >et4t1t04=vec128#80,<et4t1t04=vec128#80,<mask13=vec128#22
# asm 2: and >et4t1t04=$82,<et4t1t04=$82,<mask13=$24
and $82,$82,$24

# qhasm: et4t1t08  &= mask13
# asm 1: and >et4t1t08=vec128#84,<et4t1t08=vec128#84,<mask13=vec128#22
# asm 2: and >et4t1t08=$86,<et4t1t08=$86,<mask13=$24
and $86,$86,$24

# qhasm: et4t1t012 &= mask13
# asm 1: and >et4t1t012=vec128#88,<et4t1t012=vec128#88,<mask13=vec128#22
# asm 2: and >et4t1t012=$90,<et4t1t012=$90,<mask13=$24
and $90,$90,$24

# qhasm: et0aat14 = combine et4t1t04 and aacbbbda4 by selw0342
# asm 1: shufb >et0aat14=vec128#98,<et4t1t04=vec128#80,<aacbbbda4=vec128#61,<selw0342=vec128#28
# asm 2: shufb >et0aat14=$100,<et4t1t04=$82,<aacbbbda4=$63,<selw0342=$30
shufb $100,$82,$63,$30

# qhasm: et4t1t016 &= mask13
# asm 1: and >et4t1t016=vec128#76,<et4t1t016=vec128#76,<mask13=vec128#22
# asm 2: and >et4t1t016=$78,<et4t1t016=$78,<mask13=$24
and $78,$78,$24

# qhasm: et0aat18 = combine et4t1t08 and aacbbbda8 by selw0342
# asm 1: shufb >et0aat18=vec128#99,<et4t1t08=vec128#84,<aacbbbda8=vec128#62,<selw0342=vec128#28
# asm 2: shufb >et0aat18=$101,<et4t1t08=$86,<aacbbbda8=$64,<selw0342=$30
shufb $101,$86,$64,$30

# qhasm: int32323232 et4t1t05  += carry1
# asm 1: a >et4t1t05=vec128#77,<et4t1t05=vec128#81,<carry1=vec128#77
# asm 2: a >et4t1t05=$79,<et4t1t05=$83,<carry1=$79
a $79,$83,$79

# qhasm: et0aat112 = combine et4t1t012 and aacbbbda12 by selw0342
# asm 1: shufb >et0aat112=vec128#81,<et4t1t012=vec128#88,<aacbbbda12=vec128#71,<selw0342=vec128#28
# asm 2: shufb >et0aat112=$83,<et4t1t012=$90,<aacbbbda12=$73,<selw0342=$30
shufb $83,$90,$73,$30

# qhasm: int32323232 et4t1t09  += carry2
# asm 1: a >et4t1t09=vec128#78,<et4t1t09=vec128#85,<carry2=vec128#78
# asm 2: a >et4t1t09=$80,<et4t1t09=$87,<carry2=$80
a $80,$87,$80

# qhasm: et0aat116 = combine et4t1t016 and aacbbbda16 by selw0342
# asm 1: shufb >et0aat116=vec128#85,<et4t1t016=vec128#76,<aacbbbda16=vec128#69,<selw0342=vec128#28
# asm 2: shufb >et0aat116=$87,<et4t1t016=$78,<aacbbbda16=$71,<selw0342=$30
shufb $87,$78,$71,$30

# qhasm: int32323232 et4t1t013 += carry3
# asm 1: a >et4t1t013=vec128#79,<et4t1t013=vec128#89,<carry3=vec128#79
# asm 2: a >et4t1t013=$81,<et4t1t013=$91,<carry3=$81
a $81,$91,$81

# qhasm: t4t0bbt14 = combine et4t1t04 and aacbbbda4 by selw1362
# asm 1: shufb >t4t0bbt14=vec128#61,<et4t1t04=vec128#80,<aacbbbda4=vec128#61,<selw1362=vec128#29
# asm 2: shufb >t4t0bbt14=$63,<et4t1t04=$82,<aacbbbda4=$63,<selw1362=$31
shufb $63,$82,$63,$31

# qhasm: int32323232 et4t1t017  += carry4
# asm 1: a >et4t1t017=vec128#80,<et4t1t017=vec128#93,<carry4=vec128#92
# asm 2: a >et4t1t017=$82,<et4t1t017=$95,<carry4=$94
a $82,$95,$94

# qhasm: t4t0bbt18 = combine et4t1t08 and aacbbbda8 by selw1362
# asm 1: shufb >t4t0bbt18=vec128#62,<et4t1t08=vec128#84,<aacbbbda8=vec128#62,<selw1362=vec128#29
# asm 2: shufb >t4t0bbt18=$64,<et4t1t08=$86,<aacbbbda8=$64,<selw1362=$31
shufb $64,$86,$64,$31

# qhasm: uint32323232 carry1 = et4t1t05  >> 13
# asm 1: rotmi >carry1=vec128#84,<et4t1t05=vec128#77,-13
# asm 2: rotmi >carry1=$86,<et4t1t05=$79,-13
rotmi $86,$79,-13

# qhasm: t4t0bbt112 = combine et4t1t012 and aacbbbda12 by selw1362
# asm 1: shufb >t4t0bbt112=vec128#71,<et4t1t012=vec128#88,<aacbbbda12=vec128#71,<selw1362=vec128#29
# asm 2: shufb >t4t0bbt112=$73,<et4t1t012=$90,<aacbbbda12=$73,<selw1362=$31
shufb $73,$90,$73,$31

# qhasm: uint32323232 carry2 = et4t1t09  >> 13
# asm 1: rotmi >carry2=vec128#88,<et4t1t09=vec128#78,-13
# asm 2: rotmi >carry2=$90,<et4t1t09=$80,-13
rotmi $90,$80,-13

# qhasm: t4t0bbt116 = combine et4t1t016 and aacbbbda16 by selw1362
# asm 1: shufb >t4t0bbt116=vec128#69,<et4t1t016=vec128#76,<aacbbbda16=vec128#69,<selw1362=vec128#29
# asm 2: shufb >t4t0bbt116=$71,<et4t1t016=$78,<aacbbbda16=$71,<selw1362=$31
shufb $71,$78,$71,$31

# qhasm: uint32323232 carry3 = et4t1t013 >> 13
# asm 1: rotmi >carry3=vec128#76,<et4t1t013=vec128#79,-13
# asm 2: rotmi >carry3=$78,<et4t1t013=$81,-13
rotmi $78,$81,-13

# qhasm: uint32323232 carry4 = et4t1t017 >> 13
# asm 1: rotmi >carry4=vec128#89,<et4t1t017=vec128#80,-13
# asm 2: rotmi >carry4=$91,<et4t1t017=$82,-13
rotmi $91,$82,-13

# qhasm: et4t1t05  &= mask13
# asm 1: and >et4t1t05=vec128#77,<et4t1t05=vec128#77,<mask13=vec128#22
# asm 2: and >et4t1t05=$79,<et4t1t05=$79,<mask13=$24
and $79,$79,$24

# qhasm: et4t1t09  &= mask13
# asm 1: and >et4t1t09=vec128#78,<et4t1t09=vec128#78,<mask13=vec128#22
# asm 2: and >et4t1t09=$80,<et4t1t09=$80,<mask13=$24
and $80,$80,$24

# qhasm: et4t1t013 &= mask13
# asm 1: and >et4t1t013=vec128#79,<et4t1t013=vec128#79,<mask13=vec128#22
# asm 2: and >et4t1t013=$81,<et4t1t013=$81,<mask13=$24
and $81,$81,$24

# qhasm: et0aat15 = combine et4t1t05 and aacbbbda5 by selw0342
# asm 1: shufb >et0aat15=vec128#92,<et4t1t05=vec128#77,<aacbbbda5=vec128#65,<selw0342=vec128#28
# asm 2: shufb >et0aat15=$94,<et4t1t05=$79,<aacbbbda5=$67,<selw0342=$30
shufb $94,$79,$67,$30

# qhasm: et4t1t017 &= mask13
# asm 1: and >et4t1t017=vec128#80,<et4t1t017=vec128#80,<mask13=vec128#22
# asm 2: and >et4t1t017=$82,<et4t1t017=$82,<mask13=$24
and $82,$82,$24

# qhasm: et0aat19 = combine et4t1t09 and aacbbbda9 by selw0342
# asm 1: shufb >et0aat19=vec128#93,<et4t1t09=vec128#78,<aacbbbda9=vec128#64,<selw0342=vec128#28
# asm 2: shufb >et0aat19=$95,<et4t1t09=$80,<aacbbbda9=$66,<selw0342=$30
shufb $95,$80,$66,$30

# qhasm: int32323232 et4t1t06  += carry1
# asm 1: a >et4t1t06=vec128#82,<et4t1t06=vec128#82,<carry1=vec128#84
# asm 2: a >et4t1t06=$84,<et4t1t06=$84,<carry1=$86
a $84,$84,$86

# qhasm: et0aat113 = combine et4t1t013 and aacbbbda13 by selw0342
# asm 1: shufb >et0aat113=vec128#84,<et4t1t013=vec128#79,<aacbbbda13=vec128#60,<selw0342=vec128#28
# asm 2: shufb >et0aat113=$86,<et4t1t013=$81,<aacbbbda13=$62,<selw0342=$30
shufb $86,$81,$62,$30

# qhasm: int32323232 et4t1t010 += carry2
# asm 1: a >et4t1t010=vec128#86,<et4t1t010=vec128#86,<carry2=vec128#88
# asm 2: a >et4t1t010=$88,<et4t1t010=$88,<carry2=$90
a $88,$88,$90

# qhasm: et0aat117 = combine et4t1t017 and aacbbbda17 by selw0342
# asm 1: shufb >et0aat117=vec128#88,<et4t1t017=vec128#80,<aacbbbda17=vec128#70,<selw0342=vec128#28
# asm 2: shufb >et0aat117=$90,<et4t1t017=$82,<aacbbbda17=$72,<selw0342=$30
shufb $90,$82,$72,$30

# qhasm: int32323232 et4t1t014 += carry3
# asm 1: a >et4t1t014=vec128#76,<et4t1t014=vec128#90,<carry3=vec128#76
# asm 2: a >et4t1t014=$78,<et4t1t014=$92,<carry3=$78
a $78,$92,$78

# qhasm: t4t0bbt15 = combine et4t1t05 and aacbbbda5 by selw1362
# asm 1: shufb >t4t0bbt15=vec128#65,<et4t1t05=vec128#77,<aacbbbda5=vec128#65,<selw1362=vec128#29
# asm 2: shufb >t4t0bbt15=$67,<et4t1t05=$79,<aacbbbda5=$67,<selw1362=$31
shufb $67,$79,$67,$31

# qhasm: int32323232 et4t1t018 += carry4
# asm 1: a >et4t1t018=vec128#77,<et4t1t018=vec128#94,<carry4=vec128#89
# asm 2: a >et4t1t018=$79,<et4t1t018=$96,<carry4=$91
a $79,$96,$91

# qhasm: t4t0bbt19 = combine et4t1t09 and aacbbbda9 by selw1362
# asm 1: shufb >t4t0bbt19=vec128#64,<et4t1t09=vec128#78,<aacbbbda9=vec128#64,<selw1362=vec128#29
# asm 2: shufb >t4t0bbt19=$66,<et4t1t09=$80,<aacbbbda9=$66,<selw1362=$31
shufb $66,$80,$66,$31

# qhasm: uint32323232 carry1 = et4t1t06  >> 13
# asm 1: rotmi >carry1=vec128#78,<et4t1t06=vec128#82,-13
# asm 2: rotmi >carry1=$80,<et4t1t06=$84,-13
rotmi $80,$84,-13

# qhasm: t4t0bbt113 = combine et4t1t013 and aacbbbda13 by selw1362
# asm 1: shufb >t4t0bbt113=vec128#60,<et4t1t013=vec128#79,<aacbbbda13=vec128#60,<selw1362=vec128#29
# asm 2: shufb >t4t0bbt113=$62,<et4t1t013=$81,<aacbbbda13=$62,<selw1362=$31
shufb $62,$81,$62,$31

# qhasm: uint32323232 carry2 = et4t1t010 >> 13
# asm 1: rotmi >carry2=vec128#79,<et4t1t010=vec128#86,-13
# asm 2: rotmi >carry2=$81,<et4t1t010=$88,-13
rotmi $81,$88,-13

# qhasm: t4t0bbt117 = combine et4t1t017 and aacbbbda17 by selw1362
# asm 1: shufb >t4t0bbt117=vec128#70,<et4t1t017=vec128#80,<aacbbbda17=vec128#70,<selw1362=vec128#29
# asm 2: shufb >t4t0bbt117=$72,<et4t1t017=$82,<aacbbbda17=$72,<selw1362=$31
shufb $72,$82,$72,$31

# qhasm: uint32323232 carry3 = et4t1t014 >> 13
# asm 1: rotmi >carry3=vec128#80,<et4t1t014=vec128#76,-13
# asm 2: rotmi >carry3=$82,<et4t1t014=$78,-13
rotmi $82,$78,-13

# qhasm: uint32323232 carry4 = et4t1t018  >> 13
# asm 1: rotmi >carry4=vec128#89,<et4t1t018=vec128#77,-13
# asm 2: rotmi >carry4=$91,<et4t1t018=$79,-13
rotmi $91,$79,-13

# qhasm: et4t1t06  &= mask13
# asm 1: and >et4t1t06=vec128#82,<et4t1t06=vec128#82,<mask13=vec128#22
# asm 2: and >et4t1t06=$84,<et4t1t06=$84,<mask13=$24
and $84,$84,$24

# qhasm: et4t1t010 &= mask13
# asm 1: and >et4t1t010=vec128#86,<et4t1t010=vec128#86,<mask13=vec128#22
# asm 2: and >et4t1t010=$88,<et4t1t010=$88,<mask13=$24
and $88,$88,$24

# qhasm: et4t1t014 &= mask13
# asm 1: and >et4t1t014=vec128#76,<et4t1t014=vec128#76,<mask13=vec128#22
# asm 2: and >et4t1t014=$78,<et4t1t014=$78,<mask13=$24
and $78,$78,$24

# qhasm: et0aat16 = combine et4t1t06 and aacbbbda6 by selw0342
# asm 1: shufb >et0aat16=vec128#90,<et4t1t06=vec128#82,<aacbbbda6=vec128#55,<selw0342=vec128#28
# asm 2: shufb >et0aat16=$92,<et4t1t06=$84,<aacbbbda6=$57,<selw0342=$30
shufb $92,$84,$57,$30

# qhasm: et4t1t018 &= mask13
# asm 1: and >et4t1t018=vec128#77,<et4t1t018=vec128#77,<mask13=vec128#22
# asm 2: and >et4t1t018=$79,<et4t1t018=$79,<mask13=$24
and $79,$79,$24

# qhasm: et0aat110 = combine et4t1t010 and aacbbbda10 by selw0342
# asm 1: shufb >et0aat110=vec128#94,<et4t1t010=vec128#86,<aacbbbda10=vec128#56,<selw0342=vec128#28
# asm 2: shufb >et0aat110=$96,<et4t1t010=$88,<aacbbbda10=$58,<selw0342=$30
shufb $96,$88,$58,$30

# qhasm: int32323232 et4t1t07  += carry1
# asm 1: a >et4t1t07=vec128#78,<et4t1t07=vec128#83,<carry1=vec128#78
# asm 2: a >et4t1t07=$80,<et4t1t07=$85,<carry1=$80
a $80,$85,$80

# qhasm: et0aat114 = combine et4t1t014 and aacbbbda14 by selw0342
# asm 1: shufb >et0aat114=vec128#83,<et4t1t014=vec128#76,<aacbbbda14=vec128#57,<selw0342=vec128#28
# asm 2: shufb >et0aat114=$85,<et4t1t014=$78,<aacbbbda14=$59,<selw0342=$30
shufb $85,$78,$59,$30

# qhasm: int32323232 et4t1t011 += carry2
# asm 1: a >et4t1t011=vec128#79,<et4t1t011=vec128#87,<carry2=vec128#79
# asm 2: a >et4t1t011=$81,<et4t1t011=$89,<carry2=$81
a $81,$89,$81

# qhasm: et0aat118 = combine et4t1t018 and aacbbbda18 by selw0342
# asm 1: shufb >et0aat118=vec128#87,<et4t1t018=vec128#77,<aacbbbda18=vec128#59,<selw0342=vec128#28
# asm 2: shufb >et0aat118=$89,<et4t1t018=$79,<aacbbbda18=$61,<selw0342=$30
shufb $89,$79,$61,$30

# qhasm: int32323232 et4t1t015 += carry3
# asm 1: a >et4t1t015=vec128#80,<et4t1t015=vec128#91,<carry3=vec128#80
# asm 2: a >et4t1t015=$82,<et4t1t015=$93,<carry3=$82
a $82,$93,$82

# qhasm: t4t0bbt16 = combine et4t1t06 and aacbbbda6 by selw1362
# asm 1: shufb >t4t0bbt16=vec128#55,<et4t1t06=vec128#82,<aacbbbda6=vec128#55,<selw1362=vec128#29
# asm 2: shufb >t4t0bbt16=$57,<et4t1t06=$84,<aacbbbda6=$57,<selw1362=$31
shufb $57,$84,$57,$31

# qhasm: int32323232 et4t1t019  += carry4
# asm 1: a >et4t1t019=vec128#82,<et4t1t019=vec128#95,<carry4=vec128#89
# asm 2: a >et4t1t019=$84,<et4t1t019=$97,<carry4=$91
a $84,$97,$91

# qhasm: t4t0bbt110 = combine et4t1t010 and aacbbbda10 by selw1362
# asm 1: shufb >t4t0bbt110=vec128#56,<et4t1t010=vec128#86,<aacbbbda10=vec128#56,<selw1362=vec128#29
# asm 2: shufb >t4t0bbt110=$58,<et4t1t010=$88,<aacbbbda10=$58,<selw1362=$31
shufb $58,$88,$58,$31

# qhasm: int32323232 z4x5x4t20  = (t4t0bbt10 & 0xffff) * (et0aat10 & 0xffff)
# asm 1: mpy >z4x5x4t20=vec128#86,<t4t0bbt10=vec128#58,<et0aat10=vec128#100
# asm 2: mpy >z4x5x4t20=$88,<t4t0bbt10=$60,<et0aat10=$102
mpy $88,$60,$102

# qhasm: et0aat17 = combine et4t1t07 and aacbbbda7 by selw0342
# asm 1: shufb >et0aat17=vec128#89,<et4t1t07=vec128#78,<aacbbbda7=vec128#67,<selw0342=vec128#28
# asm 2: shufb >et0aat17=$91,<et4t1t07=$80,<aacbbbda7=$69,<selw0342=$30
shufb $91,$80,$69,$30

# qhasm: int32323232 z4x5x4t21  = (t4t0bbt10 & 0xffff) * (et0aat11 & 0xffff)
# asm 1: mpy >z4x5x4t21=vec128#91,<t4t0bbt10=vec128#58,<et0aat11=vec128#101
# asm 2: mpy >z4x5x4t21=$93,<t4t0bbt10=$60,<et0aat11=$103
mpy $93,$60,$103

# qhasm: t4t0bbt17 = combine et4t1t07 and aacbbbda7 by selw1362
# asm 1: shufb >t4t0bbt17=vec128#67,<et4t1t07=vec128#78,<aacbbbda7=vec128#67,<selw1362=vec128#29
# asm 2: shufb >t4t0bbt17=$69,<et4t1t07=$80,<aacbbbda7=$69,<selw1362=$31
shufb $69,$80,$69,$31

# qhasm: int32323232 z4x5x4t22  = (t4t0bbt10 & 0xffff) * (et0aat12 & 0xffff)
# asm 1: mpy >z4x5x4t22=vec128#78,<t4t0bbt10=vec128#58,<et0aat12=vec128#96
# asm 2: mpy >z4x5x4t22=$80,<t4t0bbt10=$60,<et0aat12=$98
mpy $80,$60,$98

# qhasm: et0aat111 = combine et4t1t011 and aacbbbda11 by selw0342
# asm 1: shufb >et0aat111=vec128#95,<et4t1t011=vec128#79,<aacbbbda11=vec128#68,<selw0342=vec128#28
# asm 2: shufb >et0aat111=$97,<et4t1t011=$81,<aacbbbda11=$70,<selw0342=$30
shufb $97,$81,$70,$30

# qhasm: int32323232 z4x5x4t23  = (t4t0bbt10 & 0xffff) * (et0aat13 & 0xffff)
# asm 1: mpy >z4x5x4t23=vec128#102,<t4t0bbt10=vec128#58,<et0aat13=vec128#97
# asm 2: mpy >z4x5x4t23=$104,<t4t0bbt10=$60,<et0aat13=$99
mpy $104,$60,$99

# qhasm: t4t0bbt111 = combine et4t1t011 and aacbbbda11 by selw1362
# asm 1: shufb >t4t0bbt111=vec128#68,<et4t1t011=vec128#79,<aacbbbda11=vec128#68,<selw1362=vec128#29
# asm 2: shufb >t4t0bbt111=$70,<et4t1t011=$81,<aacbbbda11=$70,<selw1362=$31
shufb $70,$81,$70,$31

# qhasm: int32323232 z4x5x4t24  = (t4t0bbt11 & 0xffff) * (et0aat13 & 0xffff)
# asm 1: mpy >z4x5x4t24=vec128#79,<t4t0bbt11=vec128#63,<et0aat13=vec128#97
# asm 2: mpy >z4x5x4t24=$81,<t4t0bbt11=$65,<et0aat13=$99
mpy $81,$65,$99

# qhasm: t4t0bbt114 = combine et4t1t014 and aacbbbda14 by selw1362
# asm 1: shufb >t4t0bbt114=vec128#57,<et4t1t014=vec128#76,<aacbbbda14=vec128#57,<selw1362=vec128#29
# asm 2: shufb >t4t0bbt114=$59,<et4t1t014=$78,<aacbbbda14=$59,<selw1362=$31
shufb $59,$78,$59,$31

# qhasm: int32323232 z4x5x4t25  = (t4t0bbt12 & 0xffff) * (et0aat13 & 0xffff)
# asm 1: mpy >z4x5x4t25=vec128#76,<t4t0bbt12=vec128#5,<et0aat13=vec128#97
# asm 2: mpy >z4x5x4t25=$78,<t4t0bbt12=$7,<et0aat13=$99
mpy $78,$7,$99

# qhasm: et0aat115 = combine et4t1t015 and aacbbbda15 by selw0342
# asm 1: shufb >et0aat115=vec128#103,<et4t1t015=vec128#80,<aacbbbda15=vec128#72,<selw0342=vec128#28
# asm 2: shufb >et0aat115=$105,<et4t1t015=$82,<aacbbbda15=$74,<selw0342=$30
shufb $105,$82,$74,$30

# qhasm: int32323232 z4x5x4t26  = (t4t0bbt13 & 0xffff) * (et0aat13 & 0xffff)
# asm 1: mpy >z4x5x4t26=vec128#104,<t4t0bbt13=vec128#66,<et0aat13=vec128#97
# asm 2: mpy >z4x5x4t26=$106,<t4t0bbt13=$68,<et0aat13=$99
mpy $106,$68,$99

# qhasm: t4t0bbt115 = combine et4t1t015 and aacbbbda15 by selw1362
# asm 1: shufb >t4t0bbt115=vec128#72,<et4t1t015=vec128#80,<aacbbbda15=vec128#72,<selw1362=vec128#29
# asm 2: shufb >t4t0bbt115=$74,<et4t1t015=$82,<aacbbbda15=$74,<selw1362=$31
shufb $74,$82,$74,$31

# qhasm: int32323232 z4x5x4t27  = (t4t0bbt10 & 0xffff) * (et0aat17 & 0xffff)
# asm 1: mpy >z4x5x4t27=vec128#80,<t4t0bbt10=vec128#58,<et0aat17=vec128#89
# asm 2: mpy >z4x5x4t27=$82,<t4t0bbt10=$60,<et0aat17=$91
mpy $82,$60,$91

# qhasm: t4t0bbt118 = combine et4t1t018 and aacbbbda18 by selw1362
# asm 1: shufb >t4t0bbt118=vec128#59,<et4t1t018=vec128#77,<aacbbbda18=vec128#59,<selw1362=vec128#29
# asm 2: shufb >t4t0bbt118=$61,<et4t1t018=$79,<aacbbbda18=$61,<selw1362=$31
shufb $61,$79,$61,$31

# qhasm: int32323232 z4x5x4t21 += (t4t0bbt11 & 0xffff) * (et0aat10 & 0xffff)
# asm 1: mpya >z4x5x4t21=vec128#77,<t4t0bbt11=vec128#63,<et0aat10=vec128#100,<z4x5x4t21=vec128#91
# asm 2: mpya >z4x5x4t21=$79,<t4t0bbt11=$65,<et0aat10=$102,<z4x5x4t21=$93
mpya $79,$65,$102,$93

# qhasm: et0aat119 = combine et4t1t019 and aacbbbda19 by selw0342
# asm 1: shufb >et0aat119=vec128#91,<et4t1t019=vec128#82,<aacbbbda19=vec128#73,<selw0342=vec128#28
# asm 2: shufb >et0aat119=$93,<et4t1t019=$84,<aacbbbda19=$75,<selw0342=$30
shufb $93,$84,$75,$30

# qhasm: int32323232 z4x5x4t22 += (t4t0bbt11 & 0xffff) * (et0aat11 & 0xffff)
# asm 1: mpya >z4x5x4t22=vec128#78,<t4t0bbt11=vec128#63,<et0aat11=vec128#101,<z4x5x4t22=vec128#78
# asm 2: mpya >z4x5x4t22=$80,<t4t0bbt11=$65,<et0aat11=$103,<z4x5x4t22=$80
mpya $80,$65,$103,$80

# qhasm: t4t0bbt119 = combine et4t1t019 and aacbbbda19 by selw1362
# asm 1: shufb >t4t0bbt119=vec128#73,<et4t1t019=vec128#82,<aacbbbda19=vec128#73,<selw1362=vec128#29
# asm 2: shufb >t4t0bbt119=$75,<et4t1t019=$84,<aacbbbda19=$75,<selw1362=$31
shufb $75,$84,$75,$31

# qhasm: int32323232 z4x5x4t23 += (t4t0bbt11 & 0xffff) * (et0aat12 & 0xffff)
# asm 1: mpya >z4x5x4t23=vec128#82,<t4t0bbt11=vec128#63,<et0aat12=vec128#96,<z4x5x4t23=vec128#102
# asm 2: mpya >z4x5x4t23=$84,<t4t0bbt11=$65,<et0aat12=$98,<z4x5x4t23=$104
mpya $84,$65,$98,$104

# qhasm: int32323232 z4x5x4t24 += (t4t0bbt12 & 0xffff) * (et0aat12 & 0xffff)
# asm 1: mpya >z4x5x4t24=vec128#79,<t4t0bbt12=vec128#5,<et0aat12=vec128#96,<z4x5x4t24=vec128#79
# asm 2: mpya >z4x5x4t24=$81,<t4t0bbt12=$7,<et0aat12=$98,<z4x5x4t24=$81
mpya $81,$7,$98,$81

# qhasm: int32323232 z4x5x4t25 += (t4t0bbt13 & 0xffff) * (et0aat12 & 0xffff)
# asm 1: mpya >z4x5x4t25=vec128#76,<t4t0bbt13=vec128#66,<et0aat12=vec128#96,<z4x5x4t25=vec128#76
# asm 2: mpya >z4x5x4t25=$78,<t4t0bbt13=$68,<et0aat12=$98,<z4x5x4t25=$78
mpya $78,$68,$98,$78

# qhasm: int32323232 z4x5x4t26 <<= 1
# asm 1: shli >z4x5x4t26=vec128#102,<z4x5x4t26=vec128#104,1
# asm 2: shli >z4x5x4t26=$104,<z4x5x4t26=$106,1
shli $104,$106,1

# qhasm: int32323232 z4x5x4t27 += (t4t0bbt11 & 0xffff) * (et0aat16 & 0xffff)
# asm 1: mpya >z4x5x4t27=vec128#80,<t4t0bbt11=vec128#63,<et0aat16=vec128#90,<z4x5x4t27=vec128#80
# asm 2: mpya >z4x5x4t27=$82,<t4t0bbt11=$65,<et0aat16=$92,<z4x5x4t27=$82
mpya $82,$65,$92,$82

# qhasm: int32323232 z4x5x4t28  = (t4t0bbt11 & 0xffff) * (et0aat17 & 0xffff)
# asm 1: mpy >z4x5x4t28=vec128#104,<t4t0bbt11=vec128#63,<et0aat17=vec128#89
# asm 2: mpy >z4x5x4t28=$106,<t4t0bbt11=$65,<et0aat17=$91
mpy $106,$65,$91

# qhasm: int32323232 z4x5x4t22 += (t4t0bbt12 & 0xffff) * (et0aat10 & 0xffff)
# asm 1: mpya >z4x5x4t22=vec128#78,<t4t0bbt12=vec128#5,<et0aat10=vec128#100,<z4x5x4t22=vec128#78
# asm 2: mpya >z4x5x4t22=$80,<t4t0bbt12=$7,<et0aat10=$102,<z4x5x4t22=$80
mpya $80,$7,$102,$80

# qhasm: int32323232 z4x5x4t23 += (t4t0bbt12 & 0xffff) * (et0aat11 & 0xffff)
# asm 1: mpya >z4x5x4t23=vec128#82,<t4t0bbt12=vec128#5,<et0aat11=vec128#101,<z4x5x4t23=vec128#82
# asm 2: mpya >z4x5x4t23=$84,<t4t0bbt12=$7,<et0aat11=$103,<z4x5x4t23=$84
mpya $84,$7,$103,$84

# qhasm: int32323232 z4x5x4t24 += (t4t0bbt13 & 0xffff) * (et0aat11 & 0xffff)
# asm 1: mpya >z4x5x4t24=vec128#79,<t4t0bbt13=vec128#66,<et0aat11=vec128#101,<z4x5x4t24=vec128#79
# asm 2: mpya >z4x5x4t24=$81,<t4t0bbt13=$68,<et0aat11=$103,<z4x5x4t24=$81
mpya $81,$68,$103,$81

# qhasm: int32323232 z4x5x4t25 <<= 1
# asm 1: shli >z4x5x4t25=vec128#76,<z4x5x4t25=vec128#76,1
# asm 2: shli >z4x5x4t25=$78,<z4x5x4t25=$78,1
shli $78,$78,1

# qhasm: int32323232 z4x5x4t26 += (t4t0bbt10 & 0xffff) * (et0aat16 & 0xffff)
# asm 1: mpya >z4x5x4t26=vec128#102,<t4t0bbt10=vec128#58,<et0aat16=vec128#90,<z4x5x4t26=vec128#102
# asm 2: mpya >z4x5x4t26=$104,<t4t0bbt10=$60,<et0aat16=$92,<z4x5x4t26=$104
mpya $104,$60,$92,$104

# qhasm: int32323232 z4x5x4t27 += (t4t0bbt12 & 0xffff) * (et0aat15 & 0xffff)
# asm 1: mpya >z4x5x4t27=vec128#80,<t4t0bbt12=vec128#5,<et0aat15=vec128#92,<z4x5x4t27=vec128#80
# asm 2: mpya >z4x5x4t27=$82,<t4t0bbt12=$7,<et0aat15=$94,<z4x5x4t27=$82
mpya $82,$7,$94,$82

# qhasm: int32323232 z4x5x4t28 += (t4t0bbt12 & 0xffff) * (et0aat16 & 0xffff)
# asm 1: mpya >z4x5x4t28=vec128#104,<t4t0bbt12=vec128#5,<et0aat16=vec128#90,<z4x5x4t28=vec128#104
# asm 2: mpya >z4x5x4t28=$106,<t4t0bbt12=$7,<et0aat16=$92,<z4x5x4t28=$106
mpya $106,$7,$92,$106

# qhasm: int32323232 z4x5x4t29  = (t4t0bbt12 & 0xffff) * (et0aat17 & 0xffff)
# asm 1: mpy >z4x5x4t29=vec128#105,<t4t0bbt12=vec128#5,<et0aat17=vec128#89
# asm 2: mpy >z4x5x4t29=$107,<t4t0bbt12=$7,<et0aat17=$91
mpy $107,$7,$91

# qhasm: int32323232 z4x5x4t23 += (t4t0bbt13 & 0xffff) * (et0aat10 & 0xffff)
# asm 1: mpya >z4x5x4t23=vec128#82,<t4t0bbt13=vec128#66,<et0aat10=vec128#100,<z4x5x4t23=vec128#82
# asm 2: mpya >z4x5x4t23=$84,<t4t0bbt13=$68,<et0aat10=$102,<z4x5x4t23=$84
mpya $84,$68,$102,$84

# qhasm: int32323232 z4x5x4t24 <<= 1 
# asm 1: shli >z4x5x4t24=vec128#79,<z4x5x4t24=vec128#79,1
# asm 2: shli >z4x5x4t24=$81,<z4x5x4t24=$81,1
shli $81,$81,1

# qhasm: int32323232 z4x5x4t25 += (t4t0bbt10 & 0xffff) * (et0aat15 & 0xffff)
# asm 1: mpya >z4x5x4t25=vec128#76,<t4t0bbt10=vec128#58,<et0aat15=vec128#92,<z4x5x4t25=vec128#76
# asm 2: mpya >z4x5x4t25=$78,<t4t0bbt10=$60,<et0aat15=$94,<z4x5x4t25=$78
mpya $78,$60,$94,$78

# qhasm: int32323232 z4x5x4t26 += (t4t0bbt11 & 0xffff) * (et0aat15 & 0xffff)
# asm 1: mpya >z4x5x4t26=vec128#102,<t4t0bbt11=vec128#63,<et0aat15=vec128#92,<z4x5x4t26=vec128#102
# asm 2: mpya >z4x5x4t26=$104,<t4t0bbt11=$65,<et0aat15=$94,<z4x5x4t26=$104
mpya $104,$65,$94,$104

# qhasm: int32323232 z4x5x4t27 += (t4t0bbt13 & 0xffff) * (et0aat14 & 0xffff)
# asm 1: mpya >z4x5x4t27=vec128#80,<t4t0bbt13=vec128#66,<et0aat14=vec128#98,<z4x5x4t27=vec128#80
# asm 2: mpya >z4x5x4t27=$82,<t4t0bbt13=$68,<et0aat14=$100,<z4x5x4t27=$82
mpya $82,$68,$100,$82

# qhasm: int32323232 z4x5x4t28 += (t4t0bbt13 & 0xffff) * (et0aat15 & 0xffff)
# asm 1: mpya >z4x5x4t28=vec128#104,<t4t0bbt13=vec128#66,<et0aat15=vec128#92,<z4x5x4t28=vec128#104
# asm 2: mpya >z4x5x4t28=$106,<t4t0bbt13=$68,<et0aat15=$94,<z4x5x4t28=$106
mpya $106,$68,$94,$106

# qhasm: int32323232 z4x5x4t29 += (t4t0bbt13 & 0xffff) * (et0aat16 & 0xffff)
# asm 1: mpya >z4x5x4t29=vec128#105,<t4t0bbt13=vec128#66,<et0aat16=vec128#90,<z4x5x4t29=vec128#105
# asm 2: mpya >z4x5x4t29=$107,<t4t0bbt13=$68,<et0aat16=$92,<z4x5x4t29=$107
mpya $107,$68,$92,$107

# qhasm: int32323232 z4x5x4t210  = (t4t0bbt13  & 0xffff) * (et0aat17  & 0xffff)
# asm 1: mpy >z4x5x4t210=vec128#106,<t4t0bbt13=vec128#66,<et0aat17=vec128#89
# asm 2: mpy >z4x5x4t210=$108,<t4t0bbt13=$68,<et0aat17=$91
mpy $108,$68,$91

# qhasm: int32323232 z4x5x4t24 += (t4t0bbt10 & 0xffff) * (et0aat14 & 0xffff)
# asm 1: mpya >z4x5x4t24=vec128#79,<t4t0bbt10=vec128#58,<et0aat14=vec128#98,<z4x5x4t24=vec128#79
# asm 2: mpya >z4x5x4t24=$81,<t4t0bbt10=$60,<et0aat14=$100,<z4x5x4t24=$81
mpya $81,$60,$100,$81

# qhasm: int32323232 z4x5x4t25 += (t4t0bbt11 & 0xffff) * (et0aat14 & 0xffff)
# asm 1: mpya >z4x5x4t25=vec128#76,<t4t0bbt11=vec128#63,<et0aat14=vec128#98,<z4x5x4t25=vec128#76
# asm 2: mpya >z4x5x4t25=$78,<t4t0bbt11=$65,<et0aat14=$100,<z4x5x4t25=$78
mpya $78,$65,$100,$78

# qhasm: int32323232 z4x5x4t26 += (t4t0bbt12 & 0xffff) * (et0aat14 & 0xffff)
# asm 1: mpya >z4x5x4t26=vec128#102,<t4t0bbt12=vec128#5,<et0aat14=vec128#98,<z4x5x4t26=vec128#102
# asm 2: mpya >z4x5x4t26=$104,<t4t0bbt12=$7,<et0aat14=$100,<z4x5x4t26=$104
mpya $104,$7,$100,$104

# qhasm: int32323232 z4x5x4t27 += (t4t0bbt14 & 0xffff) * (et0aat13 & 0xffff)
# asm 1: mpya >z4x5x4t27=vec128#80,<t4t0bbt14=vec128#61,<et0aat13=vec128#97,<z4x5x4t27=vec128#80
# asm 2: mpya >z4x5x4t27=$82,<t4t0bbt14=$63,<et0aat13=$99,<z4x5x4t27=$82
mpya $82,$63,$99,$82

# qhasm: int32323232 z4x5x4t28 += (t4t0bbt15 & 0xffff) * (et0aat13 & 0xffff)
# asm 1: mpya >z4x5x4t28=vec128#104,<t4t0bbt15=vec128#65,<et0aat13=vec128#97,<z4x5x4t28=vec128#104
# asm 2: mpya >z4x5x4t28=$106,<t4t0bbt15=$67,<et0aat13=$99,<z4x5x4t28=$106
mpya $106,$67,$99,$106

# qhasm: int32323232 z4x5x4t29 += (t4t0bbt16 & 0xffff) * (et0aat13 & 0xffff)
# asm 1: mpya >z4x5x4t29=vec128#105,<t4t0bbt16=vec128#55,<et0aat13=vec128#97,<z4x5x4t29=vec128#105
# asm 2: mpya >z4x5x4t29=$107,<t4t0bbt16=$57,<et0aat13=$99,<z4x5x4t29=$107
mpya $107,$57,$99,$107

# qhasm: int32323232 z4x5x4t210 += (t4t0bbt17  & 0xffff) * (et0aat13  & 0xffff)
# asm 1: mpya >z4x5x4t210=vec128#106,<t4t0bbt17=vec128#67,<et0aat13=vec128#97,<z4x5x4t210=vec128#106
# asm 2: mpya >z4x5x4t210=$108,<t4t0bbt17=$69,<et0aat13=$99,<z4x5x4t210=$108
mpya $108,$69,$99,$108

# qhasm: int32323232 z4x5x4t211  = (t4t0bbt111 & 0xffff) * (et0aat10  & 0xffff)
# asm 1: mpy >z4x5x4t211=vec128#107,<t4t0bbt111=vec128#68,<et0aat10=vec128#100
# asm 2: mpy >z4x5x4t211=$109,<t4t0bbt111=$70,<et0aat10=$102
mpy $109,$70,$102

# qhasm: int32323232 z4x5x4t24 += (t4t0bbt14 & 0xffff) * (et0aat10 & 0xffff)
# asm 1: mpya >z4x5x4t24=vec128#79,<t4t0bbt14=vec128#61,<et0aat10=vec128#100,<z4x5x4t24=vec128#79
# asm 2: mpya >z4x5x4t24=$81,<t4t0bbt14=$63,<et0aat10=$102,<z4x5x4t24=$81
mpya $81,$63,$102,$81

# qhasm: int32323232 z4x5x4t25 += (t4t0bbt14 & 0xffff) * (et0aat11 & 0xffff)
# asm 1: mpya >z4x5x4t25=vec128#76,<t4t0bbt14=vec128#61,<et0aat11=vec128#101,<z4x5x4t25=vec128#76
# asm 2: mpya >z4x5x4t25=$78,<t4t0bbt14=$63,<et0aat11=$103,<z4x5x4t25=$78
mpya $78,$63,$103,$78

# qhasm: int32323232 z4x5x4t26 += (t4t0bbt14 & 0xffff) * (et0aat12 & 0xffff)
# asm 1: mpya >z4x5x4t26=vec128#102,<t4t0bbt14=vec128#61,<et0aat12=vec128#96,<z4x5x4t26=vec128#102
# asm 2: mpya >z4x5x4t26=$104,<t4t0bbt14=$63,<et0aat12=$98,<z4x5x4t26=$104
mpya $104,$63,$98,$104

# qhasm: int32323232 z4x5x4t27 += (t4t0bbt15 & 0xffff) * (et0aat12 & 0xffff)
# asm 1: mpya >z4x5x4t27=vec128#80,<t4t0bbt15=vec128#65,<et0aat12=vec128#96,<z4x5x4t27=vec128#80
# asm 2: mpya >z4x5x4t27=$82,<t4t0bbt15=$67,<et0aat12=$98,<z4x5x4t27=$82
mpya $82,$67,$98,$82

# qhasm: int32323232 z4x5x4t28 += (t4t0bbt16 & 0xffff) * (et0aat12 & 0xffff)
# asm 1: mpya >z4x5x4t28=vec128#104,<t4t0bbt16=vec128#55,<et0aat12=vec128#96,<z4x5x4t28=vec128#104
# asm 2: mpya >z4x5x4t28=$106,<t4t0bbt16=$57,<et0aat12=$98,<z4x5x4t28=$106
mpya $106,$57,$98,$106

# qhasm: int32323232 z4x5x4t29 += (t4t0bbt17 & 0xffff) * (et0aat12 & 0xffff)
# asm 1: mpya >z4x5x4t29=vec128#105,<t4t0bbt17=vec128#67,<et0aat12=vec128#96,<z4x5x4t29=vec128#105
# asm 2: mpya >z4x5x4t29=$107,<t4t0bbt17=$69,<et0aat12=$98,<z4x5x4t29=$107
mpya $107,$69,$98,$107

# qhasm: int32323232 z4x5x4t210 <<= 1
# asm 1: shli >z4x5x4t210=vec128#106,<z4x5x4t210=vec128#106,1
# asm 2: shli >z4x5x4t210=$108,<z4x5x4t210=$108,1
shli $108,$108,1

# qhasm: int32323232 z4x5x4t211 += (t4t0bbt110 & 0xffff) * (et0aat11  & 0xffff)
# asm 1: mpya >z4x5x4t211=vec128#107,<t4t0bbt110=vec128#56,<et0aat11=vec128#101,<z4x5x4t211=vec128#107
# asm 2: mpya >z4x5x4t211=$109,<t4t0bbt110=$58,<et0aat11=$103,<z4x5x4t211=$109
mpya $109,$58,$103,$109

# qhasm: int32323232 z4x5x4t212  = (t4t0bbt11  & 0xffff) * (et0aat111 & 0xffff)
# asm 1: mpy >z4x5x4t212=vec128#108,<t4t0bbt11=vec128#63,<et0aat111=vec128#95
# asm 2: mpy >z4x5x4t212=$110,<t4t0bbt11=$65,<et0aat111=$97
mpy $110,$65,$97

# qhasm: int32323232 z4x5x4t25 += (t4t0bbt15 & 0xffff) * (et0aat10 & 0xffff)
# asm 1: mpya >z4x5x4t25=vec128#76,<t4t0bbt15=vec128#65,<et0aat10=vec128#100,<z4x5x4t25=vec128#76
# asm 2: mpya >z4x5x4t25=$78,<t4t0bbt15=$67,<et0aat10=$102,<z4x5x4t25=$78
mpya $78,$67,$102,$78

# qhasm: int32323232 z4x5x4t26 += (t4t0bbt15 & 0xffff) * (et0aat11 & 0xffff)
# asm 1: mpya >z4x5x4t26=vec128#102,<t4t0bbt15=vec128#65,<et0aat11=vec128#101,<z4x5x4t26=vec128#102
# asm 2: mpya >z4x5x4t26=$104,<t4t0bbt15=$67,<et0aat11=$103,<z4x5x4t26=$104
mpya $104,$67,$103,$104

# qhasm: int32323232 z4x5x4t27 += (t4t0bbt16 & 0xffff) * (et0aat11 & 0xffff)
# asm 1: mpya >z4x5x4t27=vec128#80,<t4t0bbt16=vec128#55,<et0aat11=vec128#101,<z4x5x4t27=vec128#80
# asm 2: mpya >z4x5x4t27=$82,<t4t0bbt16=$57,<et0aat11=$103,<z4x5x4t27=$82
mpya $82,$57,$103,$82

# qhasm: int32323232 z4x5x4t28 += (t4t0bbt17 & 0xffff) * (et0aat11 & 0xffff)
# asm 1: mpya >z4x5x4t28=vec128#104,<t4t0bbt17=vec128#67,<et0aat11=vec128#101,<z4x5x4t28=vec128#104
# asm 2: mpya >z4x5x4t28=$106,<t4t0bbt17=$69,<et0aat11=$103,<z4x5x4t28=$106
mpya $106,$69,$103,$106

# qhasm: int32323232 z4x5x4t29 <<= 1
# asm 1: shli >z4x5x4t29=vec128#105,<z4x5x4t29=vec128#105,1
# asm 2: shli >z4x5x4t29=$107,<z4x5x4t29=$107,1
shli $107,$107,1

# qhasm: int32323232 z4x5x4t210 += (t4t0bbt10  & 0xffff) * (et0aat110 & 0xffff)
# asm 1: mpya >z4x5x4t210=vec128#106,<t4t0bbt10=vec128#58,<et0aat110=vec128#94,<z4x5x4t210=vec128#106
# asm 2: mpya >z4x5x4t210=$108,<t4t0bbt10=$60,<et0aat110=$96,<z4x5x4t210=$108
mpya $108,$60,$96,$108

# qhasm: int32323232 z4x5x4t211 += (t4t0bbt19  & 0xffff) * (et0aat12  & 0xffff)
# asm 1: mpya >z4x5x4t211=vec128#107,<t4t0bbt19=vec128#64,<et0aat12=vec128#96,<z4x5x4t211=vec128#107
# asm 2: mpya >z4x5x4t211=$109,<t4t0bbt19=$66,<et0aat12=$98,<z4x5x4t211=$109
mpya $109,$66,$98,$109

# qhasm: int32323232 z4x5x4t212 += (t4t0bbt12  & 0xffff) * (et0aat110 & 0xffff)
# asm 1: mpya >z4x5x4t212=vec128#108,<t4t0bbt12=vec128#5,<et0aat110=vec128#94,<z4x5x4t212=vec128#108
# asm 2: mpya >z4x5x4t212=$110,<t4t0bbt12=$7,<et0aat110=$96,<z4x5x4t212=$110
mpya $110,$7,$96,$110

# qhasm: int32323232 z4x5x4t26 += (t4t0bbt16 & 0xffff) * (et0aat10 & 0xffff)
# asm 1: mpya >z4x5x4t26=vec128#102,<t4t0bbt16=vec128#55,<et0aat10=vec128#100,<z4x5x4t26=vec128#102
# asm 2: mpya >z4x5x4t26=$104,<t4t0bbt16=$57,<et0aat10=$102,<z4x5x4t26=$104
mpya $104,$57,$102,$104

# qhasm: int32323232 z4x5x4t27 += (t4t0bbt17 & 0xffff) * (et0aat10 & 0xffff)
# asm 1: mpya >z4x5x4t27=vec128#80,<t4t0bbt17=vec128#67,<et0aat10=vec128#100,<z4x5x4t27=vec128#80
# asm 2: mpya >z4x5x4t27=$82,<t4t0bbt17=$69,<et0aat10=$102,<z4x5x4t27=$82
mpya $82,$69,$102,$82

# qhasm: int32323232 z4x5x4t28 <<= 1
# asm 1: shli >z4x5x4t28=vec128#104,<z4x5x4t28=vec128#104,1
# asm 2: shli >z4x5x4t28=$106,<z4x5x4t28=$106,1
shli $106,$106,1

# qhasm: int32323232 z4x5x4t29 += (t4t0bbt10 & 0xffff) * (et0aat19 & 0xffff)
# asm 1: mpya >z4x5x4t29=vec128#105,<t4t0bbt10=vec128#58,<et0aat19=vec128#93,<z4x5x4t29=vec128#105
# asm 2: mpya >z4x5x4t29=$107,<t4t0bbt10=$60,<et0aat19=$95,<z4x5x4t29=$107
mpya $107,$60,$95,$107

# qhasm: int32323232 z4x5x4t210 += (t4t0bbt11  & 0xffff) * (et0aat19  & 0xffff)
# asm 1: mpya >z4x5x4t210=vec128#106,<t4t0bbt11=vec128#63,<et0aat19=vec128#93,<z4x5x4t210=vec128#106
# asm 2: mpya >z4x5x4t210=$108,<t4t0bbt11=$65,<et0aat19=$95,<z4x5x4t210=$108
mpya $108,$65,$95,$108

# qhasm: int32323232 z4x5x4t211 += (t4t0bbt18  & 0xffff) * (et0aat13  & 0xffff)
# asm 1: mpya >z4x5x4t211=vec128#107,<t4t0bbt18=vec128#62,<et0aat13=vec128#97,<z4x5x4t211=vec128#107
# asm 2: mpya >z4x5x4t211=$109,<t4t0bbt18=$64,<et0aat13=$99,<z4x5x4t211=$109
mpya $109,$64,$99,$109

# qhasm: int32323232 z4x5x4t212 += (t4t0bbt13  & 0xffff) * (et0aat19  & 0xffff)
# asm 1: mpya >z4x5x4t212=vec128#108,<t4t0bbt13=vec128#66,<et0aat19=vec128#93,<z4x5x4t212=vec128#108
# asm 2: mpya >z4x5x4t212=$110,<t4t0bbt13=$68,<et0aat19=$95,<z4x5x4t212=$110
mpya $110,$68,$95,$110

# qhasm: int32323232 z4x5x4t213  = (t4t0bbt12  & 0xffff) * (et0aat111 & 0xffff)
# asm 1: mpy >z4x5x4t213=vec128#109,<t4t0bbt12=vec128#5,<et0aat111=vec128#95
# asm 2: mpy >z4x5x4t213=$111,<t4t0bbt12=$7,<et0aat111=$97
mpy $111,$7,$97

# qhasm: int32323232 z4x5x4t214  = (t4t0bbt13  & 0xffff) * (et0aat111 & 0xffff)
# asm 1: mpy >z4x5x4t214=vec128#110,<t4t0bbt13=vec128#66,<et0aat111=vec128#95
# asm 2: mpy >z4x5x4t214=$112,<t4t0bbt13=$68,<et0aat111=$97
mpy $112,$68,$97

# qhasm: int32323232 z4x5x4t28 += (t4t0bbt10 & 0xffff) * (et0aat18 & 0xffff)
# asm 1: mpya >z4x5x4t28=vec128#104,<t4t0bbt10=vec128#58,<et0aat18=vec128#99,<z4x5x4t28=vec128#104
# asm 2: mpya >z4x5x4t28=$106,<t4t0bbt10=$60,<et0aat18=$101,<z4x5x4t28=$106
mpya $106,$60,$101,$106

# qhasm: int32323232 z4x5x4t29 += (t4t0bbt11 & 0xffff) * (et0aat18 & 0xffff)
# asm 1: mpya >z4x5x4t29=vec128#105,<t4t0bbt11=vec128#63,<et0aat18=vec128#99,<z4x5x4t29=vec128#105
# asm 2: mpya >z4x5x4t29=$107,<t4t0bbt11=$65,<et0aat18=$101,<z4x5x4t29=$107
mpya $107,$65,$101,$107

# qhasm: int32323232 z4x5x4t210 += (t4t0bbt12  & 0xffff) * (et0aat18  & 0xffff)
# asm 1: mpya >z4x5x4t210=vec128#106,<t4t0bbt12=vec128#5,<et0aat18=vec128#99,<z4x5x4t210=vec128#106
# asm 2: mpya >z4x5x4t210=$108,<t4t0bbt12=$7,<et0aat18=$101,<z4x5x4t210=$108
mpya $108,$7,$101,$108

# qhasm: int32323232 z4x5x4t211 += (t4t0bbt17  & 0xffff) * (et0aat14  & 0xffff)
# asm 1: mpya >z4x5x4t211=vec128#107,<t4t0bbt17=vec128#67,<et0aat14=vec128#98,<z4x5x4t211=vec128#107
# asm 2: mpya >z4x5x4t211=$109,<t4t0bbt17=$69,<et0aat14=$100,<z4x5x4t211=$109
mpya $109,$69,$100,$109

# qhasm: int32323232 z4x5x4t212 += (t4t0bbt15  & 0xffff) * (et0aat17  & 0xffff)
# asm 1: mpya >z4x5x4t212=vec128#108,<t4t0bbt15=vec128#65,<et0aat17=vec128#89,<z4x5x4t212=vec128#108
# asm 2: mpya >z4x5x4t212=$110,<t4t0bbt15=$67,<et0aat17=$91,<z4x5x4t212=$110
mpya $110,$67,$91,$110

# qhasm: int32323232 z4x5x4t213 += (t4t0bbt13  & 0xffff) * (et0aat110 & 0xffff)
# asm 1: mpya >z4x5x4t213=vec128#109,<t4t0bbt13=vec128#66,<et0aat110=vec128#94,<z4x5x4t213=vec128#109
# asm 2: mpya >z4x5x4t213=$111,<t4t0bbt13=$68,<et0aat110=$96,<z4x5x4t213=$111
mpya $111,$68,$96,$111

# qhasm: int32323232 z4x5x4t214 += (t4t0bbt17  & 0xffff) * (et0aat17  & 0xffff)
# asm 1: mpya >z4x5x4t214=vec128#110,<t4t0bbt17=vec128#67,<et0aat17=vec128#89,<z4x5x4t214=vec128#110
# asm 2: mpya >z4x5x4t214=$112,<t4t0bbt17=$69,<et0aat17=$91,<z4x5x4t214=$112
mpya $112,$69,$91,$112

# qhasm: int32323232 z4x5x4t28 += (t4t0bbt14 & 0xffff) * (et0aat14 & 0xffff)
# asm 1: mpya >z4x5x4t28=vec128#104,<t4t0bbt14=vec128#61,<et0aat14=vec128#98,<z4x5x4t28=vec128#104
# asm 2: mpya >z4x5x4t28=$106,<t4t0bbt14=$63,<et0aat14=$100,<z4x5x4t28=$106
mpya $106,$63,$100,$106

# qhasm: int32323232 z4x5x4t29 += (t4t0bbt14 & 0xffff) * (et0aat15 & 0xffff)
# asm 1: mpya >z4x5x4t29=vec128#105,<t4t0bbt14=vec128#61,<et0aat15=vec128#92,<z4x5x4t29=vec128#105
# asm 2: mpya >z4x5x4t29=$107,<t4t0bbt14=$63,<et0aat15=$94,<z4x5x4t29=$107
mpya $107,$63,$94,$107

# qhasm: int32323232 z4x5x4t210 += (t4t0bbt14  & 0xffff) * (et0aat16  & 0xffff)
# asm 1: mpya >z4x5x4t210=vec128#106,<t4t0bbt14=vec128#61,<et0aat16=vec128#90,<z4x5x4t210=vec128#106
# asm 2: mpya >z4x5x4t210=$108,<t4t0bbt14=$63,<et0aat16=$92,<z4x5x4t210=$108
mpya $108,$63,$92,$108

# qhasm: int32323232 z4x5x4t211 += (t4t0bbt16  & 0xffff) * (et0aat15  & 0xffff)
# asm 1: mpya >z4x5x4t211=vec128#107,<t4t0bbt16=vec128#55,<et0aat15=vec128#92,<z4x5x4t211=vec128#107
# asm 2: mpya >z4x5x4t211=$109,<t4t0bbt16=$57,<et0aat15=$94,<z4x5x4t211=$109
mpya $109,$57,$94,$109

# qhasm: int32323232 z4x5x4t212 += (t4t0bbt16  & 0xffff) * (et0aat16  & 0xffff)
# asm 1: mpya >z4x5x4t212=vec128#108,<t4t0bbt16=vec128#55,<et0aat16=vec128#90,<z4x5x4t212=vec128#108
# asm 2: mpya >z4x5x4t212=$110,<t4t0bbt16=$57,<et0aat16=$92,<z4x5x4t212=$110
mpya $110,$57,$92,$110

# qhasm: int32323232 z4x5x4t213 += (t4t0bbt16  & 0xffff) * (et0aat17  & 0xffff)
# asm 1: mpya >z4x5x4t213=vec128#109,<t4t0bbt16=vec128#55,<et0aat17=vec128#89,<z4x5x4t213=vec128#109
# asm 2: mpya >z4x5x4t213=$111,<t4t0bbt16=$57,<et0aat17=$91,<z4x5x4t213=$111
mpya $111,$57,$91,$111

# qhasm: int32323232 z4x5x4t214 += (t4t0bbt111 & 0xffff) * (et0aat13  & 0xffff)
# asm 1: mpya >z4x5x4t214=vec128#110,<t4t0bbt111=vec128#68,<et0aat13=vec128#97,<z4x5x4t214=vec128#110
# asm 2: mpya >z4x5x4t214=$112,<t4t0bbt111=$70,<et0aat13=$99,<z4x5x4t214=$112
mpya $112,$70,$99,$112

# qhasm: int32323232 z4x5x4t28 += (t4t0bbt18 & 0xffff) * (et0aat10 & 0xffff)
# asm 1: mpya >z4x5x4t28=vec128#104,<t4t0bbt18=vec128#62,<et0aat10=vec128#100,<z4x5x4t28=vec128#104
# asm 2: mpya >z4x5x4t28=$106,<t4t0bbt18=$64,<et0aat10=$102,<z4x5x4t28=$106
mpya $106,$64,$102,$106

# qhasm: int32323232 z4x5x4t29 += (t4t0bbt15 & 0xffff) * (et0aat14 & 0xffff)
# asm 1: mpya >z4x5x4t29=vec128#105,<t4t0bbt15=vec128#65,<et0aat14=vec128#98,<z4x5x4t29=vec128#105
# asm 2: mpya >z4x5x4t29=$107,<t4t0bbt15=$67,<et0aat14=$100,<z4x5x4t29=$107
mpya $107,$67,$100,$107

# qhasm: int32323232 z4x5x4t210 += (t4t0bbt15  & 0xffff) * (et0aat15  & 0xffff)
# asm 1: mpya >z4x5x4t210=vec128#106,<t4t0bbt15=vec128#65,<et0aat15=vec128#92,<z4x5x4t210=vec128#106
# asm 2: mpya >z4x5x4t210=$108,<t4t0bbt15=$67,<et0aat15=$94,<z4x5x4t210=$108
mpya $108,$67,$94,$108

# qhasm: int32323232 z4x5x4t211 += (t4t0bbt15  & 0xffff) * (et0aat16  & 0xffff)
# asm 1: mpya >z4x5x4t211=vec128#107,<t4t0bbt15=vec128#65,<et0aat16=vec128#90,<z4x5x4t211=vec128#107
# asm 2: mpya >z4x5x4t211=$109,<t4t0bbt15=$67,<et0aat16=$92,<z4x5x4t211=$109
mpya $109,$67,$92,$109

# qhasm: int32323232 z4x5x4t212 += (t4t0bbt17  & 0xffff) * (et0aat15  & 0xffff)
# asm 1: mpya >z4x5x4t212=vec128#108,<t4t0bbt17=vec128#67,<et0aat15=vec128#92,<z4x5x4t212=vec128#108
# asm 2: mpya >z4x5x4t212=$110,<t4t0bbt17=$69,<et0aat15=$94,<z4x5x4t212=$110
mpya $110,$69,$94,$110

# qhasm: int32323232 z4x5x4t213 += (t4t0bbt17  & 0xffff) * (et0aat16  & 0xffff)
# asm 1: mpya >z4x5x4t213=vec128#109,<t4t0bbt17=vec128#67,<et0aat16=vec128#90,<z4x5x4t213=vec128#109
# asm 2: mpya >z4x5x4t213=$111,<t4t0bbt17=$69,<et0aat16=$92,<z4x5x4t213=$111
mpya $111,$69,$92,$111

# qhasm: int32323232 z4x5x4t214 <<= 1
# asm 1: shli >z4x5x4t214=vec128#110,<z4x5x4t214=vec128#110,1
# asm 2: shli >z4x5x4t214=$112,<z4x5x4t214=$112,1
shli $112,$112,1

# qhasm: int32323232 z4x5x4t215  = (t4t0bbt10  & 0xffff) * (et0aat115 & 0xffff)
# asm 1: mpy >z4x5x4t215=vec128#111,<t4t0bbt10=vec128#58,<et0aat115=vec128#103
# asm 2: mpy >z4x5x4t215=$113,<t4t0bbt10=$60,<et0aat115=$105
mpy $113,$60,$105

# qhasm: int32323232 z4x5x4t29 += (t4t0bbt18 & 0xffff) * (et0aat11 & 0xffff)
# asm 1: mpya >z4x5x4t29=vec128#105,<t4t0bbt18=vec128#62,<et0aat11=vec128#101,<z4x5x4t29=vec128#105
# asm 2: mpya >z4x5x4t29=$107,<t4t0bbt18=$64,<et0aat11=$103,<z4x5x4t29=$107
mpya $107,$64,$103,$107

# qhasm: int32323232 z4x5x4t210 += (t4t0bbt16  & 0xffff) * (et0aat14  & 0xffff)
# asm 1: mpya >z4x5x4t210=vec128#106,<t4t0bbt16=vec128#55,<et0aat14=vec128#98,<z4x5x4t210=vec128#106
# asm 2: mpya >z4x5x4t210=$108,<t4t0bbt16=$57,<et0aat14=$100,<z4x5x4t210=$108
mpya $108,$57,$100,$108

# qhasm: int32323232 z4x5x4t211 += (t4t0bbt14  & 0xffff) * (et0aat17  & 0xffff)
# asm 1: mpya >z4x5x4t211=vec128#107,<t4t0bbt14=vec128#61,<et0aat17=vec128#89,<z4x5x4t211=vec128#107
# asm 2: mpya >z4x5x4t211=$109,<t4t0bbt14=$63,<et0aat17=$91,<z4x5x4t211=$109
mpya $109,$63,$91,$109

# qhasm: int32323232 z4x5x4t212 += (t4t0bbt19  & 0xffff) * (et0aat13  & 0xffff)
# asm 1: mpya >z4x5x4t212=vec128#108,<t4t0bbt19=vec128#64,<et0aat13=vec128#97,<z4x5x4t212=vec128#108
# asm 2: mpya >z4x5x4t212=$110,<t4t0bbt19=$66,<et0aat13=$99,<z4x5x4t212=$110
mpya $110,$66,$99,$110

# qhasm: int32323232 z4x5x4t213 += (t4t0bbt110 & 0xffff) * (et0aat13  & 0xffff)
# asm 1: mpya >z4x5x4t213=vec128#109,<t4t0bbt110=vec128#56,<et0aat13=vec128#97,<z4x5x4t213=vec128#109
# asm 2: mpya >z4x5x4t213=$111,<t4t0bbt110=$58,<et0aat13=$99,<z4x5x4t213=$111
mpya $111,$58,$99,$111

# qhasm: int32323232 z4x5x4t214 += (t4t0bbt10  & 0xffff) * (et0aat114 & 0xffff)
# asm 1: mpya >z4x5x4t214=vec128#110,<t4t0bbt10=vec128#58,<et0aat114=vec128#83,<z4x5x4t214=vec128#110
# asm 2: mpya >z4x5x4t214=$112,<t4t0bbt10=$60,<et0aat114=$85,<z4x5x4t214=$112
mpya $112,$60,$85,$112

# qhasm: int32323232 z4x5x4t215 += (t4t0bbt11  & 0xffff) * (et0aat114 & 0xffff)
# asm 1: mpya >z4x5x4t215=vec128#111,<t4t0bbt11=vec128#63,<et0aat114=vec128#83,<z4x5x4t215=vec128#111
# asm 2: mpya >z4x5x4t215=$113,<t4t0bbt11=$65,<et0aat114=$85,<z4x5x4t215=$113
mpya $113,$65,$85,$113

# qhasm: int32323232 z4x5x4t29 += (t4t0bbt19 & 0xffff) * (et0aat10 & 0xffff)
# asm 1: mpya >z4x5x4t29=vec128#105,<t4t0bbt19=vec128#64,<et0aat10=vec128#100,<z4x5x4t29=vec128#105
# asm 2: mpya >z4x5x4t29=$107,<t4t0bbt19=$66,<et0aat10=$102,<z4x5x4t29=$107
mpya $107,$66,$102,$107

# qhasm: int32323232 z4x5x4t210 += (t4t0bbt18  & 0xffff) * (et0aat12  & 0xffff)
# asm 1: mpya >z4x5x4t210=vec128#106,<t4t0bbt18=vec128#62,<et0aat12=vec128#96,<z4x5x4t210=vec128#106
# asm 2: mpya >z4x5x4t210=$108,<t4t0bbt18=$64,<et0aat12=$98,<z4x5x4t210=$108
mpya $108,$64,$98,$108

# qhasm: int32323232 z4x5x4t211 += (t4t0bbt13  & 0xffff) * (et0aat18  & 0xffff)
# asm 1: mpya >z4x5x4t211=vec128#107,<t4t0bbt13=vec128#66,<et0aat18=vec128#99,<z4x5x4t211=vec128#107
# asm 2: mpya >z4x5x4t211=$109,<t4t0bbt13=$68,<et0aat18=$101,<z4x5x4t211=$109
mpya $109,$68,$101,$109

# qhasm: int32323232 z4x5x4t212 += (t4t0bbt110 & 0xffff) * (et0aat12  & 0xffff)
# asm 1: mpya >z4x5x4t212=vec128#108,<t4t0bbt110=vec128#56,<et0aat12=vec128#96,<z4x5x4t212=vec128#108
# asm 2: mpya >z4x5x4t212=$110,<t4t0bbt110=$58,<et0aat12=$98,<z4x5x4t212=$110
mpya $110,$58,$98,$110

# qhasm: int32323232 z4x5x4t213 += (t4t0bbt111 & 0xffff) * (et0aat12  & 0xffff)
# asm 1: mpya >z4x5x4t213=vec128#109,<t4t0bbt111=vec128#68,<et0aat12=vec128#96,<z4x5x4t213=vec128#109
# asm 2: mpya >z4x5x4t213=$111,<t4t0bbt111=$70,<et0aat12=$98,<z4x5x4t213=$111
mpya $111,$70,$98,$111

# qhasm: int32323232 z4x5x4t214 += (t4t0bbt11  & 0xffff) * (et0aat113 & 0xffff)
# asm 1: mpya >z4x5x4t214=vec128#110,<t4t0bbt11=vec128#63,<et0aat113=vec128#84,<z4x5x4t214=vec128#110
# asm 2: mpya >z4x5x4t214=$112,<t4t0bbt11=$65,<et0aat113=$86,<z4x5x4t214=$112
mpya $112,$65,$86,$112

# qhasm: int32323232 z4x5x4t215 += (t4t0bbt12  & 0xffff) * (et0aat113 & 0xffff)
# asm 1: mpya >z4x5x4t215=vec128#111,<t4t0bbt12=vec128#5,<et0aat113=vec128#84,<z4x5x4t215=vec128#111
# asm 2: mpya >z4x5x4t215=$113,<t4t0bbt12=$7,<et0aat113=$86,<z4x5x4t215=$113
mpya $113,$7,$86,$113

# qhasm: int32323232 z4x5x4t216  = (t4t0bbt11  & 0xffff) * (et0aat115 & 0xffff)
# asm 1: mpy >z4x5x4t216=vec128#112,<t4t0bbt11=vec128#63,<et0aat115=vec128#103
# asm 2: mpy >z4x5x4t216=$114,<t4t0bbt11=$65,<et0aat115=$105
mpy $114,$65,$105

# qhasm: int32323232 z4x5x4t210 += (t4t0bbt19  & 0xffff) * (et0aat11  & 0xffff)
# asm 1: mpya >z4x5x4t210=vec128#106,<t4t0bbt19=vec128#64,<et0aat11=vec128#101,<z4x5x4t210=vec128#106
# asm 2: mpya >z4x5x4t210=$108,<t4t0bbt19=$66,<et0aat11=$103,<z4x5x4t210=$108
mpya $108,$66,$103,$108

# qhasm: int32323232 z4x5x4t211 += (t4t0bbt12  & 0xffff) * (et0aat19  & 0xffff)
# asm 1: mpya >z4x5x4t211=vec128#107,<t4t0bbt12=vec128#5,<et0aat19=vec128#93,<z4x5x4t211=vec128#107
# asm 2: mpya >z4x5x4t211=$109,<t4t0bbt12=$7,<et0aat19=$95,<z4x5x4t211=$109
mpya $109,$7,$95,$109

# qhasm: int32323232 z4x5x4t212 += (t4t0bbt111 & 0xffff) * (et0aat11  & 0xffff)
# asm 1: mpya >z4x5x4t212=vec128#108,<t4t0bbt111=vec128#68,<et0aat11=vec128#101,<z4x5x4t212=vec128#108
# asm 2: mpya >z4x5x4t212=$110,<t4t0bbt111=$70,<et0aat11=$103,<z4x5x4t212=$110
mpya $110,$70,$103,$110

# qhasm: int32323232 z4x5x4t213 <<= 1
# asm 1: shli >z4x5x4t213=vec128#109,<z4x5x4t213=vec128#109,1
# asm 2: shli >z4x5x4t213=$111,<z4x5x4t213=$111,1
shli $111,$111,1

# qhasm: int32323232 z4x5x4t214 += (t4t0bbt12  & 0xffff) * (et0aat112 & 0xffff)
# asm 1: mpya >z4x5x4t214=vec128#110,<t4t0bbt12=vec128#5,<et0aat112=vec128#81,<z4x5x4t214=vec128#110
# asm 2: mpya >z4x5x4t214=$112,<t4t0bbt12=$7,<et0aat112=$83,<z4x5x4t214=$112
mpya $112,$7,$83,$112

# qhasm: int32323232 z4x5x4t215 += (t4t0bbt13  & 0xffff) * (et0aat112 & 0xffff)
# asm 1: mpya >z4x5x4t215=vec128#111,<t4t0bbt13=vec128#66,<et0aat112=vec128#81,<z4x5x4t215=vec128#111
# asm 2: mpya >z4x5x4t215=$113,<t4t0bbt13=$68,<et0aat112=$83,<z4x5x4t215=$113
mpya $113,$68,$83,$113

# qhasm: int32323232 z4x5x4t216 += (t4t0bbt12  & 0xffff) * (et0aat114 & 0xffff)
# asm 1: mpya >z4x5x4t216=vec128#112,<t4t0bbt12=vec128#5,<et0aat114=vec128#83,<z4x5x4t216=vec128#112
# asm 2: mpya >z4x5x4t216=$114,<t4t0bbt12=$7,<et0aat114=$85,<z4x5x4t216=$114
mpya $114,$7,$85,$114

# qhasm: int32323232 z4x5x4t210 += (t4t0bbt110 & 0xffff) * (et0aat10  & 0xffff)
# asm 1: mpya >z4x5x4t210=vec128#106,<t4t0bbt110=vec128#56,<et0aat10=vec128#100,<z4x5x4t210=vec128#106
# asm 2: mpya >z4x5x4t210=$108,<t4t0bbt110=$58,<et0aat10=$102,<z4x5x4t210=$108
mpya $108,$58,$102,$108

# qhasm: int32323232 z4x5x4t211 += (t4t0bbt11  & 0xffff) * (et0aat110 & 0xffff)
# asm 1: mpya >z4x5x4t211=vec128#107,<t4t0bbt11=vec128#63,<et0aat110=vec128#94,<z4x5x4t211=vec128#107
# asm 2: mpya >z4x5x4t211=$109,<t4t0bbt11=$65,<et0aat110=$96,<z4x5x4t211=$109
mpya $109,$65,$96,$109

# qhasm: int32323232 z4x5x4t212 <<= 1
# asm 1: shli >z4x5x4t212=vec128#108,<z4x5x4t212=vec128#108,1
# asm 2: shli >z4x5x4t212=$110,<z4x5x4t212=$110,1
shli $110,$110,1

# qhasm: int32323232 z4x5x4t213 += (t4t0bbt10  & 0xffff) * (et0aat113 & 0xffff)
# asm 1: mpya >z4x5x4t213=vec128#109,<t4t0bbt10=vec128#58,<et0aat113=vec128#84,<z4x5x4t213=vec128#109
# asm 2: mpya >z4x5x4t213=$111,<t4t0bbt10=$60,<et0aat113=$86,<z4x5x4t213=$111
mpya $111,$60,$86,$111

# qhasm: int32323232 z4x5x4t214 += (t4t0bbt14  & 0xffff) * (et0aat110 & 0xffff)
# asm 1: mpya >z4x5x4t214=vec128#110,<t4t0bbt14=vec128#61,<et0aat110=vec128#94,<z4x5x4t214=vec128#110
# asm 2: mpya >z4x5x4t214=$112,<t4t0bbt14=$63,<et0aat110=$96,<z4x5x4t214=$112
mpya $112,$63,$96,$112

# qhasm: int32323232 z4x5x4t215 += (t4t0bbt14  & 0xffff) * (et0aat111 & 0xffff)
# asm 1: mpya >z4x5x4t215=vec128#111,<t4t0bbt14=vec128#61,<et0aat111=vec128#95,<z4x5x4t215=vec128#111
# asm 2: mpya >z4x5x4t215=$113,<t4t0bbt14=$63,<et0aat111=$97,<z4x5x4t215=$113
mpya $113,$63,$97,$113

# qhasm: int32323232 z4x5x4t216 += (t4t0bbt13  & 0xffff) * (et0aat113 & 0xffff)
# asm 1: mpya >z4x5x4t216=vec128#112,<t4t0bbt13=vec128#66,<et0aat113=vec128#84,<z4x5x4t216=vec128#112
# asm 2: mpya >z4x5x4t216=$114,<t4t0bbt13=$68,<et0aat113=$86,<z4x5x4t216=$114
mpya $114,$68,$86,$114

# qhasm: int32323232 z4x5x4t217  = (t4t0bbt12  & 0xffff) * (et0aat115 & 0xffff)
# asm 1: mpy >z4x5x4t217=vec128#113,<t4t0bbt12=vec128#5,<et0aat115=vec128#103
# asm 2: mpy >z4x5x4t217=$115,<t4t0bbt12=$7,<et0aat115=$105
mpy $115,$7,$105

# qhasm: int32323232 z4x5x4t211 += (t4t0bbt10  & 0xffff) * (et0aat111 & 0xffff)
# asm 1: mpya >z4x5x4t211=vec128#107,<t4t0bbt10=vec128#58,<et0aat111=vec128#95,<z4x5x4t211=vec128#107
# asm 2: mpya >z4x5x4t211=$109,<t4t0bbt10=$60,<et0aat111=$97,<z4x5x4t211=$109
mpya $109,$60,$97,$109

# qhasm: int32323232 z4x5x4t212 += (t4t0bbt10  & 0xffff) * (et0aat112 & 0xffff)
# asm 1: mpya >z4x5x4t212=vec128#108,<t4t0bbt10=vec128#58,<et0aat112=vec128#81,<z4x5x4t212=vec128#108
# asm 2: mpya >z4x5x4t212=$110,<t4t0bbt10=$60,<et0aat112=$83,<z4x5x4t212=$110
mpya $110,$60,$83,$110

# qhasm: int32323232 z4x5x4t213 += (t4t0bbt11  & 0xffff) * (et0aat112 & 0xffff)
# asm 1: mpya >z4x5x4t213=vec128#109,<t4t0bbt11=vec128#63,<et0aat112=vec128#81,<z4x5x4t213=vec128#109
# asm 2: mpya >z4x5x4t213=$111,<t4t0bbt11=$65,<et0aat112=$83,<z4x5x4t213=$111
mpya $111,$65,$83,$111

# qhasm: int32323232 z4x5x4t214 += (t4t0bbt15  & 0xffff) * (et0aat19  & 0xffff)
# asm 1: mpya >z4x5x4t214=vec128#110,<t4t0bbt15=vec128#65,<et0aat19=vec128#93,<z4x5x4t214=vec128#110
# asm 2: mpya >z4x5x4t214=$112,<t4t0bbt15=$67,<et0aat19=$95,<z4x5x4t214=$112
mpya $112,$67,$95,$112

# qhasm: int32323232 z4x5x4t215 += (t4t0bbt15  & 0xffff) * (et0aat110 & 0xffff)
# asm 1: mpya >z4x5x4t215=vec128#111,<t4t0bbt15=vec128#65,<et0aat110=vec128#94,<z4x5x4t215=vec128#111
# asm 2: mpya >z4x5x4t215=$113,<t4t0bbt15=$67,<et0aat110=$96,<z4x5x4t215=$113
mpya $113,$67,$96,$113

# qhasm: int32323232 z4x5x4t216 += (t4t0bbt15  & 0xffff) * (et0aat111 & 0xffff)
# asm 1: mpya >z4x5x4t216=vec128#112,<t4t0bbt15=vec128#65,<et0aat111=vec128#95,<z4x5x4t216=vec128#112
# asm 2: mpya >z4x5x4t216=$114,<t4t0bbt15=$67,<et0aat111=$97,<z4x5x4t216=$114
mpya $114,$67,$97,$114

# qhasm: int32323232 z4x5x4t217 += (t4t0bbt13  & 0xffff) * (et0aat114 & 0xffff)
# asm 1: mpya >z4x5x4t217=vec128#113,<t4t0bbt13=vec128#66,<et0aat114=vec128#83,<z4x5x4t217=vec128#113
# asm 2: mpya >z4x5x4t217=$115,<t4t0bbt13=$68,<et0aat114=$85,<z4x5x4t217=$115
mpya $115,$68,$85,$115

# qhasm: int32323232 z4x5x4t218  = (t4t0bbt13  & 0xffff) * (et0aat115 & 0xffff)
# asm 1: mpy >z4x5x4t218=vec128#114,<t4t0bbt13=vec128#66,<et0aat115=vec128#103
# asm 2: mpy >z4x5x4t218=$116,<t4t0bbt13=$68,<et0aat115=$105
mpy $116,$68,$105

# qhasm: int32323232 z4x5x4t212 += (t4t0bbt14  & 0xffff) * (et0aat18  & 0xffff)
# asm 1: mpya >z4x5x4t212=vec128#108,<t4t0bbt14=vec128#61,<et0aat18=vec128#99,<z4x5x4t212=vec128#108
# asm 2: mpya >z4x5x4t212=$110,<t4t0bbt14=$63,<et0aat18=$101,<z4x5x4t212=$110
mpya $110,$63,$101,$110

# qhasm: int32323232 z4x5x4t213 += (t4t0bbt14  & 0xffff) * (et0aat19  & 0xffff)
# asm 1: mpya >z4x5x4t213=vec128#109,<t4t0bbt14=vec128#61,<et0aat19=vec128#93,<z4x5x4t213=vec128#109
# asm 2: mpya >z4x5x4t213=$111,<t4t0bbt14=$63,<et0aat19=$95,<z4x5x4t213=$111
mpya $111,$63,$95,$111

# qhasm: int32323232 z4x5x4t214 += (t4t0bbt16  & 0xffff) * (et0aat18  & 0xffff)
# asm 1: mpya >z4x5x4t214=vec128#110,<t4t0bbt16=vec128#55,<et0aat18=vec128#99,<z4x5x4t214=vec128#110
# asm 2: mpya >z4x5x4t214=$112,<t4t0bbt16=$57,<et0aat18=$101,<z4x5x4t214=$112
mpya $112,$57,$101,$112

# qhasm: int32323232 z4x5x4t215 += (t4t0bbt16  & 0xffff) * (et0aat19  & 0xffff)
# asm 1: mpya >z4x5x4t215=vec128#111,<t4t0bbt16=vec128#55,<et0aat19=vec128#93,<z4x5x4t215=vec128#111
# asm 2: mpya >z4x5x4t215=$113,<t4t0bbt16=$57,<et0aat19=$95,<z4x5x4t215=$113
mpya $113,$57,$95,$113

# qhasm: int32323232 z4x5x4t216 += (t4t0bbt16  & 0xffff) * (et0aat110 & 0xffff)
# asm 1: mpya >z4x5x4t216=vec128#112,<t4t0bbt16=vec128#55,<et0aat110=vec128#94,<z4x5x4t216=vec128#112
# asm 2: mpya >z4x5x4t216=$114,<t4t0bbt16=$57,<et0aat110=$96,<z4x5x4t216=$114
mpya $114,$57,$96,$114

# qhasm: int32323232 z4x5x4t217 += (t4t0bbt16  & 0xffff) * (et0aat111 & 0xffff)
# asm 1: mpya >z4x5x4t217=vec128#113,<t4t0bbt16=vec128#55,<et0aat111=vec128#95,<z4x5x4t217=vec128#113
# asm 2: mpya >z4x5x4t217=$115,<t4t0bbt16=$57,<et0aat111=$97,<z4x5x4t217=$115
mpya $115,$57,$97,$115

# qhasm: int32323232 z4x5x4t218 += (t4t0bbt17  & 0xffff) * (et0aat111 & 0xffff)
# asm 1: mpya >z4x5x4t218=vec128#114,<t4t0bbt17=vec128#67,<et0aat111=vec128#95,<z4x5x4t218=vec128#114
# asm 2: mpya >z4x5x4t218=$116,<t4t0bbt17=$69,<et0aat111=$97,<z4x5x4t218=$116
mpya $116,$69,$97,$116

# qhasm: int32323232 z4x5x4t212 += (t4t0bbt18  & 0xffff) * (et0aat14  & 0xffff)
# asm 1: mpya >z4x5x4t212=vec128#108,<t4t0bbt18=vec128#62,<et0aat14=vec128#98,<z4x5x4t212=vec128#108
# asm 2: mpya >z4x5x4t212=$110,<t4t0bbt18=$64,<et0aat14=$100,<z4x5x4t212=$110
mpya $110,$64,$100,$110

# qhasm: int32323232 z4x5x4t213 += (t4t0bbt15  & 0xffff) * (et0aat18  & 0xffff)
# asm 1: mpya >z4x5x4t213=vec128#109,<t4t0bbt15=vec128#65,<et0aat18=vec128#99,<z4x5x4t213=vec128#109
# asm 2: mpya >z4x5x4t213=$111,<t4t0bbt15=$67,<et0aat18=$101,<z4x5x4t213=$111
mpya $111,$67,$101,$111

# qhasm: int32323232 z4x5x4t214 += (t4t0bbt18  & 0xffff) * (et0aat16  & 0xffff)
# asm 1: mpya >z4x5x4t214=vec128#110,<t4t0bbt18=vec128#62,<et0aat16=vec128#90,<z4x5x4t214=vec128#110
# asm 2: mpya >z4x5x4t214=$112,<t4t0bbt18=$64,<et0aat16=$92,<z4x5x4t214=$112
mpya $112,$64,$92,$112

# qhasm: int32323232 z4x5x4t215 += (t4t0bbt17  & 0xffff) * (et0aat18  & 0xffff)
# asm 1: mpya >z4x5x4t215=vec128#111,<t4t0bbt17=vec128#67,<et0aat18=vec128#99,<z4x5x4t215=vec128#111
# asm 2: mpya >z4x5x4t215=$113,<t4t0bbt17=$69,<et0aat18=$101,<z4x5x4t215=$113
mpya $113,$69,$101,$113

# qhasm: int32323232 z4x5x4t216 += (t4t0bbt17  & 0xffff) * (et0aat19  & 0xffff)
# asm 1: mpya >z4x5x4t216=vec128#112,<t4t0bbt17=vec128#67,<et0aat19=vec128#93,<z4x5x4t216=vec128#112
# asm 2: mpya >z4x5x4t216=$114,<t4t0bbt17=$69,<et0aat19=$95,<z4x5x4t216=$114
mpya $114,$69,$95,$114

# qhasm: int32323232 z4x5x4t217 += (t4t0bbt17  & 0xffff) * (et0aat110 & 0xffff)
# asm 1: mpya >z4x5x4t217=vec128#113,<t4t0bbt17=vec128#67,<et0aat110=vec128#94,<z4x5x4t217=vec128#113
# asm 2: mpya >z4x5x4t217=$115,<t4t0bbt17=$69,<et0aat110=$96,<z4x5x4t217=$115
mpya $115,$69,$96,$115

# qhasm: int32323232 z4x5x4t218 += (t4t0bbt111 & 0xffff) * (et0aat17  & 0xffff)
# asm 1: mpya >z4x5x4t218=vec128#114,<t4t0bbt111=vec128#68,<et0aat17=vec128#89,<z4x5x4t218=vec128#114
# asm 2: mpya >z4x5x4t218=$116,<t4t0bbt111=$70,<et0aat17=$91,<z4x5x4t218=$116
mpya $116,$70,$91,$116

# qhasm: int32323232 z4x5x4t212 += (t4t0bbt112 & 0xffff) * (et0aat10  & 0xffff)
# asm 1: mpya >z4x5x4t212=vec128#108,<t4t0bbt112=vec128#71,<et0aat10=vec128#100,<z4x5x4t212=vec128#108
# asm 2: mpya >z4x5x4t212=$110,<t4t0bbt112=$73,<et0aat10=$102,<z4x5x4t212=$110
mpya $110,$73,$102,$110

# qhasm: int32323232 z4x5x4t213 += (t4t0bbt18  & 0xffff) * (et0aat15  & 0xffff)
# asm 1: mpya >z4x5x4t213=vec128#109,<t4t0bbt18=vec128#62,<et0aat15=vec128#92,<z4x5x4t213=vec128#109
# asm 2: mpya >z4x5x4t213=$111,<t4t0bbt18=$64,<et0aat15=$94,<z4x5x4t213=$111
mpya $111,$64,$94,$111

# qhasm: int32323232 z4x5x4t214 += (t4t0bbt19  & 0xffff) * (et0aat15  & 0xffff)
# asm 1: mpya >z4x5x4t214=vec128#110,<t4t0bbt19=vec128#64,<et0aat15=vec128#92,<z4x5x4t214=vec128#110
# asm 2: mpya >z4x5x4t214=$112,<t4t0bbt19=$66,<et0aat15=$94,<z4x5x4t214=$112
mpya $112,$66,$94,$112

# qhasm: int32323232 z4x5x4t215 += (t4t0bbt18  & 0xffff) * (et0aat17  & 0xffff)
# asm 1: mpya >z4x5x4t215=vec128#111,<t4t0bbt18=vec128#62,<et0aat17=vec128#89,<z4x5x4t215=vec128#111
# asm 2: mpya >z4x5x4t215=$113,<t4t0bbt18=$64,<et0aat17=$91,<z4x5x4t215=$113
mpya $113,$64,$91,$113

# qhasm: int32323232 z4x5x4t216 += (t4t0bbt19  & 0xffff) * (et0aat17  & 0xffff)
# asm 1: mpya >z4x5x4t216=vec128#112,<t4t0bbt19=vec128#64,<et0aat17=vec128#89,<z4x5x4t216=vec128#112
# asm 2: mpya >z4x5x4t216=$114,<t4t0bbt19=$66,<et0aat17=$91,<z4x5x4t216=$114
mpya $114,$66,$91,$114

# qhasm: int32323232 z4x5x4t217 += (t4t0bbt110 & 0xffff) * (et0aat17  & 0xffff)
# asm 1: mpya >z4x5x4t217=vec128#113,<t4t0bbt110=vec128#56,<et0aat17=vec128#89,<z4x5x4t217=vec128#113
# asm 2: mpya >z4x5x4t217=$115,<t4t0bbt110=$58,<et0aat17=$91,<z4x5x4t217=$115
mpya $115,$58,$91,$115

# qhasm: int32323232 z4x5x4t218 += (t4t0bbt115 & 0xffff) * (et0aat13  & 0xffff)
# asm 1: mpya >z4x5x4t218=vec128#114,<t4t0bbt115=vec128#72,<et0aat13=vec128#97,<z4x5x4t218=vec128#114
# asm 2: mpya >z4x5x4t218=$116,<t4t0bbt115=$74,<et0aat13=$99,<z4x5x4t218=$116
mpya $116,$74,$99,$116

# qhasm: int32323232 z4x5x4t219  = (t4t0bbt10  & 0xffff) * (et0aat119 & 0xffff)
# asm 1: mpy >z4x5x4t219=vec128#115,<t4t0bbt10=vec128#58,<et0aat119=vec128#91
# asm 2: mpy >z4x5x4t219=$117,<t4t0bbt10=$60,<et0aat119=$93
mpy $117,$60,$93

# qhasm: int32323232 z4x5x4t213 += (t4t0bbt19  & 0xffff) * (et0aat14  & 0xffff)
# asm 1: mpya >z4x5x4t213=vec128#109,<t4t0bbt19=vec128#64,<et0aat14=vec128#98,<z4x5x4t213=vec128#109
# asm 2: mpya >z4x5x4t213=$111,<t4t0bbt19=$66,<et0aat14=$100,<z4x5x4t213=$111
mpya $111,$66,$100,$111

# qhasm: int32323232 z4x5x4t214 += (t4t0bbt110 & 0xffff) * (et0aat14  & 0xffff)
# asm 1: mpya >z4x5x4t214=vec128#110,<t4t0bbt110=vec128#56,<et0aat14=vec128#98,<z4x5x4t214=vec128#110
# asm 2: mpya >z4x5x4t214=$112,<t4t0bbt110=$58,<et0aat14=$100,<z4x5x4t214=$112
mpya $112,$58,$100,$112

# qhasm: int32323232 z4x5x4t215 += (t4t0bbt19  & 0xffff) * (et0aat16  & 0xffff)
# asm 1: mpya >z4x5x4t215=vec128#111,<t4t0bbt19=vec128#64,<et0aat16=vec128#90,<z4x5x4t215=vec128#111
# asm 2: mpya >z4x5x4t215=$113,<t4t0bbt19=$66,<et0aat16=$92,<z4x5x4t215=$113
mpya $113,$66,$92,$113

# qhasm: int32323232 z4x5x4t216 += (t4t0bbt110 & 0xffff) * (et0aat16  & 0xffff)
# asm 1: mpya >z4x5x4t216=vec128#112,<t4t0bbt110=vec128#56,<et0aat16=vec128#90,<z4x5x4t216=vec128#112
# asm 2: mpya >z4x5x4t216=$114,<t4t0bbt110=$58,<et0aat16=$92,<z4x5x4t216=$114
mpya $114,$58,$92,$114

# qhasm: int32323232 z4x5x4t217 += (t4t0bbt111 & 0xffff) * (et0aat16  & 0xffff)
# asm 1: mpya >z4x5x4t217=vec128#113,<t4t0bbt111=vec128#68,<et0aat16=vec128#90,<z4x5x4t217=vec128#113
# asm 2: mpya >z4x5x4t217=$115,<t4t0bbt111=$70,<et0aat16=$92,<z4x5x4t217=$115
mpya $115,$70,$92,$115

# qhasm: int32323232 z4x5x4t218 <<= 1
# asm 1: shli >z4x5x4t218=vec128#114,<z4x5x4t218=vec128#114,1
# asm 2: shli >z4x5x4t218=$116,<z4x5x4t218=$116,1
shli $116,$116,1

# qhasm: int32323232 z4x5x4t219 += (t4t0bbt11  & 0xffff) * (et0aat118 & 0xffff)
# asm 1: mpya >z4x5x4t219=vec128#115,<t4t0bbt11=vec128#63,<et0aat118=vec128#87,<z4x5x4t219=vec128#115
# asm 2: mpya >z4x5x4t219=$117,<t4t0bbt11=$65,<et0aat118=$89,<z4x5x4t219=$117
mpya $117,$65,$89,$117

# qhasm: int32323232 z4x5x4t213 += (t4t0bbt112 & 0xffff) * (et0aat11  & 0xffff)
# asm 1: mpya >z4x5x4t213=vec128#109,<t4t0bbt112=vec128#71,<et0aat11=vec128#101,<z4x5x4t213=vec128#109
# asm 2: mpya >z4x5x4t213=$111,<t4t0bbt112=$73,<et0aat11=$103,<z4x5x4t213=$111
mpya $111,$73,$103,$111

# qhasm: int32323232 z4x5x4t214 += (t4t0bbt112 & 0xffff) * (et0aat12  & 0xffff)
# asm 1: mpya >z4x5x4t214=vec128#110,<t4t0bbt112=vec128#71,<et0aat12=vec128#96,<z4x5x4t214=vec128#110
# asm 2: mpya >z4x5x4t214=$112,<t4t0bbt112=$73,<et0aat12=$98,<z4x5x4t214=$112
mpya $112,$73,$98,$112

# qhasm: int32323232 z4x5x4t215 += (t4t0bbt110 & 0xffff) * (et0aat15  & 0xffff)
# asm 1: mpya >z4x5x4t215=vec128#111,<t4t0bbt110=vec128#56,<et0aat15=vec128#92,<z4x5x4t215=vec128#111
# asm 2: mpya >z4x5x4t215=$113,<t4t0bbt110=$58,<et0aat15=$94,<z4x5x4t215=$113
mpya $113,$58,$94,$113

# qhasm: int32323232 z4x5x4t216 += (t4t0bbt111 & 0xffff) * (et0aat15  & 0xffff)
# asm 1: mpya >z4x5x4t216=vec128#112,<t4t0bbt111=vec128#68,<et0aat15=vec128#92,<z4x5x4t216=vec128#112
# asm 2: mpya >z4x5x4t216=$114,<t4t0bbt111=$70,<et0aat15=$94,<z4x5x4t216=$114
mpya $114,$70,$94,$114

# qhasm: int32323232 z4x5x4t217 += (t4t0bbt114 & 0xffff) * (et0aat13  & 0xffff)
# asm 1: mpya >z4x5x4t217=vec128#113,<t4t0bbt114=vec128#57,<et0aat13=vec128#97,<z4x5x4t217=vec128#113
# asm 2: mpya >z4x5x4t217=$115,<t4t0bbt114=$59,<et0aat13=$99,<z4x5x4t217=$115
mpya $115,$59,$99,$115

# qhasm: int32323232 z4x5x4t218 += (t4t0bbt10  & 0xffff) * (et0aat118 & 0xffff)
# asm 1: mpya >z4x5x4t218=vec128#114,<t4t0bbt10=vec128#58,<et0aat118=vec128#87,<z4x5x4t218=vec128#114
# asm 2: mpya >z4x5x4t218=$116,<t4t0bbt10=$60,<et0aat118=$89,<z4x5x4t218=$116
mpya $116,$60,$89,$116

# qhasm: int32323232 z4x5x4t219 += (t4t0bbt12  & 0xffff) * (et0aat117 & 0xffff)
# asm 1: mpya >z4x5x4t219=vec128#115,<t4t0bbt12=vec128#5,<et0aat117=vec128#88,<z4x5x4t219=vec128#115
# asm 2: mpya >z4x5x4t219=$117,<t4t0bbt12=$7,<et0aat117=$90,<z4x5x4t219=$117
mpya $117,$7,$90,$117

# qhasm: int32323232 z4x5x4t213 += (t4t0bbt113 & 0xffff) * (et0aat10  & 0xffff)
# asm 1: mpya >z4x5x4t213=vec128#109,<t4t0bbt113=vec128#60,<et0aat10=vec128#100,<z4x5x4t213=vec128#109
# asm 2: mpya >z4x5x4t213=$111,<t4t0bbt113=$62,<et0aat10=$102,<z4x5x4t213=$111
mpya $111,$62,$102,$111

# qhasm: int32323232 z4x5x4t214 += (t4t0bbt113 & 0xffff) * (et0aat11  & 0xffff)
# asm 1: mpya >z4x5x4t214=vec128#110,<t4t0bbt113=vec128#60,<et0aat11=vec128#101,<z4x5x4t214=vec128#110
# asm 2: mpya >z4x5x4t214=$112,<t4t0bbt113=$62,<et0aat11=$103,<z4x5x4t214=$112
mpya $112,$62,$103,$112

# qhasm: int32323232 z4x5x4t215 += (t4t0bbt111 & 0xffff) * (et0aat14  & 0xffff)
# asm 1: mpya >z4x5x4t215=vec128#111,<t4t0bbt111=vec128#68,<et0aat14=vec128#98,<z4x5x4t215=vec128#111
# asm 2: mpya >z4x5x4t215=$113,<t4t0bbt111=$70,<et0aat14=$100,<z4x5x4t215=$113
mpya $113,$70,$100,$113

# qhasm: int32323232 z4x5x4t216 += (t4t0bbt113 & 0xffff) * (et0aat13  & 0xffff)
# asm 1: mpya >z4x5x4t216=vec128#112,<t4t0bbt113=vec128#60,<et0aat13=vec128#97,<z4x5x4t216=vec128#112
# asm 2: mpya >z4x5x4t216=$114,<t4t0bbt113=$62,<et0aat13=$99,<z4x5x4t216=$114
mpya $114,$62,$99,$114

# qhasm: int32323232 z4x5x4t217 += (t4t0bbt115 & 0xffff) * (et0aat12  & 0xffff)
# asm 1: mpya >z4x5x4t217=vec128#113,<t4t0bbt115=vec128#72,<et0aat12=vec128#96,<z4x5x4t217=vec128#113
# asm 2: mpya >z4x5x4t217=$115,<t4t0bbt115=$74,<et0aat12=$98,<z4x5x4t217=$115
mpya $115,$74,$98,$115

# qhasm: int32323232 z4x5x4t218 += (t4t0bbt11  & 0xffff) * (et0aat117 & 0xffff)
# asm 1: mpya >z4x5x4t218=vec128#114,<t4t0bbt11=vec128#63,<et0aat117=vec128#88,<z4x5x4t218=vec128#114
# asm 2: mpya >z4x5x4t218=$116,<t4t0bbt11=$65,<et0aat117=$90,<z4x5x4t218=$116
mpya $116,$65,$90,$116

# qhasm: int32323232 z4x5x4t219 += (t4t0bbt13  & 0xffff) * (et0aat116 & 0xffff)
# asm 1: mpya >z4x5x4t219=vec128#115,<t4t0bbt13=vec128#66,<et0aat116=vec128#85,<z4x5x4t219=vec128#115
# asm 2: mpya >z4x5x4t219=$117,<t4t0bbt13=$68,<et0aat116=$87,<z4x5x4t219=$117
mpya $117,$68,$87,$117

# qhasm: int32323232 z4x5x4t220  = (t4t0bbt11  & 0xffff) * (et0aat119 & 0xffff)
# asm 1: mpy >z4x5x4t220=vec128#116,<t4t0bbt11=vec128#63,<et0aat119=vec128#91
# asm 2: mpy >z4x5x4t220=$118,<t4t0bbt11=$65,<et0aat119=$93
mpy $118,$65,$93

# qhasm: int32323232 z4x5x4t214 += (t4t0bbt114 & 0xffff) * (et0aat10  & 0xffff)
# asm 1: mpya >z4x5x4t214=vec128#110,<t4t0bbt114=vec128#57,<et0aat10=vec128#100,<z4x5x4t214=vec128#110
# asm 2: mpya >z4x5x4t214=$112,<t4t0bbt114=$59,<et0aat10=$102,<z4x5x4t214=$112
mpya $112,$59,$102,$112

# qhasm: int32323232 z4x5x4t215 += (t4t0bbt112 & 0xffff) * (et0aat13  & 0xffff)
# asm 1: mpya >z4x5x4t215=vec128#111,<t4t0bbt112=vec128#71,<et0aat13=vec128#97,<z4x5x4t215=vec128#111
# asm 2: mpya >z4x5x4t215=$113,<t4t0bbt112=$73,<et0aat13=$99,<z4x5x4t215=$113
mpya $113,$73,$99,$113

# qhasm: int32323232 z4x5x4t216 += (t4t0bbt114 & 0xffff) * (et0aat12  & 0xffff)
# asm 1: mpya >z4x5x4t216=vec128#112,<t4t0bbt114=vec128#57,<et0aat12=vec128#96,<z4x5x4t216=vec128#112
# asm 2: mpya >z4x5x4t216=$114,<t4t0bbt114=$59,<et0aat12=$98,<z4x5x4t216=$114
mpya $114,$59,$98,$114

# qhasm: int32323232 z4x5x4t217 <<= 1
# asm 1: shli >z4x5x4t217=vec128#113,<z4x5x4t217=vec128#113,1
# asm 2: shli >z4x5x4t217=$115,<z4x5x4t217=$115,1
shli $115,$115,1

# qhasm: int32323232 z4x5x4t218 += (t4t0bbt12  & 0xffff) * (et0aat116 & 0xffff)
# asm 1: mpya >z4x5x4t218=vec128#114,<t4t0bbt12=vec128#5,<et0aat116=vec128#85,<z4x5x4t218=vec128#114
# asm 2: mpya >z4x5x4t218=$116,<t4t0bbt12=$7,<et0aat116=$87,<z4x5x4t218=$116
mpya $116,$7,$87,$116

# qhasm: int32323232 z4x5x4t219 += (t4t0bbt14  & 0xffff) * (et0aat115 & 0xffff)
# asm 1: mpya >z4x5x4t219=vec128#115,<t4t0bbt14=vec128#61,<et0aat115=vec128#103,<z4x5x4t219=vec128#115
# asm 2: mpya >z4x5x4t219=$117,<t4t0bbt14=$63,<et0aat115=$105,<z4x5x4t219=$117
mpya $117,$63,$105,$117

# qhasm: int32323232 z4x5x4t220 += (t4t0bbt12  & 0xffff) * (et0aat118 & 0xffff)
# asm 1: mpya >z4x5x4t220=vec128#116,<t4t0bbt12=vec128#5,<et0aat118=vec128#87,<z4x5x4t220=vec128#116
# asm 2: mpya >z4x5x4t220=$118,<t4t0bbt12=$7,<et0aat118=$89,<z4x5x4t220=$118
mpya $118,$7,$89,$118

# qhasm: int32323232 z4x5x4t221  = (t4t0bbt12  & 0xffff) * (et0aat119 & 0xffff)
# asm 1: mpy >z4x5x4t221=vec128#5,<t4t0bbt12=vec128#5,<et0aat119=vec128#91
# asm 2: mpy >z4x5x4t221=$7,<t4t0bbt12=$7,<et0aat119=$93
mpy $7,$7,$93

# qhasm: int32323232 z4x5x4t215 += (t4t0bbt113 & 0xffff) * (et0aat12  & 0xffff)
# asm 1: mpya >z4x5x4t215=vec128#111,<t4t0bbt113=vec128#60,<et0aat12=vec128#96,<z4x5x4t215=vec128#111
# asm 2: mpya >z4x5x4t215=$113,<t4t0bbt113=$62,<et0aat12=$98,<z4x5x4t215=$113
mpya $113,$62,$98,$113

# qhasm: int32323232 z4x5x4t216 += (t4t0bbt115 & 0xffff) * (et0aat11  & 0xffff)
# asm 1: mpya >z4x5x4t216=vec128#112,<t4t0bbt115=vec128#72,<et0aat11=vec128#101,<z4x5x4t216=vec128#112
# asm 2: mpya >z4x5x4t216=$114,<t4t0bbt115=$74,<et0aat11=$103,<z4x5x4t216=$114
mpya $114,$74,$103,$114

# qhasm: int32323232 z4x5x4t217 += (t4t0bbt10  & 0xffff) * (et0aat117 & 0xffff)
# asm 1: mpya >z4x5x4t217=vec128#113,<t4t0bbt10=vec128#58,<et0aat117=vec128#88,<z4x5x4t217=vec128#113
# asm 2: mpya >z4x5x4t217=$115,<t4t0bbt10=$60,<et0aat117=$90,<z4x5x4t217=$115
mpya $115,$60,$90,$115

# qhasm: int32323232 z4x5x4t218 += (t4t0bbt14  & 0xffff) * (et0aat114 & 0xffff)
# asm 1: mpya >z4x5x4t218=vec128#114,<t4t0bbt14=vec128#61,<et0aat114=vec128#83,<z4x5x4t218=vec128#114
# asm 2: mpya >z4x5x4t218=$116,<t4t0bbt14=$63,<et0aat114=$85,<z4x5x4t218=$116
mpya $116,$63,$85,$116

# qhasm: int32323232 z4x5x4t219 += (t4t0bbt15  & 0xffff) * (et0aat114 & 0xffff)
# asm 1: mpya >z4x5x4t219=vec128#115,<t4t0bbt15=vec128#65,<et0aat114=vec128#83,<z4x5x4t219=vec128#115
# asm 2: mpya >z4x5x4t219=$117,<t4t0bbt15=$67,<et0aat114=$85,<z4x5x4t219=$117
mpya $117,$67,$85,$117

# qhasm: int32323232 z4x5x4t220 += (t4t0bbt13  & 0xffff) * (et0aat117 & 0xffff)
# asm 1: mpya >z4x5x4t220=vec128#116,<t4t0bbt13=vec128#66,<et0aat117=vec128#88,<z4x5x4t220=vec128#116
# asm 2: mpya >z4x5x4t220=$118,<t4t0bbt13=$68,<et0aat117=$90,<z4x5x4t220=$118
mpya $118,$68,$90,$118

# qhasm: int32323232 z4x5x4t221 += (t4t0bbt13  & 0xffff) * (et0aat118 & 0xffff)
# asm 1: mpya >z4x5x4t221=vec128#5,<t4t0bbt13=vec128#66,<et0aat118=vec128#87,<z4x5x4t221=vec128#5
# asm 2: mpya >z4x5x4t221=$7,<t4t0bbt13=$68,<et0aat118=$89,<z4x5x4t221=$7
mpya $7,$68,$89,$7

# qhasm: int32323232 z4x5x4t215 += (t4t0bbt114 & 0xffff) * (et0aat11  & 0xffff)
# asm 1: mpya >z4x5x4t215=vec128#111,<t4t0bbt114=vec128#57,<et0aat11=vec128#101,<z4x5x4t215=vec128#111
# asm 2: mpya >z4x5x4t215=$113,<t4t0bbt114=$59,<et0aat11=$103,<z4x5x4t215=$113
mpya $113,$59,$103,$113

# qhasm: int32323232 z4x5x4t216 <<= 1
# asm 1: shli >z4x5x4t216=vec128#112,<z4x5x4t216=vec128#112,1
# asm 2: shli >z4x5x4t216=$114,<z4x5x4t216=$114,1
shli $114,$114,1

# qhasm: int32323232 z4x5x4t217 += (t4t0bbt11  & 0xffff) * (et0aat116 & 0xffff)
# asm 1: mpya >z4x5x4t217=vec128#63,<t4t0bbt11=vec128#63,<et0aat116=vec128#85,<z4x5x4t217=vec128#113
# asm 2: mpya >z4x5x4t217=$65,<t4t0bbt11=$65,<et0aat116=$87,<z4x5x4t217=$115
mpya $65,$65,$87,$115

# qhasm: int32323232 z4x5x4t218 += (t4t0bbt15  & 0xffff) * (et0aat113 & 0xffff)
# asm 1: mpya >z4x5x4t218=vec128#113,<t4t0bbt15=vec128#65,<et0aat113=vec128#84,<z4x5x4t218=vec128#114
# asm 2: mpya >z4x5x4t218=$115,<t4t0bbt15=$67,<et0aat113=$86,<z4x5x4t218=$116
mpya $115,$67,$86,$116

# qhasm: int32323232 z4x5x4t219 += (t4t0bbt16  & 0xffff) * (et0aat113 & 0xffff)
# asm 1: mpya >z4x5x4t219=vec128#114,<t4t0bbt16=vec128#55,<et0aat113=vec128#84,<z4x5x4t219=vec128#115
# asm 2: mpya >z4x5x4t219=$116,<t4t0bbt16=$57,<et0aat113=$86,<z4x5x4t219=$117
mpya $116,$57,$86,$117

# qhasm: int32323232 z4x5x4t220 += (t4t0bbt15  & 0xffff) * (et0aat115 & 0xffff)
# asm 1: mpya >z4x5x4t220=vec128#115,<t4t0bbt15=vec128#65,<et0aat115=vec128#103,<z4x5x4t220=vec128#116
# asm 2: mpya >z4x5x4t220=$117,<t4t0bbt15=$67,<et0aat115=$105,<z4x5x4t220=$118
mpya $117,$67,$105,$118

# qhasm: int32323232 z4x5x4t221 += (t4t0bbt16  & 0xffff) * (et0aat115 & 0xffff)
# asm 1: mpya >z4x5x4t221=vec128#5,<t4t0bbt16=vec128#55,<et0aat115=vec128#103,<z4x5x4t221=vec128#5
# asm 2: mpya >z4x5x4t221=$7,<t4t0bbt16=$57,<et0aat115=$105,<z4x5x4t221=$7
mpya $7,$57,$105,$7

# qhasm: int32323232 z4x5x4t215 += (t4t0bbt115 & 0xffff) * (et0aat10  & 0xffff)
# asm 1: mpya >z4x5x4t215=vec128#111,<t4t0bbt115=vec128#72,<et0aat10=vec128#100,<z4x5x4t215=vec128#111
# asm 2: mpya >z4x5x4t215=$113,<t4t0bbt115=$74,<et0aat10=$102,<z4x5x4t215=$113
mpya $113,$74,$102,$113

# qhasm: int32323232 z4x5x4t216 += (t4t0bbt10  & 0xffff) * (et0aat116 & 0xffff)
# asm 1: mpya >z4x5x4t216=vec128#58,<t4t0bbt10=vec128#58,<et0aat116=vec128#85,<z4x5x4t216=vec128#112
# asm 2: mpya >z4x5x4t216=$60,<t4t0bbt10=$60,<et0aat116=$87,<z4x5x4t216=$114
mpya $60,$60,$87,$114

# qhasm: int32323232 z4x5x4t217 += (t4t0bbt14  & 0xffff) * (et0aat113 & 0xffff)
# asm 1: mpya >z4x5x4t217=vec128#63,<t4t0bbt14=vec128#61,<et0aat113=vec128#84,<z4x5x4t217=vec128#63
# asm 2: mpya >z4x5x4t217=$65,<t4t0bbt14=$63,<et0aat113=$86,<z4x5x4t217=$65
mpya $65,$63,$86,$65

# qhasm: int32323232 z4x5x4t218 += (t4t0bbt16  & 0xffff) * (et0aat112 & 0xffff)
# asm 1: mpya >z4x5x4t218=vec128#112,<t4t0bbt16=vec128#55,<et0aat112=vec128#81,<z4x5x4t218=vec128#113
# asm 2: mpya >z4x5x4t218=$114,<t4t0bbt16=$57,<et0aat112=$83,<z4x5x4t218=$115
mpya $114,$57,$83,$115

# qhasm: int32323232 z4x5x4t219 += (t4t0bbt17  & 0xffff) * (et0aat112 & 0xffff)
# asm 1: mpya >z4x5x4t219=vec128#113,<t4t0bbt17=vec128#67,<et0aat112=vec128#81,<z4x5x4t219=vec128#114
# asm 2: mpya >z4x5x4t219=$115,<t4t0bbt17=$69,<et0aat112=$83,<z4x5x4t219=$116
mpya $115,$69,$83,$116

# qhasm: int32323232 z4x5x4t220 += (t4t0bbt16  & 0xffff) * (et0aat114 & 0xffff)
# asm 1: mpya >z4x5x4t220=vec128#114,<t4t0bbt16=vec128#55,<et0aat114=vec128#83,<z4x5x4t220=vec128#115
# asm 2: mpya >z4x5x4t220=$116,<t4t0bbt16=$57,<et0aat114=$85,<z4x5x4t220=$117
mpya $116,$57,$85,$117

# qhasm: int32323232 z4x5x4t221 += (t4t0bbt17  & 0xffff) * (et0aat114 & 0xffff)
# asm 1: mpya >z4x5x4t221=vec128#5,<t4t0bbt17=vec128#67,<et0aat114=vec128#83,<z4x5x4t221=vec128#5
# asm 2: mpya >z4x5x4t221=$7,<t4t0bbt17=$69,<et0aat114=$85,<z4x5x4t221=$7
mpya $7,$69,$85,$7

# qhasm: int32323232 z4x5x4t222  = (t4t0bbt13  & 0xffff) * (et0aat119 & 0xffff)
# asm 1: mpy >z4x5x4t222=vec128#66,<t4t0bbt13=vec128#66,<et0aat119=vec128#91
# asm 2: mpy >z4x5x4t222=$68,<t4t0bbt13=$68,<et0aat119=$93
mpy $68,$68,$93

# qhasm: int32323232 z4x5x4t216 += (t4t0bbt14  & 0xffff) * (et0aat112 & 0xffff)
# asm 1: mpya >z4x5x4t216=vec128#58,<t4t0bbt14=vec128#61,<et0aat112=vec128#81,<z4x5x4t216=vec128#58
# asm 2: mpya >z4x5x4t216=$60,<t4t0bbt14=$63,<et0aat112=$83,<z4x5x4t216=$60
mpya $60,$63,$83,$60

# qhasm: int32323232 z4x5x4t217 += (t4t0bbt15  & 0xffff) * (et0aat112 & 0xffff)
# asm 1: mpya >z4x5x4t217=vec128#63,<t4t0bbt15=vec128#65,<et0aat112=vec128#81,<z4x5x4t217=vec128#63
# asm 2: mpya >z4x5x4t217=$65,<t4t0bbt15=$67,<et0aat112=$83,<z4x5x4t217=$65
mpya $65,$67,$83,$65

# qhasm: int32323232 z4x5x4t218 += (t4t0bbt18  & 0xffff) * (et0aat110 & 0xffff)
# asm 1: mpya >z4x5x4t218=vec128#112,<t4t0bbt18=vec128#62,<et0aat110=vec128#94,<z4x5x4t218=vec128#112
# asm 2: mpya >z4x5x4t218=$114,<t4t0bbt18=$64,<et0aat110=$96,<z4x5x4t218=$114
mpya $114,$64,$96,$114

# qhasm: int32323232 z4x5x4t219 += (t4t0bbt18  & 0xffff) * (et0aat111 & 0xffff)
# asm 1: mpya >z4x5x4t219=vec128#113,<t4t0bbt18=vec128#62,<et0aat111=vec128#95,<z4x5x4t219=vec128#113
# asm 2: mpya >z4x5x4t219=$115,<t4t0bbt18=$64,<et0aat111=$97,<z4x5x4t219=$115
mpya $115,$64,$97,$115

# qhasm: int32323232 z4x5x4t220 += (t4t0bbt17  & 0xffff) * (et0aat113 & 0xffff)
# asm 1: mpya >z4x5x4t220=vec128#114,<t4t0bbt17=vec128#67,<et0aat113=vec128#84,<z4x5x4t220=vec128#114
# asm 2: mpya >z4x5x4t220=$116,<t4t0bbt17=$69,<et0aat113=$86,<z4x5x4t220=$116
mpya $116,$69,$86,$116

# qhasm: int32323232 z4x5x4t221 += (t4t0bbt110 & 0xffff) * (et0aat111 & 0xffff)
# asm 1: mpya >z4x5x4t221=vec128#5,<t4t0bbt110=vec128#56,<et0aat111=vec128#95,<z4x5x4t221=vec128#5
# asm 2: mpya >z4x5x4t221=$7,<t4t0bbt110=$58,<et0aat111=$97,<z4x5x4t221=$7
mpya $7,$58,$97,$7

# qhasm: int32323232 z4x5x4t222 += (t4t0bbt17  & 0xffff) * (et0aat115 & 0xffff)
# asm 1: mpya >z4x5x4t222=vec128#66,<t4t0bbt17=vec128#67,<et0aat115=vec128#103,<z4x5x4t222=vec128#66
# asm 2: mpya >z4x5x4t222=$68,<t4t0bbt17=$69,<et0aat115=$105,<z4x5x4t222=$68
mpya $68,$69,$105,$68

# qhasm: int32323232 z4x5x4t216 += (t4t0bbt18  & 0xffff) * (et0aat18  & 0xffff)
# asm 1: mpya >z4x5x4t216=vec128#58,<t4t0bbt18=vec128#62,<et0aat18=vec128#99,<z4x5x4t216=vec128#58
# asm 2: mpya >z4x5x4t216=$60,<t4t0bbt18=$64,<et0aat18=$101,<z4x5x4t216=$60
mpya $60,$64,$101,$60

# qhasm: int32323232 z4x5x4t217 += (t4t0bbt18  & 0xffff) * (et0aat19  & 0xffff)
# asm 1: mpya >z4x5x4t217=vec128#63,<t4t0bbt18=vec128#62,<et0aat19=vec128#93,<z4x5x4t217=vec128#63
# asm 2: mpya >z4x5x4t217=$65,<t4t0bbt18=$64,<et0aat19=$95,<z4x5x4t217=$65
mpya $65,$64,$95,$65

# qhasm: int32323232 z4x5x4t218 += (t4t0bbt19  & 0xffff) * (et0aat19  & 0xffff)
# asm 1: mpya >z4x5x4t218=vec128#112,<t4t0bbt19=vec128#64,<et0aat19=vec128#93,<z4x5x4t218=vec128#112
# asm 2: mpya >z4x5x4t218=$114,<t4t0bbt19=$66,<et0aat19=$95,<z4x5x4t218=$114
mpya $114,$66,$95,$114

# qhasm: int32323232 z4x5x4t219 += (t4t0bbt19  & 0xffff) * (et0aat110 & 0xffff)
# asm 1: mpya >z4x5x4t219=vec128#113,<t4t0bbt19=vec128#64,<et0aat110=vec128#94,<z4x5x4t219=vec128#113
# asm 2: mpya >z4x5x4t219=$115,<t4t0bbt19=$66,<et0aat110=$96,<z4x5x4t219=$115
mpya $115,$66,$96,$115

# qhasm: int32323232 z4x5x4t220 += (t4t0bbt19  & 0xffff) * (et0aat111 & 0xffff)
# asm 1: mpya >z4x5x4t220=vec128#114,<t4t0bbt19=vec128#64,<et0aat111=vec128#95,<z4x5x4t220=vec128#114
# asm 2: mpya >z4x5x4t220=$116,<t4t0bbt19=$66,<et0aat111=$97,<z4x5x4t220=$116
mpya $116,$66,$97,$116

# qhasm: int32323232 z4x5x4t221 += (t4t0bbt111 & 0xffff) * (et0aat110 & 0xffff)
# asm 1: mpya >z4x5x4t221=vec128#5,<t4t0bbt111=vec128#68,<et0aat110=vec128#94,<z4x5x4t221=vec128#5
# asm 2: mpya >z4x5x4t221=$7,<t4t0bbt111=$70,<et0aat110=$96,<z4x5x4t221=$7
mpya $7,$70,$96,$7

# qhasm: int32323232 z4x5x4t222 += (t4t0bbt111 & 0xffff) * (et0aat111 & 0xffff)
# asm 1: mpya >z4x5x4t222=vec128#66,<t4t0bbt111=vec128#68,<et0aat111=vec128#95,<z4x5x4t222=vec128#66
# asm 2: mpya >z4x5x4t222=$68,<t4t0bbt111=$70,<et0aat111=$97,<z4x5x4t222=$68
mpya $68,$70,$97,$68

# qhasm: int32323232 z4x5x4t216 += (t4t0bbt112 & 0xffff) * (et0aat14  & 0xffff)
# asm 1: mpya >z4x5x4t216=vec128#58,<t4t0bbt112=vec128#71,<et0aat14=vec128#98,<z4x5x4t216=vec128#58
# asm 2: mpya >z4x5x4t216=$60,<t4t0bbt112=$73,<et0aat14=$100,<z4x5x4t216=$60
mpya $60,$73,$100,$60

# qhasm: int32323232 z4x5x4t217 += (t4t0bbt19  & 0xffff) * (et0aat18  & 0xffff)
# asm 1: mpya >z4x5x4t217=vec128#63,<t4t0bbt19=vec128#64,<et0aat18=vec128#99,<z4x5x4t217=vec128#63
# asm 2: mpya >z4x5x4t217=$65,<t4t0bbt19=$66,<et0aat18=$101,<z4x5x4t217=$65
mpya $65,$66,$101,$65

# qhasm: int32323232 z4x5x4t218 += (t4t0bbt110 & 0xffff) * (et0aat18  & 0xffff)
# asm 1: mpya >z4x5x4t218=vec128#112,<t4t0bbt110=vec128#56,<et0aat18=vec128#99,<z4x5x4t218=vec128#112
# asm 2: mpya >z4x5x4t218=$114,<t4t0bbt110=$58,<et0aat18=$101,<z4x5x4t218=$114
mpya $114,$58,$101,$114

# qhasm: int32323232 z4x5x4t219 += (t4t0bbt110 & 0xffff) * (et0aat19  & 0xffff)
# asm 1: mpya >z4x5x4t219=vec128#113,<t4t0bbt110=vec128#56,<et0aat19=vec128#93,<z4x5x4t219=vec128#113
# asm 2: mpya >z4x5x4t219=$115,<t4t0bbt110=$58,<et0aat19=$95,<z4x5x4t219=$115
mpya $115,$58,$95,$115

# qhasm: int32323232 z4x5x4t220 += (t4t0bbt110 & 0xffff) * (et0aat110 & 0xffff)
# asm 1: mpya >z4x5x4t220=vec128#114,<t4t0bbt110=vec128#56,<et0aat110=vec128#94,<z4x5x4t220=vec128#114
# asm 2: mpya >z4x5x4t220=$116,<t4t0bbt110=$58,<et0aat110=$96,<z4x5x4t220=$116
mpya $116,$58,$96,$116

# qhasm: int32323232 z4x5x4t221 += (t4t0bbt114 & 0xffff) * (et0aat17  & 0xffff)
# asm 1: mpya >z4x5x4t221=vec128#5,<t4t0bbt114=vec128#57,<et0aat17=vec128#89,<z4x5x4t221=vec128#5
# asm 2: mpya >z4x5x4t221=$7,<t4t0bbt114=$59,<et0aat17=$91,<z4x5x4t221=$7
mpya $7,$59,$91,$7

# qhasm: int32323232 z4x5x4t222 += (t4t0bbt115 & 0xffff) * (et0aat17  & 0xffff)
# asm 1: mpya >z4x5x4t222=vec128#66,<t4t0bbt115=vec128#72,<et0aat17=vec128#89,<z4x5x4t222=vec128#66
# asm 2: mpya >z4x5x4t222=$68,<t4t0bbt115=$74,<et0aat17=$91,<z4x5x4t222=$68
mpya $68,$74,$91,$68

# qhasm: int32323232 z4x5x4t216 += (t4t0bbt116 & 0xffff) * (et0aat10  & 0xffff)
# asm 1: mpya >z4x5x4t216=vec128#58,<t4t0bbt116=vec128#69,<et0aat10=vec128#100,<z4x5x4t216=vec128#58
# asm 2: mpya >z4x5x4t216=$60,<t4t0bbt116=$71,<et0aat10=$102,<z4x5x4t216=$60
mpya $60,$71,$102,$60

# qhasm: int32323232 z4x5x4t217 += (t4t0bbt112 & 0xffff) * (et0aat15  & 0xffff)
# asm 1: mpya >z4x5x4t217=vec128#63,<t4t0bbt112=vec128#71,<et0aat15=vec128#92,<z4x5x4t217=vec128#63
# asm 2: mpya >z4x5x4t217=$65,<t4t0bbt112=$73,<et0aat15=$94,<z4x5x4t217=$65
mpya $65,$73,$94,$65

# qhasm: int32323232 z4x5x4t218 += (t4t0bbt112 & 0xffff) * (et0aat16  & 0xffff)
# asm 1: mpya >z4x5x4t218=vec128#112,<t4t0bbt112=vec128#71,<et0aat16=vec128#90,<z4x5x4t218=vec128#112
# asm 2: mpya >z4x5x4t218=$114,<t4t0bbt112=$73,<et0aat16=$92,<z4x5x4t218=$114
mpya $114,$73,$92,$114

# qhasm: int32323232 z4x5x4t219 += (t4t0bbt111 & 0xffff) * (et0aat18  & 0xffff)
# asm 1: mpya >z4x5x4t219=vec128#113,<t4t0bbt111=vec128#68,<et0aat18=vec128#99,<z4x5x4t219=vec128#113
# asm 2: mpya >z4x5x4t219=$115,<t4t0bbt111=$70,<et0aat18=$101,<z4x5x4t219=$115
mpya $115,$70,$101,$115

# qhasm: int32323232 z4x5x4t220 += (t4t0bbt111 & 0xffff) * (et0aat19  & 0xffff)
# asm 1: mpya >z4x5x4t220=vec128#114,<t4t0bbt111=vec128#68,<et0aat19=vec128#93,<z4x5x4t220=vec128#114
# asm 2: mpya >z4x5x4t220=$116,<t4t0bbt111=$70,<et0aat19=$95,<z4x5x4t220=$116
mpya $116,$70,$95,$116

# qhasm: int32323232 z4x5x4t221 += (t4t0bbt115 & 0xffff) * (et0aat16  & 0xffff)
# asm 1: mpya >z4x5x4t221=vec128#5,<t4t0bbt115=vec128#72,<et0aat16=vec128#90,<z4x5x4t221=vec128#5
# asm 2: mpya >z4x5x4t221=$7,<t4t0bbt115=$74,<et0aat16=$92,<z4x5x4t221=$7
mpya $7,$74,$92,$7

# qhasm: int32323232 z4x5x4t222 += (t4t0bbt119 & 0xffff) * (et0aat13  & 0xffff)
# asm 1: mpya >z4x5x4t222=vec128#66,<t4t0bbt119=vec128#73,<et0aat13=vec128#97,<z4x5x4t222=vec128#66
# asm 2: mpya >z4x5x4t222=$68,<t4t0bbt119=$75,<et0aat13=$99,<z4x5x4t222=$68
mpya $68,$75,$99,$68

# qhasm: int32323232 z4x5x4t223  = (t4t0bbt14  & 0xffff) * (et0aat119 & 0xffff)
# asm 1: mpy >z4x5x4t223=vec128#115,<t4t0bbt14=vec128#61,<et0aat119=vec128#91
# asm 2: mpy >z4x5x4t223=$117,<t4t0bbt14=$63,<et0aat119=$93
mpy $117,$63,$93

# qhasm: int32323232 z4x5x4t217 += (t4t0bbt113 & 0xffff) * (et0aat14  & 0xffff)
# asm 1: mpya >z4x5x4t217=vec128#63,<t4t0bbt113=vec128#60,<et0aat14=vec128#98,<z4x5x4t217=vec128#63
# asm 2: mpya >z4x5x4t217=$65,<t4t0bbt113=$62,<et0aat14=$100,<z4x5x4t217=$65
mpya $65,$62,$100,$65

# qhasm: int32323232 z4x5x4t218 += (t4t0bbt113 & 0xffff) * (et0aat15  & 0xffff)
# asm 1: mpya >z4x5x4t218=vec128#112,<t4t0bbt113=vec128#60,<et0aat15=vec128#92,<z4x5x4t218=vec128#112
# asm 2: mpya >z4x5x4t218=$114,<t4t0bbt113=$62,<et0aat15=$94,<z4x5x4t218=$114
mpya $114,$62,$94,$114

# qhasm: int32323232 z4x5x4t219 += (t4t0bbt112 & 0xffff) * (et0aat17  & 0xffff)
# asm 1: mpya >z4x5x4t219=vec128#113,<t4t0bbt112=vec128#71,<et0aat17=vec128#89,<z4x5x4t219=vec128#113
# asm 2: mpya >z4x5x4t219=$115,<t4t0bbt112=$73,<et0aat17=$91,<z4x5x4t219=$115
mpya $115,$73,$91,$115

# qhasm: int32323232 z4x5x4t220 += (t4t0bbt113 & 0xffff) * (et0aat17  & 0xffff)
# asm 1: mpya >z4x5x4t220=vec128#114,<t4t0bbt113=vec128#60,<et0aat17=vec128#89,<z4x5x4t220=vec128#114
# asm 2: mpya >z4x5x4t220=$116,<t4t0bbt113=$62,<et0aat17=$91,<z4x5x4t220=$116
mpya $116,$62,$91,$116

# qhasm: int32323232 z4x5x4t221 += (t4t0bbt118 & 0xffff) * (et0aat13  & 0xffff)
# asm 1: mpya >z4x5x4t221=vec128#5,<t4t0bbt118=vec128#59,<et0aat13=vec128#97,<z4x5x4t221=vec128#5
# asm 2: mpya >z4x5x4t221=$7,<t4t0bbt118=$61,<et0aat13=$99,<z4x5x4t221=$7
mpya $7,$61,$99,$7

# qhasm: int32323232 z4x5x4t222 <<= 1
# asm 1: shli >z4x5x4t222=vec128#66,<z4x5x4t222=vec128#66,1
# asm 2: shli >z4x5x4t222=$68,<z4x5x4t222=$68,1
shli $68,$68,1

# qhasm: int32323232 z4x5x4t223 += (t4t0bbt15  & 0xffff) * (et0aat118 & 0xffff)
# asm 1: mpya >z4x5x4t223=vec128#115,<t4t0bbt15=vec128#65,<et0aat118=vec128#87,<z4x5x4t223=vec128#115
# asm 2: mpya >z4x5x4t223=$117,<t4t0bbt15=$67,<et0aat118=$89,<z4x5x4t223=$117
mpya $117,$67,$89,$117

# qhasm: int32323232 z4x5x4t217 += (t4t0bbt116 & 0xffff) * (et0aat11  & 0xffff)
# asm 1: mpya >z4x5x4t217=vec128#63,<t4t0bbt116=vec128#69,<et0aat11=vec128#101,<z4x5x4t217=vec128#63
# asm 2: mpya >z4x5x4t217=$65,<t4t0bbt116=$71,<et0aat11=$103,<z4x5x4t217=$65
mpya $65,$71,$103,$65

# qhasm: int32323232 z4x5x4t218 += (t4t0bbt114 & 0xffff) * (et0aat14  & 0xffff)
# asm 1: mpya >z4x5x4t218=vec128#112,<t4t0bbt114=vec128#57,<et0aat14=vec128#98,<z4x5x4t218=vec128#112
# asm 2: mpya >z4x5x4t218=$114,<t4t0bbt114=$59,<et0aat14=$100,<z4x5x4t218=$114
mpya $114,$59,$100,$114

# qhasm: int32323232 z4x5x4t219 += (t4t0bbt113 & 0xffff) * (et0aat16  & 0xffff)
# asm 1: mpya >z4x5x4t219=vec128#113,<t4t0bbt113=vec128#60,<et0aat16=vec128#90,<z4x5x4t219=vec128#113
# asm 2: mpya >z4x5x4t219=$115,<t4t0bbt113=$62,<et0aat16=$92,<z4x5x4t219=$115
mpya $115,$62,$92,$115

# qhasm: int32323232 z4x5x4t220 += (t4t0bbt114 & 0xffff) * (et0aat16  & 0xffff)
# asm 1: mpya >z4x5x4t220=vec128#114,<t4t0bbt114=vec128#57,<et0aat16=vec128#90,<z4x5x4t220=vec128#114
# asm 2: mpya >z4x5x4t220=$116,<t4t0bbt114=$59,<et0aat16=$92,<z4x5x4t220=$116
mpya $116,$59,$92,$116

# qhasm: int32323232 z4x5x4t221 += (t4t0bbt119 & 0xffff) * (et0aat12  & 0xffff)
# asm 1: mpya >z4x5x4t221=vec128#5,<t4t0bbt119=vec128#73,<et0aat12=vec128#96,<z4x5x4t221=vec128#5
# asm 2: mpya >z4x5x4t221=$7,<t4t0bbt119=$75,<et0aat12=$98,<z4x5x4t221=$7
mpya $7,$75,$98,$7

# qhasm: int32323232 z4x5x4t222 += (t4t0bbt14  & 0xffff) * (et0aat118 & 0xffff)
# asm 1: mpya >z4x5x4t222=vec128#66,<t4t0bbt14=vec128#61,<et0aat118=vec128#87,<z4x5x4t222=vec128#66
# asm 2: mpya >z4x5x4t222=$68,<t4t0bbt14=$63,<et0aat118=$89,<z4x5x4t222=$68
mpya $68,$63,$89,$68

# qhasm: int32323232 z4x5x4t223 += (t4t0bbt16  & 0xffff) * (et0aat117 & 0xffff)
# asm 1: mpya >z4x5x4t223=vec128#115,<t4t0bbt16=vec128#55,<et0aat117=vec128#88,<z4x5x4t223=vec128#115
# asm 2: mpya >z4x5x4t223=$117,<t4t0bbt16=$57,<et0aat117=$90,<z4x5x4t223=$117
mpya $117,$57,$90,$117

# qhasm: int32323232 z4x5x4t217 += (t4t0bbt117 & 0xffff) * (et0aat10  & 0xffff)
# asm 1: mpya >z4x5x4t217=vec128#63,<t4t0bbt117=vec128#70,<et0aat10=vec128#100,<z4x5x4t217=vec128#63
# asm 2: mpya >z4x5x4t217=$65,<t4t0bbt117=$72,<et0aat10=$102,<z4x5x4t217=$65
mpya $65,$72,$102,$65

# qhasm: int32323232 z4x5x4t218 += (t4t0bbt116 & 0xffff) * (et0aat12  & 0xffff)
# asm 1: mpya >z4x5x4t218=vec128#112,<t4t0bbt116=vec128#69,<et0aat12=vec128#96,<z4x5x4t218=vec128#112
# asm 2: mpya >z4x5x4t218=$114,<t4t0bbt116=$71,<et0aat12=$98,<z4x5x4t218=$114
mpya $114,$71,$98,$114

# qhasm: int32323232 z4x5x4t219 += (t4t0bbt114 & 0xffff) * (et0aat15  & 0xffff)
# asm 1: mpya >z4x5x4t219=vec128#113,<t4t0bbt114=vec128#57,<et0aat15=vec128#92,<z4x5x4t219=vec128#113
# asm 2: mpya >z4x5x4t219=$115,<t4t0bbt114=$59,<et0aat15=$94,<z4x5x4t219=$115
mpya $115,$59,$94,$115

# qhasm: int32323232 z4x5x4t220 += (t4t0bbt115 & 0xffff) * (et0aat15  & 0xffff)
# asm 1: mpya >z4x5x4t220=vec128#114,<t4t0bbt115=vec128#72,<et0aat15=vec128#92,<z4x5x4t220=vec128#114
# asm 2: mpya >z4x5x4t220=$116,<t4t0bbt115=$74,<et0aat15=$94,<z4x5x4t220=$116
mpya $116,$74,$94,$116

# qhasm: int32323232 z4x5x4t221 <<= 1
# asm 1: shli >z4x5x4t221=vec128#5,<z4x5x4t221=vec128#5,1
# asm 2: shli >z4x5x4t221=$7,<z4x5x4t221=$7,1
shli $7,$7,1

# qhasm: int32323232 z4x5x4t222 += (t4t0bbt15  & 0xffff) * (et0aat117 & 0xffff)
# asm 1: mpya >z4x5x4t222=vec128#66,<t4t0bbt15=vec128#65,<et0aat117=vec128#88,<z4x5x4t222=vec128#66
# asm 2: mpya >z4x5x4t222=$68,<t4t0bbt15=$67,<et0aat117=$90,<z4x5x4t222=$68
mpya $68,$67,$90,$68

# qhasm: int32323232 z4x5x4t223 += (t4t0bbt17  & 0xffff) * (et0aat116 & 0xffff)
# asm 1: mpya >z4x5x4t223=vec128#115,<t4t0bbt17=vec128#67,<et0aat116=vec128#85,<z4x5x4t223=vec128#115
# asm 2: mpya >z4x5x4t223=$117,<t4t0bbt17=$69,<et0aat116=$87,<z4x5x4t223=$117
mpya $117,$69,$87,$117

# qhasm: int32323232 z4x5x4t224  = (t4t0bbt15  & 0xffff) * (et0aat119  & 0xffff)
# asm 1: mpy >z4x5x4t224=vec128#116,<t4t0bbt15=vec128#65,<et0aat119=vec128#91
# asm 2: mpy >z4x5x4t224=$118,<t4t0bbt15=$67,<et0aat119=$93
mpy $118,$67,$93

# qhasm: int32323232 z4x5x4t218 += (t4t0bbt117 & 0xffff) * (et0aat11  & 0xffff)
# asm 1: mpya >z4x5x4t218=vec128#112,<t4t0bbt117=vec128#70,<et0aat11=vec128#101,<z4x5x4t218=vec128#112
# asm 2: mpya >z4x5x4t218=$114,<t4t0bbt117=$72,<et0aat11=$103,<z4x5x4t218=$114
mpya $114,$72,$103,$114

# qhasm: int32323232 z4x5x4t219 += (t4t0bbt115 & 0xffff) * (et0aat14  & 0xffff)
# asm 1: mpya >z4x5x4t219=vec128#113,<t4t0bbt115=vec128#72,<et0aat14=vec128#98,<z4x5x4t219=vec128#113
# asm 2: mpya >z4x5x4t219=$115,<t4t0bbt115=$74,<et0aat14=$100,<z4x5x4t219=$115
mpya $115,$74,$100,$115

# qhasm: int32323232 z4x5x4t220 += (t4t0bbt117 & 0xffff) * (et0aat13  & 0xffff)
# asm 1: mpya >z4x5x4t220=vec128#114,<t4t0bbt117=vec128#70,<et0aat13=vec128#97,<z4x5x4t220=vec128#114
# asm 2: mpya >z4x5x4t220=$116,<t4t0bbt117=$72,<et0aat13=$99,<z4x5x4t220=$116
mpya $116,$72,$99,$116

# qhasm: int32323232 z4x5x4t221 += (t4t0bbt14  & 0xffff) * (et0aat117  & 0xffff)
# asm 1: mpya >z4x5x4t221=vec128#5,<t4t0bbt14=vec128#61,<et0aat117=vec128#88,<z4x5x4t221=vec128#5
# asm 2: mpya >z4x5x4t221=$7,<t4t0bbt14=$63,<et0aat117=$90,<z4x5x4t221=$7
mpya $7,$63,$90,$7

# qhasm: int32323232 z4x5x4t222 += (t4t0bbt16  & 0xffff) * (et0aat116 & 0xffff)
# asm 1: mpya >z4x5x4t222=vec128#66,<t4t0bbt16=vec128#55,<et0aat116=vec128#85,<z4x5x4t222=vec128#66
# asm 2: mpya >z4x5x4t222=$68,<t4t0bbt16=$57,<et0aat116=$87,<z4x5x4t222=$68
mpya $68,$57,$87,$68

# qhasm: int32323232 z4x5x4t223 += (t4t0bbt18  & 0xffff) * (et0aat115 & 0xffff)
# asm 1: mpya >z4x5x4t223=vec128#115,<t4t0bbt18=vec128#62,<et0aat115=vec128#103,<z4x5x4t223=vec128#115
# asm 2: mpya >z4x5x4t223=$117,<t4t0bbt18=$64,<et0aat115=$105,<z4x5x4t223=$117
mpya $117,$64,$105,$117

# qhasm: int32323232 z4x5x4t224 += (t4t0bbt16  & 0xffff) * (et0aat118  & 0xffff)
# asm 1: mpya >z4x5x4t224=vec128#116,<t4t0bbt16=vec128#55,<et0aat118=vec128#87,<z4x5x4t224=vec128#116
# asm 2: mpya >z4x5x4t224=$118,<t4t0bbt16=$57,<et0aat118=$89,<z4x5x4t224=$118
mpya $118,$57,$89,$118

# qhasm: int32323232 z4x5x4t218 += (t4t0bbt118 & 0xffff) * (et0aat10  & 0xffff)
# asm 1: mpya >z4x5x4t218=vec128#112,<t4t0bbt118=vec128#59,<et0aat10=vec128#100,<z4x5x4t218=vec128#112
# asm 2: mpya >z4x5x4t218=$114,<t4t0bbt118=$61,<et0aat10=$102,<z4x5x4t218=$114
mpya $114,$61,$102,$114

# qhasm: int32323232 z4x5x4t219 += (t4t0bbt116 & 0xffff) * (et0aat13  & 0xffff)
# asm 1: mpya >z4x5x4t219=vec128#97,<t4t0bbt116=vec128#69,<et0aat13=vec128#97,<z4x5x4t219=vec128#113
# asm 2: mpya >z4x5x4t219=$99,<t4t0bbt116=$71,<et0aat13=$99,<z4x5x4t219=$115
mpya $99,$71,$99,$115

# qhasm: int32323232 z4x5x4t220 += (t4t0bbt118 & 0xffff) * (et0aat12  & 0xffff)
# asm 1: mpya >z4x5x4t220=vec128#113,<t4t0bbt118=vec128#59,<et0aat12=vec128#96,<z4x5x4t220=vec128#114
# asm 2: mpya >z4x5x4t220=$115,<t4t0bbt118=$61,<et0aat12=$98,<z4x5x4t220=$116
mpya $115,$61,$98,$116

# qhasm: int32323232 z4x5x4t221 += (t4t0bbt15  & 0xffff) * (et0aat116  & 0xffff)
# asm 1: mpya >z4x5x4t221=vec128#5,<t4t0bbt15=vec128#65,<et0aat116=vec128#85,<z4x5x4t221=vec128#5
# asm 2: mpya >z4x5x4t221=$7,<t4t0bbt15=$67,<et0aat116=$87,<z4x5x4t221=$7
mpya $7,$67,$87,$7

# qhasm: int32323232 z4x5x4t222 += (t4t0bbt18  & 0xffff) * (et0aat114 & 0xffff)
# asm 1: mpya >z4x5x4t222=vec128#65,<t4t0bbt18=vec128#62,<et0aat114=vec128#83,<z4x5x4t222=vec128#66
# asm 2: mpya >z4x5x4t222=$67,<t4t0bbt18=$64,<et0aat114=$85,<z4x5x4t222=$68
mpya $67,$64,$85,$68

# qhasm: int32323232 z4x5x4t223 += (t4t0bbt19  & 0xffff) * (et0aat114 & 0xffff)
# asm 1: mpya >z4x5x4t223=vec128#66,<t4t0bbt19=vec128#64,<et0aat114=vec128#83,<z4x5x4t223=vec128#115
# asm 2: mpya >z4x5x4t223=$68,<t4t0bbt19=$66,<et0aat114=$85,<z4x5x4t223=$117
mpya $68,$66,$85,$117

# qhasm: int32323232 z4x5x4t224 += (t4t0bbt17  & 0xffff) * (et0aat117  & 0xffff)
# asm 1: mpya >z4x5x4t224=vec128#114,<t4t0bbt17=vec128#67,<et0aat117=vec128#88,<z4x5x4t224=vec128#116
# asm 2: mpya >z4x5x4t224=$116,<t4t0bbt17=$69,<et0aat117=$90,<z4x5x4t224=$118
mpya $116,$69,$90,$118

# qhasm: int32323232 z4x5x4t225  = (t4t0bbt16  & 0xffff) * (et0aat119 & 0xffff)
# asm 1: mpy >z4x5x4t225=vec128#55,<t4t0bbt16=vec128#55,<et0aat119=vec128#91
# asm 2: mpy >z4x5x4t225=$57,<t4t0bbt16=$57,<et0aat119=$93
mpy $57,$57,$93

# qhasm: int32323232 z4x5x4t219 += (t4t0bbt117 & 0xffff) * (et0aat12  & 0xffff)
# asm 1: mpya >z4x5x4t219=vec128#96,<t4t0bbt117=vec128#70,<et0aat12=vec128#96,<z4x5x4t219=vec128#97
# asm 2: mpya >z4x5x4t219=$98,<t4t0bbt117=$72,<et0aat12=$98,<z4x5x4t219=$99
mpya $98,$72,$98,$99

# qhasm: int32323232 z4x5x4t220 += (t4t0bbt119 & 0xffff) * (et0aat11  & 0xffff)
# asm 1: mpya >z4x5x4t220=vec128#97,<t4t0bbt119=vec128#73,<et0aat11=vec128#101,<z4x5x4t220=vec128#113
# asm 2: mpya >z4x5x4t220=$99,<t4t0bbt119=$75,<et0aat11=$103,<z4x5x4t220=$115
mpya $99,$75,$103,$115

# qhasm: int32323232 z4x5x4t221 += (t4t0bbt18  & 0xffff) * (et0aat113  & 0xffff)
# asm 1: mpya >z4x5x4t221=vec128#5,<t4t0bbt18=vec128#62,<et0aat113=vec128#84,<z4x5x4t221=vec128#5
# asm 2: mpya >z4x5x4t221=$7,<t4t0bbt18=$64,<et0aat113=$86,<z4x5x4t221=$7
mpya $7,$64,$86,$7

# qhasm: int32323232 z4x5x4t222 += (t4t0bbt19  & 0xffff) * (et0aat113 & 0xffff)
# asm 1: mpya >z4x5x4t222=vec128#65,<t4t0bbt19=vec128#64,<et0aat113=vec128#84,<z4x5x4t222=vec128#65
# asm 2: mpya >z4x5x4t222=$67,<t4t0bbt19=$66,<et0aat113=$86,<z4x5x4t222=$67
mpya $67,$66,$86,$67

# qhasm: int32323232 z4x5x4t223 += (t4t0bbt110 & 0xffff) * (et0aat113 & 0xffff)
# asm 1: mpya >z4x5x4t223=vec128#66,<t4t0bbt110=vec128#56,<et0aat113=vec128#84,<z4x5x4t223=vec128#66
# asm 2: mpya >z4x5x4t223=$68,<t4t0bbt110=$58,<et0aat113=$86,<z4x5x4t223=$68
mpya $68,$58,$86,$68

# qhasm: int32323232 z4x5x4t224 += (t4t0bbt19  & 0xffff) * (et0aat115  & 0xffff)
# asm 1: mpya >z4x5x4t224=vec128#113,<t4t0bbt19=vec128#64,<et0aat115=vec128#103,<z4x5x4t224=vec128#114
# asm 2: mpya >z4x5x4t224=$115,<t4t0bbt19=$66,<et0aat115=$105,<z4x5x4t224=$116
mpya $115,$66,$105,$116

# qhasm: int32323232 z4x5x4t225 += (t4t0bbt17  & 0xffff) * (et0aat118 & 0xffff)
# asm 1: mpya >z4x5x4t225=vec128#55,<t4t0bbt17=vec128#67,<et0aat118=vec128#87,<z4x5x4t225=vec128#55
# asm 2: mpya >z4x5x4t225=$57,<t4t0bbt17=$69,<et0aat118=$89,<z4x5x4t225=$57
mpya $57,$69,$89,$57

# qhasm: int32323232 z4x5x4t219 += (t4t0bbt118 & 0xffff) * (et0aat11  & 0xffff)
# asm 1: mpya >z4x5x4t219=vec128#96,<t4t0bbt118=vec128#59,<et0aat11=vec128#101,<z4x5x4t219=vec128#96
# asm 2: mpya >z4x5x4t219=$98,<t4t0bbt118=$61,<et0aat11=$103,<z4x5x4t219=$98
mpya $98,$61,$103,$98

# qhasm: int32323232 z4x5x4t220 <<= 1
# asm 1: shli >z4x5x4t220=vec128#97,<z4x5x4t220=vec128#97,1
# asm 2: shli >z4x5x4t220=$99,<z4x5x4t220=$99,1
shli $99,$99,1

# qhasm: int32323232 z4x5x4t221 += (t4t0bbt19  & 0xffff) * (et0aat112  & 0xffff)
# asm 1: mpya >z4x5x4t221=vec128#5,<t4t0bbt19=vec128#64,<et0aat112=vec128#81,<z4x5x4t221=vec128#5
# asm 2: mpya >z4x5x4t221=$7,<t4t0bbt19=$66,<et0aat112=$83,<z4x5x4t221=$7
mpya $7,$66,$83,$7

# qhasm: int32323232 z4x5x4t222 += (t4t0bbt110 & 0xffff) * (et0aat112 & 0xffff)
# asm 1: mpya >z4x5x4t222=vec128#65,<t4t0bbt110=vec128#56,<et0aat112=vec128#81,<z4x5x4t222=vec128#65
# asm 2: mpya >z4x5x4t222=$67,<t4t0bbt110=$58,<et0aat112=$83,<z4x5x4t222=$67
mpya $67,$58,$83,$67

# qhasm: int32323232 z4x5x4t223 += (t4t0bbt111 & 0xffff) * (et0aat112 & 0xffff)
# asm 1: mpya >z4x5x4t223=vec128#66,<t4t0bbt111=vec128#68,<et0aat112=vec128#81,<z4x5x4t223=vec128#66
# asm 2: mpya >z4x5x4t223=$68,<t4t0bbt111=$70,<et0aat112=$83,<z4x5x4t223=$68
mpya $68,$70,$83,$68

# qhasm: int32323232 z4x5x4t224 += (t4t0bbt110 & 0xffff) * (et0aat114  & 0xffff)
# asm 1: mpya >z4x5x4t224=vec128#101,<t4t0bbt110=vec128#56,<et0aat114=vec128#83,<z4x5x4t224=vec128#113
# asm 2: mpya >z4x5x4t224=$103,<t4t0bbt110=$58,<et0aat114=$85,<z4x5x4t224=$115
mpya $103,$58,$85,$115

# qhasm: int32323232 z4x5x4t225 += (t4t0bbt110 & 0xffff) * (et0aat115 & 0xffff)
# asm 1: mpya >z4x5x4t225=vec128#55,<t4t0bbt110=vec128#56,<et0aat115=vec128#103,<z4x5x4t225=vec128#55
# asm 2: mpya >z4x5x4t225=$57,<t4t0bbt110=$58,<et0aat115=$105,<z4x5x4t225=$57
mpya $57,$58,$105,$57

# qhasm: int32323232 z4x5x4t219 += (t4t0bbt119 & 0xffff) * (et0aat10  & 0xffff)
# asm 1: mpya >z4x5x4t219=vec128#96,<t4t0bbt119=vec128#73,<et0aat10=vec128#100,<z4x5x4t219=vec128#96
# asm 2: mpya >z4x5x4t219=$98,<t4t0bbt119=$75,<et0aat10=$102,<z4x5x4t219=$98
mpya $98,$75,$102,$98

# qhasm: int32323232 z4x5x4t220 += (t4t0bbt14  & 0xffff) * (et0aat116 & 0xffff)
# asm 1: mpya >z4x5x4t220=vec128#61,<t4t0bbt14=vec128#61,<et0aat116=vec128#85,<z4x5x4t220=vec128#97
# asm 2: mpya >z4x5x4t220=$63,<t4t0bbt14=$63,<et0aat116=$87,<z4x5x4t220=$99
mpya $63,$63,$87,$99

# qhasm: int32323232 z4x5x4t221 += (t4t0bbt112 & 0xffff) * (et0aat19  & 0xffff)
# asm 1: mpya >z4x5x4t221=vec128#5,<t4t0bbt112=vec128#71,<et0aat19=vec128#93,<z4x5x4t221=vec128#5
# asm 2: mpya >z4x5x4t221=$7,<t4t0bbt112=$73,<et0aat19=$95,<z4x5x4t221=$7
mpya $7,$73,$95,$7

# qhasm: int32323232 z4x5x4t222 += (t4t0bbt112 & 0xffff) * (et0aat110 & 0xffff)
# asm 1: mpya >z4x5x4t222=vec128#65,<t4t0bbt112=vec128#71,<et0aat110=vec128#94,<z4x5x4t222=vec128#65
# asm 2: mpya >z4x5x4t222=$67,<t4t0bbt112=$73,<et0aat110=$96,<z4x5x4t222=$67
mpya $67,$73,$96,$67

# qhasm: int32323232 z4x5x4t223 += (t4t0bbt112 & 0xffff) * (et0aat111 & 0xffff)
# asm 1: mpya >z4x5x4t223=vec128#66,<t4t0bbt112=vec128#71,<et0aat111=vec128#95,<z4x5x4t223=vec128#66
# asm 2: mpya >z4x5x4t223=$68,<t4t0bbt112=$73,<et0aat111=$97,<z4x5x4t223=$68
mpya $68,$73,$97,$68

# qhasm: int32323232 z4x5x4t224 += (t4t0bbt111 & 0xffff) * (et0aat113  & 0xffff)
# asm 1: mpya >z4x5x4t224=vec128#97,<t4t0bbt111=vec128#68,<et0aat113=vec128#84,<z4x5x4t224=vec128#101
# asm 2: mpya >z4x5x4t224=$99,<t4t0bbt111=$70,<et0aat113=$86,<z4x5x4t224=$103
mpya $99,$70,$86,$103

# qhasm: int32323232 z4x5x4t225 += (t4t0bbt111 & 0xffff) * (et0aat114 & 0xffff)
# asm 1: mpya >z4x5x4t225=vec128#55,<t4t0bbt111=vec128#68,<et0aat114=vec128#83,<z4x5x4t225=vec128#55
# asm 2: mpya >z4x5x4t225=$57,<t4t0bbt111=$70,<et0aat114=$85,<z4x5x4t225=$57
mpya $57,$70,$85,$57

# qhasm: int32323232 z4x5x4t226  = (t4t0bbt17  & 0xffff) * (et0aat119 & 0xffff)
# asm 1: mpy >z4x5x4t226=vec128#67,<t4t0bbt17=vec128#67,<et0aat119=vec128#91
# asm 2: mpy >z4x5x4t226=$69,<t4t0bbt17=$69,<et0aat119=$93
mpy $69,$69,$93

# qhasm: int32323232 z4x5x4t220 += (t4t0bbt18  & 0xffff) * (et0aat112 & 0xffff)
# asm 1: mpya >z4x5x4t220=vec128#61,<t4t0bbt18=vec128#62,<et0aat112=vec128#81,<z4x5x4t220=vec128#61
# asm 2: mpya >z4x5x4t220=$63,<t4t0bbt18=$64,<et0aat112=$83,<z4x5x4t220=$63
mpya $63,$64,$83,$63

# qhasm: int32323232 z4x5x4t221 += (t4t0bbt113 & 0xffff) * (et0aat18  & 0xffff)
# asm 1: mpya >z4x5x4t221=vec128#5,<t4t0bbt113=vec128#60,<et0aat18=vec128#99,<z4x5x4t221=vec128#5
# asm 2: mpya >z4x5x4t221=$7,<t4t0bbt113=$62,<et0aat18=$101,<z4x5x4t221=$7
mpya $7,$62,$101,$7

# qhasm: int32323232 z4x5x4t222 += (t4t0bbt113 & 0xffff) * (et0aat19  & 0xffff)
# asm 1: mpya >z4x5x4t222=vec128#65,<t4t0bbt113=vec128#60,<et0aat19=vec128#93,<z4x5x4t222=vec128#65
# asm 2: mpya >z4x5x4t222=$67,<t4t0bbt113=$62,<et0aat19=$95,<z4x5x4t222=$67
mpya $67,$62,$95,$67

# qhasm: int32323232 z4x5x4t223 += (t4t0bbt113 & 0xffff) * (et0aat110 & 0xffff)
# asm 1: mpya >z4x5x4t223=vec128#66,<t4t0bbt113=vec128#60,<et0aat110=vec128#94,<z4x5x4t223=vec128#66
# asm 2: mpya >z4x5x4t223=$68,<t4t0bbt113=$62,<et0aat110=$96,<z4x5x4t223=$68
mpya $68,$62,$96,$68

# qhasm: int32323232 z4x5x4t224 += (t4t0bbt113 & 0xffff) * (et0aat111  & 0xffff)
# asm 1: mpya >z4x5x4t224=vec128#97,<t4t0bbt113=vec128#60,<et0aat111=vec128#95,<z4x5x4t224=vec128#97
# asm 2: mpya >z4x5x4t224=$99,<t4t0bbt113=$62,<et0aat111=$97,<z4x5x4t224=$99
mpya $99,$62,$97,$99

# qhasm: int32323232 z4x5x4t225 += (t4t0bbt114 & 0xffff) * (et0aat111 & 0xffff)
# asm 1: mpya >z4x5x4t225=vec128#55,<t4t0bbt114=vec128#57,<et0aat111=vec128#95,<z4x5x4t225=vec128#55
# asm 2: mpya >z4x5x4t225=$57,<t4t0bbt114=$59,<et0aat111=$97,<z4x5x4t225=$57
mpya $57,$59,$97,$57

# qhasm: int32323232 z4x5x4t226 += (t4t0bbt111 & 0xffff) * (et0aat115 & 0xffff)
# asm 1: mpya >z4x5x4t226=vec128#67,<t4t0bbt111=vec128#68,<et0aat115=vec128#103,<z4x5x4t226=vec128#67
# asm 2: mpya >z4x5x4t226=$69,<t4t0bbt111=$70,<et0aat115=$105,<z4x5x4t226=$69
mpya $69,$70,$105,$69

# qhasm: int32323232 z4x5x4t220 += (t4t0bbt112 & 0xffff) * (et0aat18  & 0xffff)
# asm 1: mpya >z4x5x4t220=vec128#61,<t4t0bbt112=vec128#71,<et0aat18=vec128#99,<z4x5x4t220=vec128#61
# asm 2: mpya >z4x5x4t220=$63,<t4t0bbt112=$73,<et0aat18=$101,<z4x5x4t220=$63
mpya $63,$73,$101,$63

# qhasm: int32323232 z4x5x4t221 += (t4t0bbt116 & 0xffff) * (et0aat15  & 0xffff)
# asm 1: mpya >z4x5x4t221=vec128#5,<t4t0bbt116=vec128#69,<et0aat15=vec128#92,<z4x5x4t221=vec128#5
# asm 2: mpya >z4x5x4t221=$7,<t4t0bbt116=$71,<et0aat15=$94,<z4x5x4t221=$7
mpya $7,$71,$94,$7

# qhasm: int32323232 z4x5x4t222 += (t4t0bbt114 & 0xffff) * (et0aat18  & 0xffff)
# asm 1: mpya >z4x5x4t222=vec128#65,<t4t0bbt114=vec128#57,<et0aat18=vec128#99,<z4x5x4t222=vec128#65
# asm 2: mpya >z4x5x4t222=$67,<t4t0bbt114=$59,<et0aat18=$101,<z4x5x4t222=$67
mpya $67,$59,$101,$67

# qhasm: int32323232 z4x5x4t223 += (t4t0bbt114 & 0xffff) * (et0aat19  & 0xffff)
# asm 1: mpya >z4x5x4t223=vec128#66,<t4t0bbt114=vec128#57,<et0aat19=vec128#93,<z4x5x4t223=vec128#66
# asm 2: mpya >z4x5x4t223=$68,<t4t0bbt114=$59,<et0aat19=$95,<z4x5x4t223=$68
mpya $68,$59,$95,$68

# qhasm: int32323232 z4x5x4t224 += (t4t0bbt114 & 0xffff) * (et0aat110  & 0xffff)
# asm 1: mpya >z4x5x4t224=vec128#97,<t4t0bbt114=vec128#57,<et0aat110=vec128#94,<z4x5x4t224=vec128#97
# asm 2: mpya >z4x5x4t224=$99,<t4t0bbt114=$59,<et0aat110=$96,<z4x5x4t224=$99
mpya $99,$59,$96,$99

# qhasm: int32323232 z4x5x4t225 += (t4t0bbt115 & 0xffff) * (et0aat110 & 0xffff)
# asm 1: mpya >z4x5x4t225=vec128#55,<t4t0bbt115=vec128#72,<et0aat110=vec128#94,<z4x5x4t225=vec128#55
# asm 2: mpya >z4x5x4t225=$57,<t4t0bbt115=$74,<et0aat110=$96,<z4x5x4t225=$57
mpya $57,$74,$96,$57

# qhasm: int32323232 z4x5x4t226 += (t4t0bbt115 & 0xffff) * (et0aat111 & 0xffff)
# asm 1: mpya >z4x5x4t226=vec128#67,<t4t0bbt115=vec128#72,<et0aat111=vec128#95,<z4x5x4t226=vec128#67
# asm 2: mpya >z4x5x4t226=$69,<t4t0bbt115=$74,<et0aat111=$97,<z4x5x4t226=$69
mpya $69,$74,$97,$69

# qhasm: int32323232 z4x5x4t220 += (t4t0bbt116 & 0xffff) * (et0aat14  & 0xffff)
# asm 1: mpya >z4x5x4t220=vec128#61,<t4t0bbt116=vec128#69,<et0aat14=vec128#98,<z4x5x4t220=vec128#61
# asm 2: mpya >z4x5x4t220=$63,<t4t0bbt116=$71,<et0aat14=$100,<z4x5x4t220=$63
mpya $63,$71,$100,$63

# qhasm: int32323232 z4x5x4t221 += (t4t0bbt117 & 0xffff) * (et0aat14  & 0xffff)
# asm 1: mpya >z4x5x4t221=vec128#5,<t4t0bbt117=vec128#70,<et0aat14=vec128#98,<z4x5x4t221=vec128#5
# asm 2: mpya >z4x5x4t221=$7,<t4t0bbt117=$72,<et0aat14=$100,<z4x5x4t221=$7
mpya $7,$72,$100,$7

# qhasm: int32323232 z4x5x4t222 += (t4t0bbt116 & 0xffff) * (et0aat16  & 0xffff)
# asm 1: mpya >z4x5x4t222=vec128#65,<t4t0bbt116=vec128#69,<et0aat16=vec128#90,<z4x5x4t222=vec128#65
# asm 2: mpya >z4x5x4t222=$67,<t4t0bbt116=$71,<et0aat16=$92,<z4x5x4t222=$67
mpya $67,$71,$92,$67

# qhasm: int32323232 z4x5x4t223 += (t4t0bbt115 & 0xffff) * (et0aat18  & 0xffff)
# asm 1: mpya >z4x5x4t223=vec128#66,<t4t0bbt115=vec128#72,<et0aat18=vec128#99,<z4x5x4t223=vec128#66
# asm 2: mpya >z4x5x4t223=$68,<t4t0bbt115=$74,<et0aat18=$101,<z4x5x4t223=$68
mpya $68,$74,$101,$68

# qhasm: int32323232 z4x5x4t224 += (t4t0bbt115 & 0xffff) * (et0aat19  & 0xffff)
# asm 1: mpya >z4x5x4t224=vec128#97,<t4t0bbt115=vec128#72,<et0aat19=vec128#93,<z4x5x4t224=vec128#97
# asm 2: mpya >z4x5x4t224=$99,<t4t0bbt115=$74,<et0aat19=$95,<z4x5x4t224=$99
mpya $99,$74,$95,$99

# qhasm: int32323232 z4x5x4t225 += (t4t0bbt118 & 0xffff) * (et0aat17  & 0xffff)
# asm 1: mpya >z4x5x4t225=vec128#55,<t4t0bbt118=vec128#59,<et0aat17=vec128#89,<z4x5x4t225=vec128#55
# asm 2: mpya >z4x5x4t225=$57,<t4t0bbt118=$61,<et0aat17=$91,<z4x5x4t225=$57
mpya $57,$61,$91,$57

# qhasm: int32323232 z4x5x4t226 += (t4t0bbt119 & 0xffff) * (et0aat17  & 0xffff)
# asm 1: mpya >z4x5x4t226=vec128#67,<t4t0bbt119=vec128#73,<et0aat17=vec128#89,<z4x5x4t226=vec128#67
# asm 2: mpya >z4x5x4t226=$69,<t4t0bbt119=$75,<et0aat17=$91,<z4x5x4t226=$69
mpya $69,$75,$91,$69

# qhasm: int32323232 z4x5x4t227  = (t4t0bbt18  & 0xffff) * (et0aat119 & 0xffff)
# asm 1: mpy >z4x5x4t227=vec128#100,<t4t0bbt18=vec128#62,<et0aat119=vec128#91
# asm 2: mpy >z4x5x4t227=$102,<t4t0bbt18=$64,<et0aat119=$93
mpy $102,$64,$93

# qhasm: int32323232 z4x5x4t228  = (t4t0bbt19  & 0xffff) * (et0aat119 & 0xffff)
# asm 1: mpy >z4x5x4t228=vec128#101,<t4t0bbt19=vec128#64,<et0aat119=vec128#91
# asm 2: mpy >z4x5x4t228=$103,<t4t0bbt19=$66,<et0aat119=$93
mpy $103,$66,$93

# qhasm: int32323232 z4x5x4t222 += (t4t0bbt117 & 0xffff) * (et0aat15  & 0xffff)
# asm 1: mpya >z4x5x4t222=vec128#65,<t4t0bbt117=vec128#70,<et0aat15=vec128#92,<z4x5x4t222=vec128#65
# asm 2: mpya >z4x5x4t222=$67,<t4t0bbt117=$72,<et0aat15=$94,<z4x5x4t222=$67
mpya $67,$72,$94,$67

# qhasm: int32323232 z4x5x4t223 += (t4t0bbt116 & 0xffff) * (et0aat17  & 0xffff)
# asm 1: mpya >z4x5x4t223=vec128#66,<t4t0bbt116=vec128#69,<et0aat17=vec128#89,<z4x5x4t223=vec128#66
# asm 2: mpya >z4x5x4t223=$68,<t4t0bbt116=$71,<et0aat17=$91,<z4x5x4t223=$68
mpya $68,$71,$91,$68

# qhasm: int32323232 z4x5x4t224 += (t4t0bbt117 & 0xffff) * (et0aat17  & 0xffff)
# asm 1: mpya >z4x5x4t224=vec128#89,<t4t0bbt117=vec128#70,<et0aat17=vec128#89,<z4x5x4t224=vec128#97
# asm 2: mpya >z4x5x4t224=$91,<t4t0bbt117=$72,<et0aat17=$91,<z4x5x4t224=$99
mpya $91,$72,$91,$99

# qhasm: int32323232 z4x5x4t225 += (t4t0bbt119 & 0xffff) * (et0aat16  & 0xffff)
# asm 1: mpya >z4x5x4t225=vec128#55,<t4t0bbt119=vec128#73,<et0aat16=vec128#90,<z4x5x4t225=vec128#55
# asm 2: mpya >z4x5x4t225=$57,<t4t0bbt119=$75,<et0aat16=$92,<z4x5x4t225=$57
mpya $57,$75,$92,$57

# qhasm: int32323232 z4x5x4t226 <<= 1
# asm 1: shli >z4x5x4t226=vec128#67,<z4x5x4t226=vec128#67,1
# asm 2: shli >z4x5x4t226=$69,<z4x5x4t226=$69,1
shli $69,$69,1

# qhasm: int32323232 z4x5x4t227 += (t4t0bbt19  & 0xffff) * (et0aat118 & 0xffff)
# asm 1: mpya >z4x5x4t227=vec128#97,<t4t0bbt19=vec128#64,<et0aat118=vec128#87,<z4x5x4t227=vec128#100
# asm 2: mpya >z4x5x4t227=$99,<t4t0bbt19=$66,<et0aat118=$89,<z4x5x4t227=$102
mpya $99,$66,$89,$102

# qhasm: int32323232 z4x5x4t228 += (t4t0bbt110 & 0xffff) * (et0aat118 & 0xffff)
# asm 1: mpya >z4x5x4t228=vec128#100,<t4t0bbt110=vec128#56,<et0aat118=vec128#87,<z4x5x4t228=vec128#101
# asm 2: mpya >z4x5x4t228=$102,<t4t0bbt110=$58,<et0aat118=$89,<z4x5x4t228=$103
mpya $102,$58,$89,$103

# qhasm: int32323232 z4x5x4t222 += (t4t0bbt118 & 0xffff) * (et0aat14  & 0xffff)
# asm 1: mpya >z4x5x4t222=vec128#65,<t4t0bbt118=vec128#59,<et0aat14=vec128#98,<z4x5x4t222=vec128#65
# asm 2: mpya >z4x5x4t222=$67,<t4t0bbt118=$61,<et0aat14=$100,<z4x5x4t222=$67
mpya $67,$61,$100,$67

# qhasm: int32323232 z4x5x4t223 += (t4t0bbt117 & 0xffff) * (et0aat16  & 0xffff)
# asm 1: mpya >z4x5x4t223=vec128#66,<t4t0bbt117=vec128#70,<et0aat16=vec128#90,<z4x5x4t223=vec128#66
# asm 2: mpya >z4x5x4t223=$68,<t4t0bbt117=$72,<et0aat16=$92,<z4x5x4t223=$68
mpya $68,$72,$92,$68

# qhasm: int32323232 z4x5x4t224 += (t4t0bbt118 & 0xffff) * (et0aat16  & 0xffff)
# asm 1: mpya >z4x5x4t224=vec128#89,<t4t0bbt118=vec128#59,<et0aat16=vec128#90,<z4x5x4t224=vec128#89
# asm 2: mpya >z4x5x4t224=$91,<t4t0bbt118=$61,<et0aat16=$92,<z4x5x4t224=$91
mpya $91,$61,$92,$91

# qhasm: int32323232 z4x5x4t225 <<= 1
# asm 1: shli >z4x5x4t225=vec128#55,<z4x5x4t225=vec128#55,1
# asm 2: shli >z4x5x4t225=$57,<z4x5x4t225=$57,1
shli $57,$57,1

# qhasm: int32323232 z4x5x4t226 += (t4t0bbt18  & 0xffff) * (et0aat118 & 0xffff)
# asm 1: mpya >z4x5x4t226=vec128#67,<t4t0bbt18=vec128#62,<et0aat118=vec128#87,<z4x5x4t226=vec128#67
# asm 2: mpya >z4x5x4t226=$69,<t4t0bbt18=$64,<et0aat118=$89,<z4x5x4t226=$69
mpya $69,$64,$89,$69

# qhasm: int32323232 z4x5x4t227 += (t4t0bbt110 & 0xffff) * (et0aat117 & 0xffff)
# asm 1: mpya >z4x5x4t227=vec128#90,<t4t0bbt110=vec128#56,<et0aat117=vec128#88,<z4x5x4t227=vec128#97
# asm 2: mpya >z4x5x4t227=$92,<t4t0bbt110=$58,<et0aat117=$90,<z4x5x4t227=$99
mpya $92,$58,$90,$99

# qhasm: int32323232 z4x5x4t228 += (t4t0bbt111 & 0xffff) * (et0aat117 & 0xffff)
# asm 1: mpya >z4x5x4t228=vec128#97,<t4t0bbt111=vec128#68,<et0aat117=vec128#88,<z4x5x4t228=vec128#100
# asm 2: mpya >z4x5x4t228=$99,<t4t0bbt111=$70,<et0aat117=$90,<z4x5x4t228=$102
mpya $99,$70,$90,$102

# qhasm: int32323232 z4x5x4t229  = (t4t0bbt110 & 0xffff) * (et0aat119 & 0xffff)
# asm 1: mpy >z4x5x4t229=vec128#100,<t4t0bbt110=vec128#56,<et0aat119=vec128#91
# asm 2: mpy >z4x5x4t229=$102,<t4t0bbt110=$58,<et0aat119=$93
mpy $102,$58,$93

# qhasm: int32323232 z4x5x4t223 += (t4t0bbt118 & 0xffff) * (et0aat15  & 0xffff)
# asm 1: mpya >z4x5x4t223=vec128#66,<t4t0bbt118=vec128#59,<et0aat15=vec128#92,<z4x5x4t223=vec128#66
# asm 2: mpya >z4x5x4t223=$68,<t4t0bbt118=$61,<et0aat15=$94,<z4x5x4t223=$68
mpya $68,$61,$94,$68

# qhasm: int32323232 z4x5x4t224 += (t4t0bbt119 & 0xffff) * (et0aat15  & 0xffff)
# asm 1: mpya >z4x5x4t224=vec128#89,<t4t0bbt119=vec128#73,<et0aat15=vec128#92,<z4x5x4t224=vec128#89
# asm 2: mpya >z4x5x4t224=$91,<t4t0bbt119=$75,<et0aat15=$94,<z4x5x4t224=$91
mpya $91,$75,$94,$91

# qhasm: int32323232 z4x5x4t225 += (t4t0bbt18  & 0xffff) * (et0aat117 & 0xffff)
# asm 1: mpya >z4x5x4t225=vec128#55,<t4t0bbt18=vec128#62,<et0aat117=vec128#88,<z4x5x4t225=vec128#55
# asm 2: mpya >z4x5x4t225=$57,<t4t0bbt18=$64,<et0aat117=$90,<z4x5x4t225=$57
mpya $57,$64,$90,$57

# qhasm: int32323232 z4x5x4t226 += (t4t0bbt19  & 0xffff) * (et0aat117 & 0xffff)
# asm 1: mpya >z4x5x4t226=vec128#67,<t4t0bbt19=vec128#64,<et0aat117=vec128#88,<z4x5x4t226=vec128#67
# asm 2: mpya >z4x5x4t226=$69,<t4t0bbt19=$66,<et0aat117=$90,<z4x5x4t226=$69
mpya $69,$66,$90,$69

# qhasm: int32323232 z4x5x4t227 += (t4t0bbt111 & 0xffff) * (et0aat116 & 0xffff)
# asm 1: mpya >z4x5x4t227=vec128#90,<t4t0bbt111=vec128#68,<et0aat116=vec128#85,<z4x5x4t227=vec128#90
# asm 2: mpya >z4x5x4t227=$92,<t4t0bbt111=$70,<et0aat116=$87,<z4x5x4t227=$92
mpya $92,$70,$87,$92

# qhasm: int32323232 z4x5x4t228 += (t4t0bbt113 & 0xffff) * (et0aat115 & 0xffff)
# asm 1: mpya >z4x5x4t228=vec128#92,<t4t0bbt113=vec128#60,<et0aat115=vec128#103,<z4x5x4t228=vec128#97
# asm 2: mpya >z4x5x4t228=$94,<t4t0bbt113=$62,<et0aat115=$105,<z4x5x4t228=$99
mpya $94,$62,$105,$99

# qhasm: int32323232 z4x5x4t229 += (t4t0bbt111 & 0xffff) * (et0aat118 & 0xffff)
# asm 1: mpya >z4x5x4t229=vec128#97,<t4t0bbt111=vec128#68,<et0aat118=vec128#87,<z4x5x4t229=vec128#100
# asm 2: mpya >z4x5x4t229=$99,<t4t0bbt111=$70,<et0aat118=$89,<z4x5x4t229=$102
mpya $99,$70,$89,$102

# qhasm: int32323232 z4x5x4t223 += (t4t0bbt119 & 0xffff) * (et0aat14  & 0xffff)
# asm 1: mpya >z4x5x4t223=vec128#66,<t4t0bbt119=vec128#73,<et0aat14=vec128#98,<z4x5x4t223=vec128#66
# asm 2: mpya >z4x5x4t223=$68,<t4t0bbt119=$75,<et0aat14=$100,<z4x5x4t223=$68
mpya $68,$75,$100,$68

# qhasm: int32323232 z4x5x4t224 <<= 1
# asm 1: shli >z4x5x4t224=vec128#89,<z4x5x4t224=vec128#89,1
# asm 2: shli >z4x5x4t224=$91,<z4x5x4t224=$91,1
shli $91,$91,1

# qhasm: int32323232 z4x5x4t225 += (t4t0bbt19  & 0xffff) * (et0aat116 & 0xffff)
# asm 1: mpya >z4x5x4t225=vec128#55,<t4t0bbt19=vec128#64,<et0aat116=vec128#85,<z4x5x4t225=vec128#55
# asm 2: mpya >z4x5x4t225=$57,<t4t0bbt19=$66,<et0aat116=$87,<z4x5x4t225=$57
mpya $57,$66,$87,$57

# qhasm: int32323232 z4x5x4t226 += (t4t0bbt110 & 0xffff) * (et0aat116 & 0xffff)
# asm 1: mpya >z4x5x4t226=vec128#56,<t4t0bbt110=vec128#56,<et0aat116=vec128#85,<z4x5x4t226=vec128#67
# asm 2: mpya >z4x5x4t226=$58,<t4t0bbt110=$58,<et0aat116=$87,<z4x5x4t226=$69
mpya $58,$58,$87,$69

# qhasm: int32323232 z4x5x4t227 += (t4t0bbt112 & 0xffff) * (et0aat115 & 0xffff)
# asm 1: mpya >z4x5x4t227=vec128#64,<t4t0bbt112=vec128#71,<et0aat115=vec128#103,<z4x5x4t227=vec128#90
# asm 2: mpya >z4x5x4t227=$66,<t4t0bbt112=$73,<et0aat115=$105,<z4x5x4t227=$92
mpya $66,$73,$105,$92

# qhasm: int32323232 z4x5x4t228 += (t4t0bbt114 & 0xffff) * (et0aat114 & 0xffff)
# asm 1: mpya >z4x5x4t228=vec128#67,<t4t0bbt114=vec128#57,<et0aat114=vec128#83,<z4x5x4t228=vec128#92
# asm 2: mpya >z4x5x4t228=$69,<t4t0bbt114=$59,<et0aat114=$85,<z4x5x4t228=$94
mpya $69,$59,$85,$94

# qhasm: int32323232 z4x5x4t229 += (t4t0bbt114 & 0xffff) * (et0aat115 & 0xffff)
# asm 1: mpya >z4x5x4t229=vec128#90,<t4t0bbt114=vec128#57,<et0aat115=vec128#103,<z4x5x4t229=vec128#97
# asm 2: mpya >z4x5x4t229=$92,<t4t0bbt114=$59,<et0aat115=$105,<z4x5x4t229=$99
mpya $92,$59,$105,$99

# qhasm: int32323232 z4x5x4t230  = (t4t0bbt111 & 0xffff) * (et0aat119 & 0xffff)
# asm 1: mpy >z4x5x4t230=vec128#68,<t4t0bbt111=vec128#68,<et0aat119=vec128#91
# asm 2: mpy >z4x5x4t230=$70,<t4t0bbt111=$70,<et0aat119=$93
mpy $70,$70,$93

# qhasm: int32323232 z4x5x4t224 += (t4t0bbt18  & 0xffff) * (et0aat116 & 0xffff)
# asm 1: mpya >z4x5x4t224=vec128#62,<t4t0bbt18=vec128#62,<et0aat116=vec128#85,<z4x5x4t224=vec128#89
# asm 2: mpya >z4x5x4t224=$64,<t4t0bbt18=$64,<et0aat116=$87,<z4x5x4t224=$91
mpya $64,$64,$87,$91

# qhasm: int32323232 z4x5x4t225 += (t4t0bbt112 & 0xffff) * (et0aat113 & 0xffff)
# asm 1: mpya >z4x5x4t225=vec128#55,<t4t0bbt112=vec128#71,<et0aat113=vec128#84,<z4x5x4t225=vec128#55
# asm 2: mpya >z4x5x4t225=$57,<t4t0bbt112=$73,<et0aat113=$86,<z4x5x4t225=$57
mpya $57,$73,$86,$57

# qhasm: int32323232 z4x5x4t226 += (t4t0bbt112 & 0xffff) * (et0aat114 & 0xffff)
# asm 1: mpya >z4x5x4t226=vec128#56,<t4t0bbt112=vec128#71,<et0aat114=vec128#83,<z4x5x4t226=vec128#56
# asm 2: mpya >z4x5x4t226=$58,<t4t0bbt112=$73,<et0aat114=$85,<z4x5x4t226=$58
mpya $58,$73,$85,$58

# qhasm: int32323232 z4x5x4t227 += (t4t0bbt113 & 0xffff) * (et0aat114 & 0xffff)
# asm 1: mpya >z4x5x4t227=vec128#64,<t4t0bbt113=vec128#60,<et0aat114=vec128#83,<z4x5x4t227=vec128#64
# asm 2: mpya >z4x5x4t227=$66,<t4t0bbt113=$62,<et0aat114=$85,<z4x5x4t227=$66
mpya $66,$62,$85,$66

# qhasm: int32323232 z4x5x4t228 += (t4t0bbt115 & 0xffff) * (et0aat113 & 0xffff)
# asm 1: mpya >z4x5x4t228=vec128#67,<t4t0bbt115=vec128#72,<et0aat113=vec128#84,<z4x5x4t228=vec128#67
# asm 2: mpya >z4x5x4t228=$69,<t4t0bbt115=$74,<et0aat113=$86,<z4x5x4t228=$69
mpya $69,$74,$86,$69

# qhasm: int32323232 z4x5x4t229 += (t4t0bbt115 & 0xffff) * (et0aat114 & 0xffff)
# asm 1: mpya >z4x5x4t229=vec128#89,<t4t0bbt115=vec128#72,<et0aat114=vec128#83,<z4x5x4t229=vec128#90
# asm 2: mpya >z4x5x4t229=$91,<t4t0bbt115=$74,<et0aat114=$85,<z4x5x4t229=$92
mpya $91,$74,$85,$92

# qhasm: int32323232 z4x5x4t230 += (t4t0bbt115 & 0xffff) * (et0aat115 & 0xffff)
# asm 1: mpya >z4x5x4t230=vec128#68,<t4t0bbt115=vec128#72,<et0aat115=vec128#103,<z4x5x4t230=vec128#68
# asm 2: mpya >z4x5x4t230=$70,<t4t0bbt115=$74,<et0aat115=$105,<z4x5x4t230=$70
mpya $70,$74,$105,$70

# qhasm: int32323232 z4x5x4t224 += (t4t0bbt112 & 0xffff) * (et0aat112 & 0xffff)
# asm 1: mpya >z4x5x4t224=vec128#62,<t4t0bbt112=vec128#71,<et0aat112=vec128#81,<z4x5x4t224=vec128#62
# asm 2: mpya >z4x5x4t224=$64,<t4t0bbt112=$73,<et0aat112=$83,<z4x5x4t224=$64
mpya $64,$73,$83,$64

# qhasm: int32323232 z4x5x4t225 += (t4t0bbt113 & 0xffff) * (et0aat112 & 0xffff)
# asm 1: mpya >z4x5x4t225=vec128#55,<t4t0bbt113=vec128#60,<et0aat112=vec128#81,<z4x5x4t225=vec128#55
# asm 2: mpya >z4x5x4t225=$57,<t4t0bbt113=$62,<et0aat112=$83,<z4x5x4t225=$57
mpya $57,$62,$83,$57

# qhasm: int32323232 z4x5x4t226 += (t4t0bbt113 & 0xffff) * (et0aat113 & 0xffff)
# asm 1: mpya >z4x5x4t226=vec128#56,<t4t0bbt113=vec128#60,<et0aat113=vec128#84,<z4x5x4t226=vec128#56
# asm 2: mpya >z4x5x4t226=$58,<t4t0bbt113=$62,<et0aat113=$86,<z4x5x4t226=$58
mpya $58,$62,$86,$58

# qhasm: int32323232 z4x5x4t227 += (t4t0bbt114 & 0xffff) * (et0aat113 & 0xffff)
# asm 1: mpya >z4x5x4t227=vec128#64,<t4t0bbt114=vec128#57,<et0aat113=vec128#84,<z4x5x4t227=vec128#64
# asm 2: mpya >z4x5x4t227=$66,<t4t0bbt114=$59,<et0aat113=$86,<z4x5x4t227=$66
mpya $66,$59,$86,$66

# qhasm: int32323232 z4x5x4t228 += (t4t0bbt117 & 0xffff) * (et0aat111 & 0xffff)
# asm 1: mpya >z4x5x4t228=vec128#67,<t4t0bbt117=vec128#70,<et0aat111=vec128#95,<z4x5x4t228=vec128#67
# asm 2: mpya >z4x5x4t228=$69,<t4t0bbt117=$72,<et0aat111=$97,<z4x5x4t228=$69
mpya $69,$72,$97,$69

# qhasm: int32323232 z4x5x4t229 += (t4t0bbt118 & 0xffff) * (et0aat111 & 0xffff)
# asm 1: mpya >z4x5x4t229=vec128#89,<t4t0bbt118=vec128#59,<et0aat111=vec128#95,<z4x5x4t229=vec128#89
# asm 2: mpya >z4x5x4t229=$91,<t4t0bbt118=$61,<et0aat111=$97,<z4x5x4t229=$91
mpya $91,$61,$97,$91

# qhasm: int32323232 z4x5x4t230 += (t4t0bbt119 & 0xffff) * (et0aat111 & 0xffff)
# asm 1: mpya >z4x5x4t230=vec128#68,<t4t0bbt119=vec128#73,<et0aat111=vec128#95,<z4x5x4t230=vec128#68
# asm 2: mpya >z4x5x4t230=$70,<t4t0bbt119=$75,<et0aat111=$97,<z4x5x4t230=$70
mpya $70,$75,$97,$70

# qhasm: int32323232 z4x5x4t224 += (t4t0bbt116 & 0xffff) * (et0aat18  & 0xffff)
# asm 1: mpya >z4x5x4t224=vec128#62,<t4t0bbt116=vec128#69,<et0aat18=vec128#99,<z4x5x4t224=vec128#62
# asm 2: mpya >z4x5x4t224=$64,<t4t0bbt116=$71,<et0aat18=$101,<z4x5x4t224=$64
mpya $64,$71,$101,$64

# qhasm: int32323232 z4x5x4t225 += (t4t0bbt116 & 0xffff) * (et0aat19  & 0xffff)
# asm 1: mpya >z4x5x4t225=vec128#55,<t4t0bbt116=vec128#69,<et0aat19=vec128#93,<z4x5x4t225=vec128#55
# asm 2: mpya >z4x5x4t225=$57,<t4t0bbt116=$71,<et0aat19=$95,<z4x5x4t225=$57
mpya $57,$71,$95,$57

# qhasm: int32323232 z4x5x4t226 += (t4t0bbt114 & 0xffff) * (et0aat112 & 0xffff)
# asm 1: mpya >z4x5x4t226=vec128#56,<t4t0bbt114=vec128#57,<et0aat112=vec128#81,<z4x5x4t226=vec128#56
# asm 2: mpya >z4x5x4t226=$58,<t4t0bbt114=$59,<et0aat112=$83,<z4x5x4t226=$58
mpya $58,$59,$83,$58

# qhasm: int32323232 z4x5x4t227 += (t4t0bbt115 & 0xffff) * (et0aat112 & 0xffff)
# asm 1: mpya >z4x5x4t227=vec128#64,<t4t0bbt115=vec128#72,<et0aat112=vec128#81,<z4x5x4t227=vec128#64
# asm 2: mpya >z4x5x4t227=$66,<t4t0bbt115=$74,<et0aat112=$83,<z4x5x4t227=$66
mpya $66,$74,$83,$66

# qhasm: int32323232 z4x5x4t228 += (t4t0bbt118 & 0xffff) * (et0aat110 & 0xffff)
# asm 1: mpya >z4x5x4t228=vec128#67,<t4t0bbt118=vec128#59,<et0aat110=vec128#94,<z4x5x4t228=vec128#67
# asm 2: mpya >z4x5x4t228=$69,<t4t0bbt118=$61,<et0aat110=$96,<z4x5x4t228=$69
mpya $69,$61,$96,$69

# qhasm: int32323232 z4x5x4t229 += (t4t0bbt119 & 0xffff) * (et0aat110 & 0xffff)
# asm 1: mpya >z4x5x4t229=vec128#89,<t4t0bbt119=vec128#73,<et0aat110=vec128#94,<z4x5x4t229=vec128#89
# asm 2: mpya >z4x5x4t229=$91,<t4t0bbt119=$75,<et0aat110=$96,<z4x5x4t229=$91
mpya $91,$75,$96,$91

# qhasm: int32323232 z4x5x4t230 <<= 1
# asm 1: shli >z4x5x4t230=vec128#68,<z4x5x4t230=vec128#68,1
# asm 2: shli >z4x5x4t230=$70,<z4x5x4t230=$70,1
shli $70,$70,1

# qhasm: int32323232 z4x5x4t231  = (t4t0bbt112 & 0xffff) * (et0aat119 & 0xffff)
# asm 1: mpy >z4x5x4t231=vec128#90,<t4t0bbt112=vec128#71,<et0aat119=vec128#91
# asm 2: mpy >z4x5x4t231=$92,<t4t0bbt112=$73,<et0aat119=$93
mpy $92,$73,$93

# qhasm: int32323232 z4x5x4t225 += (t4t0bbt117 & 0xffff) * (et0aat18  & 0xffff)
# asm 1: mpya >z4x5x4t225=vec128#55,<t4t0bbt117=vec128#70,<et0aat18=vec128#99,<z4x5x4t225=vec128#55
# asm 2: mpya >z4x5x4t225=$57,<t4t0bbt117=$72,<et0aat18=$101,<z4x5x4t225=$57
mpya $57,$72,$101,$57

# qhasm: int32323232 z4x5x4t226 += (t4t0bbt116 & 0xffff) * (et0aat110 & 0xffff)
# asm 1: mpya >z4x5x4t226=vec128#56,<t4t0bbt116=vec128#69,<et0aat110=vec128#94,<z4x5x4t226=vec128#56
# asm 2: mpya >z4x5x4t226=$58,<t4t0bbt116=$71,<et0aat110=$96,<z4x5x4t226=$58
mpya $58,$71,$96,$58

# qhasm: int32323232 z4x5x4t227 += (t4t0bbt116 & 0xffff) * (et0aat111 & 0xffff)
# asm 1: mpya >z4x5x4t227=vec128#64,<t4t0bbt116=vec128#69,<et0aat111=vec128#95,<z4x5x4t227=vec128#64
# asm 2: mpya >z4x5x4t227=$66,<t4t0bbt116=$71,<et0aat111=$97,<z4x5x4t227=$66
mpya $66,$71,$97,$66

# qhasm: int32323232 z4x5x4t228 += (t4t0bbt119 & 0xffff) * (et0aat19  & 0xffff)
# asm 1: mpya >z4x5x4t228=vec128#67,<t4t0bbt119=vec128#73,<et0aat19=vec128#93,<z4x5x4t228=vec128#67
# asm 2: mpya >z4x5x4t228=$69,<t4t0bbt119=$75,<et0aat19=$95,<z4x5x4t228=$69
mpya $69,$75,$95,$69

# qhasm: int32323232 z4x5x4t229 <<= 1
# asm 1: shli >z4x5x4t229=vec128#89,<z4x5x4t229=vec128#89,1
# asm 2: shli >z4x5x4t229=$91,<z4x5x4t229=$91,1
shli $91,$91,1

# qhasm: int32323232 z4x5x4t230 += (t4t0bbt112 & 0xffff) * (et0aat118 & 0xffff)
# asm 1: mpya >z4x5x4t230=vec128#68,<t4t0bbt112=vec128#71,<et0aat118=vec128#87,<z4x5x4t230=vec128#68
# asm 2: mpya >z4x5x4t230=$70,<t4t0bbt112=$73,<et0aat118=$89,<z4x5x4t230=$70
mpya $70,$73,$89,$70

# qhasm: int32323232 z4x5x4t231 += (t4t0bbt113 & 0xffff) * (et0aat118 & 0xffff)
# asm 1: mpya >z4x5x4t231=vec128#90,<t4t0bbt113=vec128#60,<et0aat118=vec128#87,<z4x5x4t231=vec128#90
# asm 2: mpya >z4x5x4t231=$92,<t4t0bbt113=$62,<et0aat118=$89,<z4x5x4t231=$92
mpya $92,$62,$89,$92

# qhasm: int32323232 z4x5x4t232  = (t4t0bbt113 & 0xffff) * (et0aat119 & 0xffff)
# asm 1: mpy >z4x5x4t232=vec128#92,<t4t0bbt113=vec128#60,<et0aat119=vec128#91
# asm 2: mpy >z4x5x4t232=$94,<t4t0bbt113=$62,<et0aat119=$93
mpy $94,$62,$93

# qhasm: int32323232 z4x5x4t226 += (t4t0bbt117 & 0xffff) * (et0aat19  & 0xffff)
# asm 1: mpya >z4x5x4t226=vec128#56,<t4t0bbt117=vec128#70,<et0aat19=vec128#93,<z4x5x4t226=vec128#56
# asm 2: mpya >z4x5x4t226=$58,<t4t0bbt117=$72,<et0aat19=$95,<z4x5x4t226=$58
mpya $58,$72,$95,$58

# qhasm: int32323232 z4x5x4t227 += (t4t0bbt117 & 0xffff) * (et0aat110 & 0xffff)
# asm 1: mpya >z4x5x4t227=vec128#64,<t4t0bbt117=vec128#70,<et0aat110=vec128#94,<z4x5x4t227=vec128#64
# asm 2: mpya >z4x5x4t227=$66,<t4t0bbt117=$72,<et0aat110=$96,<z4x5x4t227=$66
mpya $66,$72,$96,$66

# qhasm: int32323232 z4x5x4t228 <<= 1
# asm 1: shli >z4x5x4t228=vec128#67,<z4x5x4t228=vec128#67,1
# asm 2: shli >z4x5x4t228=$69,<z4x5x4t228=$69,1
shli $69,$69,1

# qhasm: int32323232 z4x5x4t229 += (t4t0bbt112 & 0xffff) * (et0aat117 & 0xffff)
# asm 1: mpya >z4x5x4t229=vec128#89,<t4t0bbt112=vec128#71,<et0aat117=vec128#88,<z4x5x4t229=vec128#89
# asm 2: mpya >z4x5x4t229=$91,<t4t0bbt112=$73,<et0aat117=$90,<z4x5x4t229=$91
mpya $91,$73,$90,$91

# qhasm: int32323232 z4x5x4t230 += (t4t0bbt113 & 0xffff) * (et0aat117 & 0xffff)
# asm 1: mpya >z4x5x4t230=vec128#68,<t4t0bbt113=vec128#60,<et0aat117=vec128#88,<z4x5x4t230=vec128#68
# asm 2: mpya >z4x5x4t230=$70,<t4t0bbt113=$62,<et0aat117=$90,<z4x5x4t230=$70
mpya $70,$62,$90,$70

# qhasm: int32323232 z4x5x4t231 += (t4t0bbt114 & 0xffff) * (et0aat117 & 0xffff)
# asm 1: mpya >z4x5x4t231=vec128#90,<t4t0bbt114=vec128#57,<et0aat117=vec128#88,<z4x5x4t231=vec128#90
# asm 2: mpya >z4x5x4t231=$92,<t4t0bbt114=$59,<et0aat117=$90,<z4x5x4t231=$92
mpya $92,$59,$90,$92

# qhasm: int32323232 z4x5x4t232 += (t4t0bbt114 & 0xffff) * (et0aat118 & 0xffff)
# asm 1: mpya >z4x5x4t232=vec128#92,<t4t0bbt114=vec128#57,<et0aat118=vec128#87,<z4x5x4t232=vec128#92
# asm 2: mpya >z4x5x4t232=$94,<t4t0bbt114=$59,<et0aat118=$89,<z4x5x4t232=$94
mpya $94,$59,$89,$94

# qhasm: int32323232 z4x5x4t226 += (t4t0bbt118 & 0xffff) * (et0aat18  & 0xffff)
# asm 1: mpya >z4x5x4t226=vec128#56,<t4t0bbt118=vec128#59,<et0aat18=vec128#99,<z4x5x4t226=vec128#56
# asm 2: mpya >z4x5x4t226=$58,<t4t0bbt118=$61,<et0aat18=$101,<z4x5x4t226=$58
mpya $58,$61,$101,$58

# qhasm: int32323232 z4x5x4t227 += (t4t0bbt118 & 0xffff) * (et0aat19  & 0xffff)
# asm 1: mpya >z4x5x4t227=vec128#64,<t4t0bbt118=vec128#59,<et0aat19=vec128#93,<z4x5x4t227=vec128#64
# asm 2: mpya >z4x5x4t227=$66,<t4t0bbt118=$61,<et0aat19=$95,<z4x5x4t227=$66
mpya $66,$61,$95,$66

# qhasm: int32323232 z4x5x4t228 += (t4t0bbt112 & 0xffff) * (et0aat116 & 0xffff)
# asm 1: mpya >z4x5x4t228=vec128#67,<t4t0bbt112=vec128#71,<et0aat116=vec128#85,<z4x5x4t228=vec128#67
# asm 2: mpya >z4x5x4t228=$69,<t4t0bbt112=$73,<et0aat116=$87,<z4x5x4t228=$69
mpya $69,$73,$87,$69

# qhasm: int32323232 z4x5x4t229 += (t4t0bbt113 & 0xffff) * (et0aat116 & 0xffff)
# asm 1: mpya >z4x5x4t229=vec128#60,<t4t0bbt113=vec128#60,<et0aat116=vec128#85,<z4x5x4t229=vec128#89
# asm 2: mpya >z4x5x4t229=$62,<t4t0bbt113=$62,<et0aat116=$87,<z4x5x4t229=$91
mpya $62,$62,$87,$91

# qhasm: int32323232 z4x5x4t230 += (t4t0bbt114 & 0xffff) * (et0aat116 & 0xffff)
# asm 1: mpya >z4x5x4t230=vec128#68,<t4t0bbt114=vec128#57,<et0aat116=vec128#85,<z4x5x4t230=vec128#68
# asm 2: mpya >z4x5x4t230=$70,<t4t0bbt114=$59,<et0aat116=$87,<z4x5x4t230=$70
mpya $70,$59,$87,$70

# qhasm: int32323232 z4x5x4t231 += (t4t0bbt115 & 0xffff) * (et0aat116 & 0xffff)
# asm 1: mpya >z4x5x4t231=vec128#71,<t4t0bbt115=vec128#72,<et0aat116=vec128#85,<z4x5x4t231=vec128#90
# asm 2: mpya >z4x5x4t231=$73,<t4t0bbt115=$74,<et0aat116=$87,<z4x5x4t231=$92
mpya $73,$74,$87,$92

# qhasm: int32323232 z4x5x4t232 += (t4t0bbt115 & 0xffff) * (et0aat117 & 0xffff)
# asm 1: mpya >z4x5x4t232=vec128#89,<t4t0bbt115=vec128#72,<et0aat117=vec128#88,<z4x5x4t232=vec128#92
# asm 2: mpya >z4x5x4t232=$91,<t4t0bbt115=$74,<et0aat117=$90,<z4x5x4t232=$94
mpya $91,$74,$90,$94

# qhasm: int32323232 z4x5x4t233  = (t4t0bbt114 & 0xffff) * (et0aat119 & 0xffff)
# asm 1: mpy >z4x5x4t233=vec128#57,<t4t0bbt114=vec128#57,<et0aat119=vec128#91
# asm 2: mpy >z4x5x4t233=$59,<t4t0bbt114=$59,<et0aat119=$93
mpy $59,$59,$93

# qhasm: int32323232 z4x5x4t227 += (t4t0bbt119 & 0xffff) * (et0aat18  & 0xffff)
# asm 1: mpya >z4x5x4t227=vec128#64,<t4t0bbt119=vec128#73,<et0aat18=vec128#99,<z4x5x4t227=vec128#64
# asm 2: mpya >z4x5x4t227=$66,<t4t0bbt119=$75,<et0aat18=$101,<z4x5x4t227=$66
mpya $66,$75,$101,$66

# qhasm: int32323232 z4x5x4t228 += (t4t0bbt116 & 0xffff) * (et0aat112 & 0xffff)
# asm 1: mpya >z4x5x4t228=vec128#67,<t4t0bbt116=vec128#69,<et0aat112=vec128#81,<z4x5x4t228=vec128#67
# asm 2: mpya >z4x5x4t228=$69,<t4t0bbt116=$71,<et0aat112=$83,<z4x5x4t228=$69
mpya $69,$71,$83,$69

# qhasm: int32323232 z4x5x4t229 += (t4t0bbt116 & 0xffff) * (et0aat113 & 0xffff)
# asm 1: mpya >z4x5x4t229=vec128#60,<t4t0bbt116=vec128#69,<et0aat113=vec128#84,<z4x5x4t229=vec128#60
# asm 2: mpya >z4x5x4t229=$62,<t4t0bbt116=$71,<et0aat113=$86,<z4x5x4t229=$62
mpya $62,$71,$86,$62

# qhasm: int32323232 z4x5x4t230 += (t4t0bbt116 & 0xffff) * (et0aat114 & 0xffff)
# asm 1: mpya >z4x5x4t230=vec128#68,<t4t0bbt116=vec128#69,<et0aat114=vec128#83,<z4x5x4t230=vec128#68
# asm 2: mpya >z4x5x4t230=$70,<t4t0bbt116=$71,<et0aat114=$85,<z4x5x4t230=$70
mpya $70,$71,$85,$70

# qhasm: int32323232 z4x5x4t231 += (t4t0bbt116 & 0xffff) * (et0aat115 & 0xffff)
# asm 1: mpya >z4x5x4t231=vec128#71,<t4t0bbt116=vec128#69,<et0aat115=vec128#103,<z4x5x4t231=vec128#71
# asm 2: mpya >z4x5x4t231=$73,<t4t0bbt116=$71,<et0aat115=$105,<z4x5x4t231=$73
mpya $73,$71,$105,$73

# qhasm: int32323232 z4x5x4t232 += (t4t0bbt117 & 0xffff) * (et0aat115 & 0xffff)
# asm 1: mpya >z4x5x4t232=vec128#89,<t4t0bbt117=vec128#70,<et0aat115=vec128#103,<z4x5x4t232=vec128#89
# asm 2: mpya >z4x5x4t232=$91,<t4t0bbt117=$72,<et0aat115=$105,<z4x5x4t232=$91
mpya $91,$72,$105,$91

# qhasm: int32323232 z4x5x4t233 += (t4t0bbt115 & 0xffff) * (et0aat118 & 0xffff)
# asm 1: mpya >z4x5x4t233=vec128#57,<t4t0bbt115=vec128#72,<et0aat118=vec128#87,<z4x5x4t233=vec128#57
# asm 2: mpya >z4x5x4t233=$59,<t4t0bbt115=$74,<et0aat118=$89,<z4x5x4t233=$59
mpya $59,$74,$89,$59

# qhasm: int32323232 z4x5x4t234 = (t4t0bbt115 & 0xffff) * (et0aat119 & 0xffff)
# asm 1: mpy >z4x5x4t234=vec128#72,<t4t0bbt115=vec128#72,<et0aat119=vec128#91
# asm 2: mpy >z4x5x4t234=$74,<t4t0bbt115=$74,<et0aat119=$93
mpy $74,$74,$93

# qhasm: int32323232 z4x5x4t235 = (t4t0bbt116 & 0xffff) * (et0aat119 & 0xffff)
# asm 1: mpy >z4x5x4t235=vec128#90,<t4t0bbt116=vec128#69,<et0aat119=vec128#91
# asm 2: mpy >z4x5x4t235=$92,<t4t0bbt116=$71,<et0aat119=$93
mpy $92,$71,$93

# qhasm: int32323232 z4x5x4t229 += (t4t0bbt117 & 0xffff) * (et0aat112 & 0xffff)
# asm 1: mpya >z4x5x4t229=vec128#60,<t4t0bbt117=vec128#70,<et0aat112=vec128#81,<z4x5x4t229=vec128#60
# asm 2: mpya >z4x5x4t229=$62,<t4t0bbt117=$72,<et0aat112=$83,<z4x5x4t229=$62
mpya $62,$72,$83,$62

# qhasm: int32323232 z4x5x4t230 += (t4t0bbt117 & 0xffff) * (et0aat113 & 0xffff)
# asm 1: mpya >z4x5x4t230=vec128#68,<t4t0bbt117=vec128#70,<et0aat113=vec128#84,<z4x5x4t230=vec128#68
# asm 2: mpya >z4x5x4t230=$70,<t4t0bbt117=$72,<et0aat113=$86,<z4x5x4t230=$70
mpya $70,$72,$86,$70

# qhasm: int32323232 z4x5x4t231 += (t4t0bbt117 & 0xffff) * (et0aat114 & 0xffff)
# asm 1: mpya >z4x5x4t231=vec128#71,<t4t0bbt117=vec128#70,<et0aat114=vec128#83,<z4x5x4t231=vec128#71
# asm 2: mpya >z4x5x4t231=$73,<t4t0bbt117=$72,<et0aat114=$85,<z4x5x4t231=$73
mpya $73,$72,$85,$73

# qhasm: int32323232 z4x5x4t232 += (t4t0bbt118 & 0xffff) * (et0aat114 & 0xffff)
# asm 1: mpya >z4x5x4t232=vec128#89,<t4t0bbt118=vec128#59,<et0aat114=vec128#83,<z4x5x4t232=vec128#89
# asm 2: mpya >z4x5x4t232=$91,<t4t0bbt118=$61,<et0aat114=$85,<z4x5x4t232=$91
mpya $91,$61,$85,$91

# qhasm: int32323232 z4x5x4t233 += (t4t0bbt118 & 0xffff) * (et0aat115 & 0xffff)
# asm 1: mpya >z4x5x4t233=vec128#57,<t4t0bbt118=vec128#59,<et0aat115=vec128#103,<z4x5x4t233=vec128#57
# asm 2: mpya >z4x5x4t233=$59,<t4t0bbt118=$61,<et0aat115=$105,<z4x5x4t233=$59
mpya $59,$61,$105,$59

# qhasm: int32323232 z4x5x4t234 += (t4t0bbt119 & 0xffff) * (et0aat115 & 0xffff)
# asm 1: mpya >z4x5x4t234=vec128#72,<t4t0bbt119=vec128#73,<et0aat115=vec128#103,<z4x5x4t234=vec128#72
# asm 2: mpya >z4x5x4t234=$74,<t4t0bbt119=$75,<et0aat115=$105,<z4x5x4t234=$74
mpya $74,$75,$105,$74

# qhasm: int32323232 z4x5x4t235 += (t4t0bbt117 & 0xffff) * (et0aat118 & 0xffff)
# asm 1: mpya >z4x5x4t235=vec128#90,<t4t0bbt117=vec128#70,<et0aat118=vec128#87,<z4x5x4t235=vec128#90
# asm 2: mpya >z4x5x4t235=$92,<t4t0bbt117=$72,<et0aat118=$89,<z4x5x4t235=$92
mpya $92,$72,$89,$92

# qhasm: int32323232 z4x5x4t236 = (t4t0bbt117 & 0xffff) * (et0aat119 & 0xffff)
# asm 1: mpy >z4x5x4t236=vec128#92,<t4t0bbt117=vec128#70,<et0aat119=vec128#91
# asm 2: mpy >z4x5x4t236=$94,<t4t0bbt117=$72,<et0aat119=$93
mpy $94,$72,$93

# qhasm: int32323232 z4x5x4t230 += (t4t0bbt118 & 0xffff) * (et0aat112 & 0xffff)
# asm 1: mpya >z4x5x4t230=vec128#68,<t4t0bbt118=vec128#59,<et0aat112=vec128#81,<z4x5x4t230=vec128#68
# asm 2: mpya >z4x5x4t230=$70,<t4t0bbt118=$61,<et0aat112=$83,<z4x5x4t230=$70
mpya $70,$61,$83,$70

# qhasm: int32323232 z4x5x4t231 += (t4t0bbt118 & 0xffff) * (et0aat113 & 0xffff)
# asm 1: mpya >z4x5x4t231=vec128#71,<t4t0bbt118=vec128#59,<et0aat113=vec128#84,<z4x5x4t231=vec128#71
# asm 2: mpya >z4x5x4t231=$73,<t4t0bbt118=$61,<et0aat113=$86,<z4x5x4t231=$73
mpya $73,$61,$86,$73

# qhasm: int32323232 z4x5x4t232 += (t4t0bbt119 & 0xffff) * (et0aat113 & 0xffff)
# asm 1: mpya >z4x5x4t232=vec128#84,<t4t0bbt119=vec128#73,<et0aat113=vec128#84,<z4x5x4t232=vec128#89
# asm 2: mpya >z4x5x4t232=$86,<t4t0bbt119=$75,<et0aat113=$86,<z4x5x4t232=$91
mpya $86,$75,$86,$91

# qhasm: int32323232 z4x5x4t233 += (t4t0bbt119 & 0xffff) * (et0aat114 & 0xffff)
# asm 1: mpya >z4x5x4t233=vec128#57,<t4t0bbt119=vec128#73,<et0aat114=vec128#83,<z4x5x4t233=vec128#57
# asm 2: mpya >z4x5x4t233=$59,<t4t0bbt119=$75,<et0aat114=$85,<z4x5x4t233=$59
mpya $59,$75,$85,$59

# qhasm: int32323232 z4x5x4t234 <<= 1
# asm 1: shli >z4x5x4t234=vec128#72,<z4x5x4t234=vec128#72,1
# asm 2: shli >z4x5x4t234=$74,<z4x5x4t234=$74,1
shli $74,$74,1

# qhasm: int32323232 z4x5x4t235 += (t4t0bbt118 & 0xffff) * (et0aat117 & 0xffff)
# asm 1: mpya >z4x5x4t235=vec128#83,<t4t0bbt118=vec128#59,<et0aat117=vec128#88,<z4x5x4t235=vec128#90
# asm 2: mpya >z4x5x4t235=$85,<t4t0bbt118=$61,<et0aat117=$90,<z4x5x4t235=$92
mpya $85,$61,$90,$92

# qhasm: int32323232 z4x5x4t236 += (t4t0bbt118 & 0xffff) * (et0aat118 & 0xffff)
# asm 1: mpya >z4x5x4t236=vec128#89,<t4t0bbt118=vec128#59,<et0aat118=vec128#87,<z4x5x4t236=vec128#92
# asm 2: mpya >z4x5x4t236=$91,<t4t0bbt118=$61,<et0aat118=$89,<z4x5x4t236=$94
mpya $91,$61,$89,$94

# qhasm: int32323232 z4x5x4t237 = (t4t0bbt118 & 0xffff) * (et0aat119 & 0xffff)
# asm 1: mpy >z4x5x4t237=vec128#90,<t4t0bbt118=vec128#59,<et0aat119=vec128#91
# asm 2: mpy >z4x5x4t237=$92,<t4t0bbt118=$61,<et0aat119=$93
mpy $92,$61,$93

# qhasm: int32323232 z4x5x4t231 += (t4t0bbt119 & 0xffff) * (et0aat112 & 0xffff)
# asm 1: mpya >z4x5x4t231=vec128#71,<t4t0bbt119=vec128#73,<et0aat112=vec128#81,<z4x5x4t231=vec128#71
# asm 2: mpya >z4x5x4t231=$73,<t4t0bbt119=$75,<et0aat112=$83,<z4x5x4t231=$73
mpya $73,$75,$83,$73

# qhasm: int32323232 z4x5x4t232 <<= 1
# asm 1: shli >z4x5x4t232=vec128#81,<z4x5x4t232=vec128#84,1
# asm 2: shli >z4x5x4t232=$83,<z4x5x4t232=$86,1
shli $83,$86,1

# qhasm: int32323232 z4x5x4t233 <<= 1
# asm 1: shli >z4x5x4t233=vec128#57,<z4x5x4t233=vec128#57,1
# asm 2: shli >z4x5x4t233=$59,<z4x5x4t233=$59,1
shli $59,$59,1

# qhasm: int32323232 z4x5x4t234 += (t4t0bbt116 & 0xffff) * (et0aat118 & 0xffff)
# asm 1: mpya >z4x5x4t234=vec128#72,<t4t0bbt116=vec128#69,<et0aat118=vec128#87,<z4x5x4t234=vec128#72
# asm 2: mpya >z4x5x4t234=$74,<t4t0bbt116=$71,<et0aat118=$89,<z4x5x4t234=$74
mpya $74,$71,$89,$74

# qhasm: int32323232 z4x5x4t235 += (t4t0bbt119 & 0xffff) * (et0aat116 & 0xffff)
# asm 1: mpya >z4x5x4t235=vec128#83,<t4t0bbt119=vec128#73,<et0aat116=vec128#85,<z4x5x4t235=vec128#83
# asm 2: mpya >z4x5x4t235=$85,<t4t0bbt119=$75,<et0aat116=$87,<z4x5x4t235=$85
mpya $85,$75,$87,$85

# qhasm: int32323232 z4x5x4t236 += (t4t0bbt119 & 0xffff) * (et0aat117 & 0xffff)
# asm 1: mpya >z4x5x4t236=vec128#84,<t4t0bbt119=vec128#73,<et0aat117=vec128#88,<z4x5x4t236=vec128#89
# asm 2: mpya >z4x5x4t236=$86,<t4t0bbt119=$75,<et0aat117=$90,<z4x5x4t236=$91
mpya $86,$75,$90,$91

# qhasm: int32323232 z4x5x4t237 += (t4t0bbt119 & 0xffff) * (et0aat118 & 0xffff)
# asm 1: mpya >z4x5x4t237=vec128#87,<t4t0bbt119=vec128#73,<et0aat118=vec128#87,<z4x5x4t237=vec128#90
# asm 2: mpya >z4x5x4t237=$89,<t4t0bbt119=$75,<et0aat118=$89,<z4x5x4t237=$92
mpya $89,$75,$89,$92

# qhasm: int32323232 z4x5x4t238 = (t4t0bbt119 & 0xffff) * (et0aat119 & 0xffff)
# asm 1: mpy >z4x5x4t238=vec128#73,<t4t0bbt119=vec128#73,<et0aat119=vec128#91
# asm 2: mpy >z4x5x4t238=$75,<t4t0bbt119=$75,<et0aat119=$93
mpy $75,$75,$93

# qhasm: int32323232 z4x5x4t232 += (t4t0bbt116 & 0xffff) * (et0aat116 & 0xffff)
# asm 1: mpya >z4x5x4t232=vec128#81,<t4t0bbt116=vec128#69,<et0aat116=vec128#85,<z4x5x4t232=vec128#81
# asm 2: mpya >z4x5x4t232=$83,<t4t0bbt116=$71,<et0aat116=$87,<z4x5x4t232=$83
mpya $83,$71,$87,$83

# qhasm: int32323232 z4x5x4t233 += (t4t0bbt116 & 0xffff) * (et0aat117 & 0xffff)
# asm 1: mpya >z4x5x4t233=vec128#57,<t4t0bbt116=vec128#69,<et0aat117=vec128#88,<z4x5x4t233=vec128#57
# asm 2: mpya >z4x5x4t233=$59,<t4t0bbt116=$71,<et0aat117=$90,<z4x5x4t233=$59
mpya $59,$71,$90,$59

# qhasm: int32323232 z4x5x4t234 += (t4t0bbt117 & 0xffff) * (et0aat117 & 0xffff)
# asm 1: mpya >z4x5x4t234=vec128#69,<t4t0bbt117=vec128#70,<et0aat117=vec128#88,<z4x5x4t234=vec128#72
# asm 2: mpya >z4x5x4t234=$71,<t4t0bbt117=$72,<et0aat117=$90,<z4x5x4t234=$74
mpya $71,$72,$90,$74

# qhasm: int32323232 z4x5x4t236 <<= 1
# asm 1: shli >z4x5x4t236=vec128#72,<z4x5x4t236=vec128#84,1
# asm 2: shli >z4x5x4t236=$74,<z4x5x4t236=$86,1
shli $74,$86,1

# qhasm: int32323232 z4x5x4t237 <<= 1
# asm 1: shli >z4x5x4t237=vec128#84,<z4x5x4t237=vec128#87,1
# asm 2: shli >z4x5x4t237=$86,<z4x5x4t237=$89,1
shli $86,$89,1

# qhasm: int32323232 z4x5x4t238 <<= 1
# asm 1: shli >z4x5x4t238=vec128#73,<z4x5x4t238=vec128#73,1
# asm 2: shli >z4x5x4t238=$75,<z4x5x4t238=$75,1
shli $75,$75,1

# qhasm: int32323232 z4x5x4t233 += (t4t0bbt117 & 0xffff) * (et0aat116 & 0xffff)
# asm 1: mpya >z4x5x4t233=vec128#57,<t4t0bbt117=vec128#70,<et0aat116=vec128#85,<z4x5x4t233=vec128#57
# asm 2: mpya >z4x5x4t233=$59,<t4t0bbt117=$72,<et0aat116=$87,<z4x5x4t233=$59
mpya $59,$72,$87,$59

# qhasm: int32323232 z4x5x4t234 += (t4t0bbt118 & 0xffff) * (et0aat116 & 0xffff)
# asm 1: mpya >z4x5x4t234=vec128#59,<t4t0bbt118=vec128#59,<et0aat116=vec128#85,<z4x5x4t234=vec128#69
# asm 2: mpya >z4x5x4t234=$61,<t4t0bbt118=$61,<et0aat116=$87,<z4x5x4t234=$71
mpya $61,$61,$87,$71

# qhasm: uint32323232 carry0 = z4x5x4t220 >> 13
# asm 1: rotmi >carry0=vec128#69,<z4x5x4t220=vec128#61,-13
# asm 2: rotmi >carry0=$71,<z4x5x4t220=$63,-13
rotmi $71,$63,-13

# qhasm: uint32323232 carry1 = z4x5x4t224 >> 13
# asm 1: rotmi >carry1=vec128#70,<z4x5x4t224=vec128#62,-13
# asm 2: rotmi >carry1=$72,<z4x5x4t224=$64,-13
rotmi $72,$64,-13

# qhasm: uint32323232 carry2 = z4x5x4t228 >> 13
# asm 1: rotmi >carry2=vec128#85,<z4x5x4t228=vec128#67,-13
# asm 2: rotmi >carry2=$87,<z4x5x4t228=$69,-13
rotmi $87,$69,-13

# qhasm: uint32323232 carry3 = z4x5x4t232 >> 13
# asm 1: rotmi >carry3=vec128#87,<z4x5x4t232=vec128#81,-13
# asm 2: rotmi >carry3=$89,<z4x5x4t232=$83,-13
rotmi $89,$83,-13

# qhasm: int32323232 z4x5x4t221 += carry0
# asm 1: a >z4x5x4t221=vec128#5,<z4x5x4t221=vec128#5,<carry0=vec128#69
# asm 2: a >z4x5x4t221=$7,<z4x5x4t221=$7,<carry0=$71
a $7,$7,$71

# qhasm: z4x5x4t220 &= mask13
# asm 1: and >z4x5x4t220=vec128#61,<z4x5x4t220=vec128#61,<mask13=vec128#22
# asm 2: and >z4x5x4t220=$63,<z4x5x4t220=$63,<mask13=$24
and $63,$63,$24

# qhasm: int32323232 z4x5x4t225 += carry1
# asm 1: a >z4x5x4t225=vec128#55,<z4x5x4t225=vec128#55,<carry1=vec128#70
# asm 2: a >z4x5x4t225=$57,<z4x5x4t225=$57,<carry1=$72
a $57,$57,$72

# qhasm: z4x5x4t224 &= mask13
# asm 1: and >z4x5x4t224=vec128#62,<z4x5x4t224=vec128#62,<mask13=vec128#22
# asm 2: and >z4x5x4t224=$64,<z4x5x4t224=$64,<mask13=$24
and $64,$64,$24

# qhasm: int32323232 z4x5x4t229 += carry2
# asm 1: a >z4x5x4t229=vec128#60,<z4x5x4t229=vec128#60,<carry2=vec128#85
# asm 2: a >z4x5x4t229=$62,<z4x5x4t229=$62,<carry2=$87
a $62,$62,$87

# qhasm: z4x5x4t228 &= mask13
# asm 1: and >z4x5x4t228=vec128#67,<z4x5x4t228=vec128#67,<mask13=vec128#22
# asm 2: and >z4x5x4t228=$69,<z4x5x4t228=$69,<mask13=$24
and $69,$69,$24

# qhasm: int32323232 z4x5x4t233 += carry3
# asm 1: a >z4x5x4t233=vec128#57,<z4x5x4t233=vec128#57,<carry3=vec128#87
# asm 2: a >z4x5x4t233=$59,<z4x5x4t233=$59,<carry3=$89
a $59,$59,$89

# qhasm: z4x5x4t232 &= mask13
# asm 1: and >z4x5x4t232=vec128#69,<z4x5x4t232=vec128#81,<mask13=vec128#22
# asm 2: and >z4x5x4t232=$71,<z4x5x4t232=$83,<mask13=$24
and $71,$83,$24

# qhasm: uint32323232 carry0 = z4x5x4t221 >> 13
# asm 1: rotmi >carry0=vec128#70,<z4x5x4t221=vec128#5,-13
# asm 2: rotmi >carry0=$72,<z4x5x4t221=$7,-13
rotmi $72,$7,-13

# qhasm: uint32323232 carry1 = z4x5x4t225 >> 13
# asm 1: rotmi >carry1=vec128#81,<z4x5x4t225=vec128#55,-13
# asm 2: rotmi >carry1=$83,<z4x5x4t225=$57,-13
rotmi $83,$57,-13

# qhasm: uint32323232 carry2 = z4x5x4t229 >> 13
# asm 1: rotmi >carry2=vec128#85,<z4x5x4t229=vec128#60,-13
# asm 2: rotmi >carry2=$87,<z4x5x4t229=$62,-13
rotmi $87,$62,-13

# qhasm: uint32323232 carry3 = z4x5x4t233 >> 13
# asm 1: rotmi >carry3=vec128#87,<z4x5x4t233=vec128#57,-13
# asm 2: rotmi >carry3=$89,<z4x5x4t233=$59,-13
rotmi $89,$59,-13

# qhasm: int32323232 z4x5x4t222 += carry0
# asm 1: a >z4x5x4t222=vec128#65,<z4x5x4t222=vec128#65,<carry0=vec128#70
# asm 2: a >z4x5x4t222=$67,<z4x5x4t222=$67,<carry0=$72
a $67,$67,$72

# qhasm: z4x5x4t221 &= mask13
# asm 1: and >z4x5x4t221=vec128#5,<z4x5x4t221=vec128#5,<mask13=vec128#22
# asm 2: and >z4x5x4t221=$7,<z4x5x4t221=$7,<mask13=$24
and $7,$7,$24

# qhasm: int32323232 z4x5x4t226 += carry1
# asm 1: a >z4x5x4t226=vec128#56,<z4x5x4t226=vec128#56,<carry1=vec128#81
# asm 2: a >z4x5x4t226=$58,<z4x5x4t226=$58,<carry1=$83
a $58,$58,$83

# qhasm: z4x5x4t225 &= mask13
# asm 1: and >z4x5x4t225=vec128#55,<z4x5x4t225=vec128#55,<mask13=vec128#22
# asm 2: and >z4x5x4t225=$57,<z4x5x4t225=$57,<mask13=$24
and $57,$57,$24

# qhasm: int32323232 z4x5x4t230 += carry2
# asm 1: a >z4x5x4t230=vec128#68,<z4x5x4t230=vec128#68,<carry2=vec128#85
# asm 2: a >z4x5x4t230=$70,<z4x5x4t230=$70,<carry2=$87
a $70,$70,$87

# qhasm: z4x5x4t229 &= mask13
# asm 1: and >z4x5x4t229=vec128#60,<z4x5x4t229=vec128#60,<mask13=vec128#22
# asm 2: and >z4x5x4t229=$62,<z4x5x4t229=$62,<mask13=$24
and $62,$62,$24

# qhasm: int32323232 z4x5x4t234 += carry3
# asm 1: a >z4x5x4t234=vec128#59,<z4x5x4t234=vec128#59,<carry3=vec128#87
# asm 2: a >z4x5x4t234=$61,<z4x5x4t234=$61,<carry3=$89
a $61,$61,$89

# qhasm: z4x5x4t233 &= mask13
# asm 1: and >z4x5x4t233=vec128#57,<z4x5x4t233=vec128#57,<mask13=vec128#22
# asm 2: and >z4x5x4t233=$59,<z4x5x4t233=$59,<mask13=$24
and $59,$59,$24

# qhasm: uint32323232 carry0 = z4x5x4t222 >> 13
# asm 1: rotmi >carry0=vec128#70,<z4x5x4t222=vec128#65,-13
# asm 2: rotmi >carry0=$72,<z4x5x4t222=$67,-13
rotmi $72,$67,-13

# qhasm: uint32323232 carry1 = z4x5x4t226 >> 13
# asm 1: rotmi >carry1=vec128#81,<z4x5x4t226=vec128#56,-13
# asm 2: rotmi >carry1=$83,<z4x5x4t226=$58,-13
rotmi $83,$58,-13

# qhasm: uint32323232 carry2 = z4x5x4t230 >> 13
# asm 1: rotmi >carry2=vec128#85,<z4x5x4t230=vec128#68,-13
# asm 2: rotmi >carry2=$87,<z4x5x4t230=$70,-13
rotmi $87,$70,-13

# qhasm: uint32323232 carry3 = z4x5x4t234 >> 13
# asm 1: rotmi >carry3=vec128#87,<z4x5x4t234=vec128#59,-13
# asm 2: rotmi >carry3=$89,<z4x5x4t234=$61,-13
rotmi $89,$61,-13

# qhasm: int32323232 z4x5x4t223 += carry0
# asm 1: a >z4x5x4t223=vec128#66,<z4x5x4t223=vec128#66,<carry0=vec128#70
# asm 2: a >z4x5x4t223=$68,<z4x5x4t223=$68,<carry0=$72
a $68,$68,$72

# qhasm: z4x5x4t222 &= mask13
# asm 1: and >z4x5x4t222=vec128#65,<z4x5x4t222=vec128#65,<mask13=vec128#22
# asm 2: and >z4x5x4t222=$67,<z4x5x4t222=$67,<mask13=$24
and $67,$67,$24

# qhasm: int32323232 z4x5x4t227 += carry1
# asm 1: a >z4x5x4t227=vec128#64,<z4x5x4t227=vec128#64,<carry1=vec128#81
# asm 2: a >z4x5x4t227=$66,<z4x5x4t227=$66,<carry1=$83
a $66,$66,$83

# qhasm: z4x5x4t226 &= mask13
# asm 1: and >z4x5x4t226=vec128#56,<z4x5x4t226=vec128#56,<mask13=vec128#22
# asm 2: and >z4x5x4t226=$58,<z4x5x4t226=$58,<mask13=$24
and $58,$58,$24

# qhasm: int32323232 z4x5x4t231 += carry2
# asm 1: a >z4x5x4t231=vec128#70,<z4x5x4t231=vec128#71,<carry2=vec128#85
# asm 2: a >z4x5x4t231=$72,<z4x5x4t231=$73,<carry2=$87
a $72,$73,$87

# qhasm: z4x5x4t230 &= mask13
# asm 1: and >z4x5x4t230=vec128#68,<z4x5x4t230=vec128#68,<mask13=vec128#22
# asm 2: and >z4x5x4t230=$70,<z4x5x4t230=$70,<mask13=$24
and $70,$70,$24

# qhasm: int32323232 z4x5x4t235 += carry3
# asm 1: a >z4x5x4t235=vec128#71,<z4x5x4t235=vec128#83,<carry3=vec128#87
# asm 2: a >z4x5x4t235=$73,<z4x5x4t235=$85,<carry3=$89
a $73,$85,$89

# qhasm: z4x5x4t234 &= mask13
# asm 1: and >z4x5x4t234=vec128#59,<z4x5x4t234=vec128#59,<mask13=vec128#22
# asm 2: and >z4x5x4t234=$61,<z4x5x4t234=$61,<mask13=$24
and $61,$61,$24

# qhasm: uint32323232 carry0 = z4x5x4t223 >> 12
# asm 1: rotmi >carry0=vec128#81,<z4x5x4t223=vec128#66,-12
# asm 2: rotmi >carry0=$83,<z4x5x4t223=$68,-12
rotmi $83,$68,-12

# qhasm: uint32323232 carry1 = z4x5x4t227 >> 12
# asm 1: rotmi >carry1=vec128#83,<z4x5x4t227=vec128#64,-12
# asm 2: rotmi >carry1=$85,<z4x5x4t227=$66,-12
rotmi $85,$66,-12

# qhasm: uint32323232 carry2 = z4x5x4t231 >> 12
# asm 1: rotmi >carry2=vec128#85,<z4x5x4t231=vec128#70,-12
# asm 2: rotmi >carry2=$87,<z4x5x4t231=$72,-12
rotmi $87,$72,-12

# qhasm: uint32323232 carry3 = z4x5x4t235 >> 12
# asm 1: rotmi >carry3=vec128#87,<z4x5x4t235=vec128#71,-12
# asm 2: rotmi >carry3=$89,<z4x5x4t235=$73,-12
rotmi $89,$73,-12

# qhasm: int32323232 z4x5x4t224 += carry0
# asm 1: a >z4x5x4t224=vec128#62,<z4x5x4t224=vec128#62,<carry0=vec128#81
# asm 2: a >z4x5x4t224=$64,<z4x5x4t224=$64,<carry0=$83
a $64,$64,$83

# qhasm: z4x5x4t223 &= mask12
# asm 1: and >z4x5x4t223=vec128#66,<z4x5x4t223=vec128#66,<mask12=vec128#21
# asm 2: and >z4x5x4t223=$68,<z4x5x4t223=$68,<mask12=$23
and $68,$68,$23

# qhasm: int32323232 z4x5x4t228 += carry1
# asm 1: a >z4x5x4t228=vec128#67,<z4x5x4t228=vec128#67,<carry1=vec128#83
# asm 2: a >z4x5x4t228=$69,<z4x5x4t228=$69,<carry1=$85
a $69,$69,$85

# qhasm: z4x5x4t227 &= mask12
# asm 1: and >z4x5x4t227=vec128#64,<z4x5x4t227=vec128#64,<mask12=vec128#21
# asm 2: and >z4x5x4t227=$66,<z4x5x4t227=$66,<mask12=$23
and $66,$66,$23

# qhasm: int32323232 z4x5x4t232 += carry2
# asm 1: a >z4x5x4t232=vec128#69,<z4x5x4t232=vec128#69,<carry2=vec128#85
# asm 2: a >z4x5x4t232=$71,<z4x5x4t232=$71,<carry2=$87
a $71,$71,$87

# qhasm: z4x5x4t231 &= mask12
# asm 1: and >z4x5x4t231=vec128#70,<z4x5x4t231=vec128#70,<mask12=vec128#21
# asm 2: and >z4x5x4t231=$72,<z4x5x4t231=$72,<mask12=$23
and $72,$72,$23

# qhasm: int32323232 z4x5x4t236 += carry3
# asm 1: a >z4x5x4t236=vec128#72,<z4x5x4t236=vec128#72,<carry3=vec128#87
# asm 2: a >z4x5x4t236=$74,<z4x5x4t236=$74,<carry3=$89
a $74,$74,$89

# qhasm: z4x5x4t235 &= mask12
# asm 1: and >z4x5x4t235=vec128#71,<z4x5x4t235=vec128#71,<mask12=vec128#21
# asm 2: and >z4x5x4t235=$73,<z4x5x4t235=$73,<mask12=$23
and $73,$73,$23

# qhasm: uint32323232 carry1 = z4x5x4t224 >> 13
# asm 1: rotmi >carry1=vec128#81,<z4x5x4t224=vec128#62,-13
# asm 2: rotmi >carry1=$83,<z4x5x4t224=$64,-13
rotmi $83,$64,-13

# qhasm: uint32323232 carry2 = z4x5x4t228 >> 13
# asm 1: rotmi >carry2=vec128#83,<z4x5x4t228=vec128#67,-13
# asm 2: rotmi >carry2=$85,<z4x5x4t228=$69,-13
rotmi $85,$69,-13

# qhasm: uint32323232 carry3 = z4x5x4t232 >> 13
# asm 1: rotmi >carry3=vec128#85,<z4x5x4t232=vec128#69,-13
# asm 2: rotmi >carry3=$87,<z4x5x4t232=$71,-13
rotmi $87,$71,-13

# qhasm: uint32323232 carry4 = z4x5x4t236 >> 13
# asm 1: rotmi >carry4=vec128#87,<z4x5x4t236=vec128#72,-13
# asm 2: rotmi >carry4=$89,<z4x5x4t236=$74,-13
rotmi $89,$74,-13

# qhasm: int32323232 z4x5x4t225 += carry1
# asm 1: a >z4x5x4t225=vec128#55,<z4x5x4t225=vec128#55,<carry1=vec128#81
# asm 2: a >z4x5x4t225=$57,<z4x5x4t225=$57,<carry1=$83
a $57,$57,$83

# qhasm: z4x5x4t224 &= mask13
# asm 1: and >z4x5x4t224=vec128#62,<z4x5x4t224=vec128#62,<mask13=vec128#22
# asm 2: and >z4x5x4t224=$64,<z4x5x4t224=$64,<mask13=$24
and $64,$64,$24

# qhasm: int32323232 z4x5x4t229 += carry2
# asm 1: a >z4x5x4t229=vec128#60,<z4x5x4t229=vec128#60,<carry2=vec128#83
# asm 2: a >z4x5x4t229=$62,<z4x5x4t229=$62,<carry2=$85
a $62,$62,$85

# qhasm: z4x5x4t228 &= mask13
# asm 1: and >z4x5x4t228=vec128#67,<z4x5x4t228=vec128#67,<mask13=vec128#22
# asm 2: and >z4x5x4t228=$69,<z4x5x4t228=$69,<mask13=$24
and $69,$69,$24

# qhasm: int32323232 z4x5x4t233 += carry3
# asm 1: a >z4x5x4t233=vec128#57,<z4x5x4t233=vec128#57,<carry3=vec128#85
# asm 2: a >z4x5x4t233=$59,<z4x5x4t233=$59,<carry3=$87
a $59,$59,$87

# qhasm: z4x5x4t232 &= mask13
# asm 1: and >z4x5x4t232=vec128#69,<z4x5x4t232=vec128#69,<mask13=vec128#22
# asm 2: and >z4x5x4t232=$71,<z4x5x4t232=$71,<mask13=$24
and $71,$71,$24

# qhasm: int32323232 z4x5x4t237 += carry4
# asm 1: a >z4x5x4t237=vec128#81,<z4x5x4t237=vec128#84,<carry4=vec128#87
# asm 2: a >z4x5x4t237=$83,<z4x5x4t237=$86,<carry4=$89
a $83,$86,$89

# qhasm: z4x5x4t236 &= mask13
# asm 1: and >z4x5x4t236=vec128#72,<z4x5x4t236=vec128#72,<mask13=vec128#22
# asm 2: and >z4x5x4t236=$74,<z4x5x4t236=$74,<mask13=$24
and $74,$74,$24

# qhasm: uint32323232 carry1 = z4x5x4t225 >> 13
# asm 1: rotmi >carry1=vec128#83,<z4x5x4t225=vec128#55,-13
# asm 2: rotmi >carry1=$85,<z4x5x4t225=$57,-13
rotmi $85,$57,-13

# qhasm: uint32323232 carry2 = z4x5x4t229 >> 13
# asm 1: rotmi >carry2=vec128#84,<z4x5x4t229=vec128#60,-13
# asm 2: rotmi >carry2=$86,<z4x5x4t229=$62,-13
rotmi $86,$62,-13

# qhasm: uint32323232 carry3 = z4x5x4t233 >> 13
# asm 1: rotmi >carry3=vec128#85,<z4x5x4t233=vec128#57,-13
# asm 2: rotmi >carry3=$87,<z4x5x4t233=$59,-13
rotmi $87,$59,-13

# qhasm: uint32323232 carry4 = z4x5x4t237 >> 13
# asm 1: rotmi >carry4=vec128#87,<z4x5x4t237=vec128#81,-13
# asm 2: rotmi >carry4=$89,<z4x5x4t237=$83,-13
rotmi $89,$83,-13

# qhasm: int32323232 z4x5x4t226 += carry1
# asm 1: a >z4x5x4t226=vec128#56,<z4x5x4t226=vec128#56,<carry1=vec128#83
# asm 2: a >z4x5x4t226=$58,<z4x5x4t226=$58,<carry1=$85
a $58,$58,$85

# qhasm: z4x5x4t225 &= mask13
# asm 1: and >z4x5x4t225=vec128#55,<z4x5x4t225=vec128#55,<mask13=vec128#22
# asm 2: and >z4x5x4t225=$57,<z4x5x4t225=$57,<mask13=$24
and $57,$57,$24

# qhasm: int32323232 z4x5x4t230 += carry2
# asm 1: a >z4x5x4t230=vec128#68,<z4x5x4t230=vec128#68,<carry2=vec128#84
# asm 2: a >z4x5x4t230=$70,<z4x5x4t230=$70,<carry2=$86
a $70,$70,$86

# qhasm: z4x5x4t229 &= mask13
# asm 1: and >z4x5x4t229=vec128#60,<z4x5x4t229=vec128#60,<mask13=vec128#22
# asm 2: and >z4x5x4t229=$62,<z4x5x4t229=$62,<mask13=$24
and $62,$62,$24

# qhasm: int32323232 z4x5x4t234 += carry3
# asm 1: a >z4x5x4t234=vec128#59,<z4x5x4t234=vec128#59,<carry3=vec128#85
# asm 2: a >z4x5x4t234=$61,<z4x5x4t234=$61,<carry3=$87
a $61,$61,$87

# qhasm: z4x5x4t233 &= mask13
# asm 1: and >z4x5x4t233=vec128#57,<z4x5x4t233=vec128#57,<mask13=vec128#22
# asm 2: and >z4x5x4t233=$59,<z4x5x4t233=$59,<mask13=$24
and $59,$59,$24

# qhasm: int32323232 z4x5x4t238 += carry4
# asm 1: a >z4x5x4t238=vec128#73,<z4x5x4t238=vec128#73,<carry4=vec128#87
# asm 2: a >z4x5x4t238=$75,<z4x5x4t238=$75,<carry4=$89
a $75,$75,$89

# qhasm: z4x5x4t237 &= mask13
# asm 1: and >z4x5x4t237=vec128#81,<z4x5x4t237=vec128#81,<mask13=vec128#22
# asm 2: and >z4x5x4t237=$83,<z4x5x4t237=$83,<mask13=$24
and $83,$83,$24

# qhasm: uint32323232 carry1 = z4x5x4t226 >> 13
# asm 1: rotmi >carry1=vec128#83,<z4x5x4t226=vec128#56,-13
# asm 2: rotmi >carry1=$85,<z4x5x4t226=$58,-13
rotmi $85,$58,-13

# qhasm: uint32323232 carry2 = z4x5x4t230 >> 13
# asm 1: rotmi >carry2=vec128#84,<z4x5x4t230=vec128#68,-13
# asm 2: rotmi >carry2=$86,<z4x5x4t230=$70,-13
rotmi $86,$70,-13

# qhasm: uint32323232 carry3 = z4x5x4t234 >> 13
# asm 1: rotmi >carry3=vec128#85,<z4x5x4t234=vec128#59,-13
# asm 2: rotmi >carry3=$87,<z4x5x4t234=$61,-13
rotmi $87,$61,-13

# qhasm: uint32323232 z4x5x4t239 = z4x5x4t238 >> 13
# asm 1: rotmi >z4x5x4t239=vec128#87,<z4x5x4t238=vec128#73,-13
# asm 2: rotmi >z4x5x4t239=$89,<z4x5x4t238=$75,-13
rotmi $89,$75,-13

# qhasm: int32323232 z4x5x4t227 += carry1
# asm 1: a >z4x5x4t227=vec128#64,<z4x5x4t227=vec128#64,<carry1=vec128#83
# asm 2: a >z4x5x4t227=$66,<z4x5x4t227=$66,<carry1=$85
a $66,$66,$85

# qhasm: z4x5x4t226 &= mask13
# asm 1: and >z4x5x4t226=vec128#56,<z4x5x4t226=vec128#56,<mask13=vec128#22
# asm 2: and >z4x5x4t226=$58,<z4x5x4t226=$58,<mask13=$24
and $58,$58,$24

# qhasm: int32323232 z4x5x4t231 += carry2
# asm 1: a >z4x5x4t231=vec128#70,<z4x5x4t231=vec128#70,<carry2=vec128#84
# asm 2: a >z4x5x4t231=$72,<z4x5x4t231=$72,<carry2=$86
a $72,$72,$86

# qhasm: z4x5x4t230 &= mask13
# asm 1: and >z4x5x4t230=vec128#68,<z4x5x4t230=vec128#68,<mask13=vec128#22
# asm 2: and >z4x5x4t230=$70,<z4x5x4t230=$70,<mask13=$24
and $70,$70,$24

# qhasm: int32323232 z4x5x4t235 += carry3
# asm 1: a >z4x5x4t235=vec128#71,<z4x5x4t235=vec128#71,<carry3=vec128#85
# asm 2: a >z4x5x4t235=$73,<z4x5x4t235=$73,<carry3=$87
a $73,$73,$87

# qhasm: z4x5x4t234 &= mask13
# asm 1: and >z4x5x4t234=vec128#59,<z4x5x4t234=vec128#59,<mask13=vec128#22
# asm 2: and >z4x5x4t234=$61,<z4x5x4t234=$61,<mask13=$24
and $61,$61,$24

# qhasm: uint32323232 carry1 = z4x5x4t227 >> 12
# asm 1: rotmi >carry1=vec128#83,<z4x5x4t227=vec128#64,-12
# asm 2: rotmi >carry1=$85,<z4x5x4t227=$66,-12
rotmi $85,$66,-12

# qhasm: z4x5x4t238 &= mask13
# asm 1: and >z4x5x4t238=vec128#73,<z4x5x4t238=vec128#73,<mask13=vec128#22
# asm 2: and >z4x5x4t238=$75,<z4x5x4t238=$75,<mask13=$24
and $75,$75,$24

# qhasm: uint32323232 carry2 = z4x5x4t231 >> 12
# asm 1: rotmi >carry2=vec128#84,<z4x5x4t231=vec128#70,-12
# asm 2: rotmi >carry2=$86,<z4x5x4t231=$72,-12
rotmi $86,$72,-12

# qhasm: uint32323232 carry3 = z4x5x4t235 >> 12
# asm 1: rotmi >carry3=vec128#85,<z4x5x4t235=vec128#71,-12
# asm 2: rotmi >carry3=$87,<z4x5x4t235=$73,-12
rotmi $87,$73,-12

# qhasm: int32323232 z4x5x4t228 += carry1
# asm 1: a >z4x5x4t228=vec128#67,<z4x5x4t228=vec128#67,<carry1=vec128#83
# asm 2: a >z4x5x4t228=$69,<z4x5x4t228=$69,<carry1=$85
a $69,$69,$85

# qhasm: z4x5x4t227 &= mask12
# asm 1: and >z4x5x4t227=vec128#64,<z4x5x4t227=vec128#64,<mask12=vec128#21
# asm 2: and >z4x5x4t227=$66,<z4x5x4t227=$66,<mask12=$23
and $66,$66,$23

# qhasm: int32323232 z4x5x4t232 += carry2
# asm 1: a >z4x5x4t232=vec128#69,<z4x5x4t232=vec128#69,<carry2=vec128#84
# asm 2: a >z4x5x4t232=$71,<z4x5x4t232=$71,<carry2=$86
a $71,$71,$86

# qhasm: z4x5x4t231 &= mask12
# asm 1: and >z4x5x4t231=vec128#70,<z4x5x4t231=vec128#70,<mask12=vec128#21
# asm 2: and >z4x5x4t231=$72,<z4x5x4t231=$72,<mask12=$23
and $72,$72,$23

# qhasm: int32323232 z4x5x4t236 += carry3
# asm 1: a >z4x5x4t236=vec128#72,<z4x5x4t236=vec128#72,<carry3=vec128#85
# asm 2: a >z4x5x4t236=$74,<z4x5x4t236=$74,<carry3=$87
a $74,$74,$87

# qhasm: z4x5x4t235 &= mask12
# asm 1: and >z4x5x4t235=vec128#71,<z4x5x4t235=vec128#71,<mask12=vec128#21
# asm 2: and >z4x5x4t235=$73,<z4x5x4t235=$73,<mask12=$23
and $73,$73,$23

# qhasm: int32323232 z4x5x4t20  += (z4x5x4t220 & 0xffff) * (vec19 & 0xffff)
# asm 1: mpya >z4x5x4t20=vec128#61,<z4x5x4t220=vec128#61,<vec19=vec128#74,<z4x5x4t20=vec128#86
# asm 2: mpya >z4x5x4t20=$63,<z4x5x4t220=$63,<vec19=$76,<z4x5x4t20=$88
mpya $63,$63,$76,$88

# qhasm: int32323232 z4x5x4t21  += (z4x5x4t221 & 0xffff) * (vec19 & 0xffff)
# asm 1: mpya >z4x5x4t21=vec128#5,<z4x5x4t221=vec128#5,<vec19=vec128#74,<z4x5x4t21=vec128#77
# asm 2: mpya >z4x5x4t21=$7,<z4x5x4t221=$7,<vec19=$76,<z4x5x4t21=$79
mpya $7,$7,$76,$79

# qhasm: int32323232 z4x5x4t22  += (z4x5x4t222 & 0xffff) * (vec19 & 0xffff)
# asm 1: mpya >z4x5x4t22=vec128#65,<z4x5x4t222=vec128#65,<vec19=vec128#74,<z4x5x4t22=vec128#78
# asm 2: mpya >z4x5x4t22=$67,<z4x5x4t222=$67,<vec19=$76,<z4x5x4t22=$80
mpya $67,$67,$76,$80

# qhasm: int32323232 z4x5x4t23  += (z4x5x4t223 & 0xffff) * (vec19 & 0xffff)
# asm 1: mpya >z4x5x4t23=vec128#66,<z4x5x4t223=vec128#66,<vec19=vec128#74,<z4x5x4t23=vec128#82
# asm 2: mpya >z4x5x4t23=$68,<z4x5x4t223=$68,<vec19=$76,<z4x5x4t23=$84
mpya $68,$68,$76,$84

# qhasm: int32323232 z4x5x4t24  += (z4x5x4t224 & 0xffff) * (vec19 & 0xffff)
# asm 1: mpya >z4x5x4t24=vec128#62,<z4x5x4t224=vec128#62,<vec19=vec128#74,<z4x5x4t24=vec128#79
# asm 2: mpya >z4x5x4t24=$64,<z4x5x4t224=$64,<vec19=$76,<z4x5x4t24=$81
mpya $64,$64,$76,$81

# qhasm: int32323232 z4x5x4t25  += (z4x5x4t225 & 0xffff) * (vec19 & 0xffff)
# asm 1: mpya >z4x5x4t25=vec128#55,<z4x5x4t225=vec128#55,<vec19=vec128#74,<z4x5x4t25=vec128#76
# asm 2: mpya >z4x5x4t25=$57,<z4x5x4t225=$57,<vec19=$76,<z4x5x4t25=$78
mpya $57,$57,$76,$78

# qhasm: int32323232 z4x5x4t26  += (z4x5x4t226 & 0xffff) * (vec19 & 0xffff)
# asm 1: mpya >z4x5x4t26=vec128#56,<z4x5x4t226=vec128#56,<vec19=vec128#74,<z4x5x4t26=vec128#102
# asm 2: mpya >z4x5x4t26=$58,<z4x5x4t226=$58,<vec19=$76,<z4x5x4t26=$104
mpya $58,$58,$76,$104

# qhasm: int32323232 z4x5x4t27  += (z4x5x4t227 & 0xffff) * (vec19 & 0xffff)
# asm 1: mpya >z4x5x4t27=vec128#64,<z4x5x4t227=vec128#64,<vec19=vec128#74,<z4x5x4t27=vec128#80
# asm 2: mpya >z4x5x4t27=$66,<z4x5x4t227=$66,<vec19=$76,<z4x5x4t27=$82
mpya $66,$66,$76,$82

# qhasm: int32323232 z4x5x4t28  += (z4x5x4t228 & 0xffff) * (vec19 & 0xffff)
# asm 1: mpya >z4x5x4t28=vec128#67,<z4x5x4t228=vec128#67,<vec19=vec128#74,<z4x5x4t28=vec128#104
# asm 2: mpya >z4x5x4t28=$69,<z4x5x4t228=$69,<vec19=$76,<z4x5x4t28=$106
mpya $69,$69,$76,$106

# qhasm: int32323232 z4x5x4t29  += (z4x5x4t229 & 0xffff) * (vec19 & 0xffff)
# asm 1: mpya >z4x5x4t29=vec128#60,<z4x5x4t229=vec128#60,<vec19=vec128#74,<z4x5x4t29=vec128#105
# asm 2: mpya >z4x5x4t29=$62,<z4x5x4t229=$62,<vec19=$76,<z4x5x4t29=$107
mpya $62,$62,$76,$107

# qhasm: int32323232 z4x5x4t210 += (z4x5x4t230 & 0xffff) * (vec19 & 0xffff)
# asm 1: mpya >z4x5x4t210=vec128#68,<z4x5x4t230=vec128#68,<vec19=vec128#74,<z4x5x4t210=vec128#106
# asm 2: mpya >z4x5x4t210=$70,<z4x5x4t230=$70,<vec19=$76,<z4x5x4t210=$108
mpya $70,$70,$76,$108

# qhasm: int32323232 z4x5x4t211 += (z4x5x4t231 & 0xffff) * (vec19 & 0xffff)
# asm 1: mpya >z4x5x4t211=vec128#70,<z4x5x4t231=vec128#70,<vec19=vec128#74,<z4x5x4t211=vec128#107
# asm 2: mpya >z4x5x4t211=$72,<z4x5x4t231=$72,<vec19=$76,<z4x5x4t211=$109
mpya $72,$72,$76,$109

# qhasm: int32323232 z4x5x4t212 += (z4x5x4t232 & 0xffff) * (vec19 & 0xffff)
# asm 1: mpya >z4x5x4t212=vec128#69,<z4x5x4t232=vec128#69,<vec19=vec128#74,<z4x5x4t212=vec128#108
# asm 2: mpya >z4x5x4t212=$71,<z4x5x4t232=$71,<vec19=$76,<z4x5x4t212=$110
mpya $71,$71,$76,$110

# qhasm: int32323232 z4x5x4t213 += (z4x5x4t233 & 0xffff) * (vec19 & 0xffff)
# asm 1: mpya >z4x5x4t213=vec128#57,<z4x5x4t233=vec128#57,<vec19=vec128#74,<z4x5x4t213=vec128#109
# asm 2: mpya >z4x5x4t213=$59,<z4x5x4t233=$59,<vec19=$76,<z4x5x4t213=$111
mpya $59,$59,$76,$111

# qhasm: int32323232 z4x5x4t214 += (z4x5x4t234 & 0xffff) * (vec19 & 0xffff)
# asm 1: mpya >z4x5x4t214=vec128#59,<z4x5x4t234=vec128#59,<vec19=vec128#74,<z4x5x4t214=vec128#110
# asm 2: mpya >z4x5x4t214=$61,<z4x5x4t234=$61,<vec19=$76,<z4x5x4t214=$112
mpya $61,$61,$76,$112

# qhasm: int32323232 z4x5x4t215 += (z4x5x4t235 & 0xffff) * (vec19 & 0xffff)
# asm 1: mpya >z4x5x4t215=vec128#71,<z4x5x4t235=vec128#71,<vec19=vec128#74,<z4x5x4t215=vec128#111
# asm 2: mpya >z4x5x4t215=$73,<z4x5x4t235=$73,<vec19=$76,<z4x5x4t215=$113
mpya $73,$73,$76,$113

# qhasm: int32323232 z4x5x4t216 += (z4x5x4t236 & 0xffff) * (vec19 & 0xffff)
# asm 1: mpya >z4x5x4t216=vec128#58,<z4x5x4t236=vec128#72,<vec19=vec128#74,<z4x5x4t216=vec128#58
# asm 2: mpya >z4x5x4t216=$60,<z4x5x4t236=$74,<vec19=$76,<z4x5x4t216=$60
mpya $60,$74,$76,$60

# qhasm: int32323232 z4x5x4t217 += (z4x5x4t237 & 0xffff) * (vec19 & 0xffff)
# asm 1: mpya >z4x5x4t217=vec128#63,<z4x5x4t237=vec128#81,<vec19=vec128#74,<z4x5x4t217=vec128#63
# asm 2: mpya >z4x5x4t217=$65,<z4x5x4t237=$83,<vec19=$76,<z4x5x4t217=$65
mpya $65,$83,$76,$65

# qhasm: int32323232 z4x5x4t218 += (z4x5x4t238 & 0xffff) * (vec19 & 0xffff)
# asm 1: mpya >z4x5x4t218=vec128#72,<z4x5x4t238=vec128#73,<vec19=vec128#74,<z4x5x4t218=vec128#112
# asm 2: mpya >z4x5x4t218=$74,<z4x5x4t238=$75,<vec19=$76,<z4x5x4t218=$114
mpya $74,$75,$76,$114

# qhasm: int32323232 z4x5x4t219 += (z4x5x4t239 & 0xffff) * (vec19 & 0xffff)
# asm 1: mpya >z4x5x4t219=vec128#73,<z4x5x4t239=vec128#87,<vec19=vec128#74,<z4x5x4t219=vec128#96
# asm 2: mpya >z4x5x4t219=$75,<z4x5x4t239=$89,<vec19=$76,<z4x5x4t219=$98
mpya $75,$89,$76,$98

# qhasm: uint32323232 carry = z4x5x4t216 >> 13
# asm 1: rotmi >carry=vec128#76,<z4x5x4t216=vec128#58,-13
# asm 2: rotmi >carry=$78,<z4x5x4t216=$60,-13
rotmi $78,$60,-13

# qhasm: int32323232 z4x5x4t217 += carry
# asm 1: a >z4x5x4t217=vec128#63,<z4x5x4t217=vec128#63,<carry=vec128#76
# asm 2: a >z4x5x4t217=$65,<z4x5x4t217=$65,<carry=$78
a $65,$65,$78

# qhasm: uint32323232 carry = z4x5x4t217 >> 13
# asm 1: rotmi >carry=vec128#76,<z4x5x4t217=vec128#63,-13
# asm 2: rotmi >carry=$78,<z4x5x4t217=$65,-13
rotmi $78,$65,-13

# qhasm: int32323232 z4x5x4t218 += carry
# asm 1: a >z4x5x4t218=vec128#72,<z4x5x4t218=vec128#72,<carry=vec128#76
# asm 2: a >z4x5x4t218=$74,<z4x5x4t218=$74,<carry=$78
a $74,$74,$78

# qhasm: uint32323232 carry = z4x5x4t218 >> 13
# asm 1: rotmi >carry=vec128#76,<z4x5x4t218=vec128#72,-13
# asm 2: rotmi >carry=$78,<z4x5x4t218=$74,-13
rotmi $78,$74,-13

# qhasm: int32323232 z4x5x4t219 += carry
# asm 1: a >z4x5x4t219=vec128#73,<z4x5x4t219=vec128#73,<carry=vec128#76
# asm 2: a >z4x5x4t219=$75,<z4x5x4t219=$75,<carry=$78
a $75,$75,$78

# qhasm: uint32323232 carry = z4x5x4t219 >> 12
# asm 1: rotmi >carry=vec128#76,<z4x5x4t219=vec128#73,-12
# asm 2: rotmi >carry=$78,<z4x5x4t219=$75,-12
rotmi $78,$75,-12

# qhasm: int32323232 red = carry << 4
# asm 1: shli >red=vec128#77,<carry=vec128#76,4
# asm 2: shli >red=$79,<carry=$78,4
shli $79,$78,4

# qhasm: int32323232 red += carry
# asm 1: a >red=vec128#77,<red=vec128#77,<carry=vec128#76
# asm 2: a >red=$79,<red=$79,<carry=$78
a $79,$79,$78

# qhasm: int32323232 red += carry
# asm 1: a >red=vec128#77,<red=vec128#77,<carry=vec128#76
# asm 2: a >red=$79,<red=$79,<carry=$78
a $79,$79,$78

# qhasm: int32323232 red += carry
# asm 1: a >red=vec128#76,<red=vec128#77,<carry=vec128#76
# asm 2: a >red=$78,<red=$79,<carry=$78
a $78,$79,$78

# qhasm: int32323232 z4x5x4t20 += red
# asm 1: a >z4x5x4t20=vec128#61,<z4x5x4t20=vec128#61,<red=vec128#76
# asm 2: a >z4x5x4t20=$63,<z4x5x4t20=$63,<red=$78
a $63,$63,$78

# qhasm: z4x5x4t219 &= mask12
# asm 1: and >z4x5x4t219=vec128#73,<z4x5x4t219=vec128#73,<mask12=vec128#21
# asm 2: and >z4x5x4t219=$75,<z4x5x4t219=$75,<mask12=$23
and $75,$75,$23

# qhasm: z4x5x4t216 &= mask13
# asm 1: and >z4x5x4t216=vec128#58,<z4x5x4t216=vec128#58,<mask13=vec128#22
# asm 2: and >z4x5x4t216=$60,<z4x5x4t216=$60,<mask13=$24
and $60,$60,$24

# qhasm: z4x5x4t217 &= mask13
# asm 1: and >z4x5x4t217=vec128#63,<z4x5x4t217=vec128#63,<mask13=vec128#22
# asm 2: and >z4x5x4t217=$65,<z4x5x4t217=$65,<mask13=$24
and $65,$65,$24

# qhasm: z4x5x4t218 &= mask13
# asm 1: and >z4x5x4t218=vec128#72,<z4x5x4t218=vec128#72,<mask13=vec128#22
# asm 2: and >z4x5x4t218=$74,<z4x5x4t218=$74,<mask13=$24
and $74,$74,$24

# qhasm: z4x5x4t219 &= mask12
# asm 1: and >z4x5x4t219=vec128#73,<z4x5x4t219=vec128#73,<mask12=vec128#21
# asm 2: and >z4x5x4t219=$75,<z4x5x4t219=$75,<mask12=$23
and $75,$75,$23

# qhasm: uint32323232 carry0 = z4x5x4t20  >> 13
# asm 1: rotmi >carry0=vec128#76,<z4x5x4t20=vec128#61,-13
# asm 2: rotmi >carry0=$78,<z4x5x4t20=$63,-13
rotmi $78,$63,-13

# qhasm: uint32323232 carry1 = z4x5x4t24  >> 13
# asm 1: rotmi >carry1=vec128#77,<z4x5x4t24=vec128#62,-13
# asm 2: rotmi >carry1=$79,<z4x5x4t24=$64,-13
rotmi $79,$64,-13

# qhasm: uint32323232 carry2 = z4x5x4t28  >> 13
# asm 1: rotmi >carry2=vec128#78,<z4x5x4t28=vec128#67,-13
# asm 2: rotmi >carry2=$80,<z4x5x4t28=$69,-13
rotmi $80,$69,-13

# qhasm: uint32323232 carry3 = z4x5x4t212 >> 13
# asm 1: rotmi >carry3=vec128#79,<z4x5x4t212=vec128#69,-13
# asm 2: rotmi >carry3=$81,<z4x5x4t212=$71,-13
rotmi $81,$71,-13

# qhasm: int32323232 z4x5x4t21  += carry0
# asm 1: a >z4x5x4t21=vec128#5,<z4x5x4t21=vec128#5,<carry0=vec128#76
# asm 2: a >z4x5x4t21=$7,<z4x5x4t21=$7,<carry0=$78
a $7,$7,$78

# qhasm: z4x5x4t20  &= mask13
# asm 1: and >z4x5x4t20=vec128#76,<z4x5x4t20=vec128#61,<mask13=vec128#22
# asm 2: and >z4x5x4t20=$78,<z4x5x4t20=$63,<mask13=$24
and $78,$63,$24

# qhasm: int32323232 z4x5x4t25  += carry1
# asm 1: a >z4x5x4t25=vec128#55,<z4x5x4t25=vec128#55,<carry1=vec128#77
# asm 2: a >z4x5x4t25=$57,<z4x5x4t25=$57,<carry1=$79
a $57,$57,$79

# qhasm: z4x5x4t24  &= mask13
# asm 1: and >z4x5x4t24=vec128#61,<z4x5x4t24=vec128#62,<mask13=vec128#22
# asm 2: and >z4x5x4t24=$63,<z4x5x4t24=$64,<mask13=$24
and $63,$64,$24

# qhasm: int32323232 z4x5x4t29  += carry2
# asm 1: a >z4x5x4t29=vec128#60,<z4x5x4t29=vec128#60,<carry2=vec128#78
# asm 2: a >z4x5x4t29=$62,<z4x5x4t29=$62,<carry2=$80
a $62,$62,$80

# qhasm: z4x5x4t28  &= mask13
# asm 1: and >z4x5x4t28=vec128#62,<z4x5x4t28=vec128#67,<mask13=vec128#22
# asm 2: and >z4x5x4t28=$64,<z4x5x4t28=$69,<mask13=$24
and $64,$69,$24

# qhasm: int32323232 z4x5x4t213 += carry3
# asm 1: a >z4x5x4t213=vec128#57,<z4x5x4t213=vec128#57,<carry3=vec128#79
# asm 2: a >z4x5x4t213=$59,<z4x5x4t213=$59,<carry3=$81
a $59,$59,$81

# qhasm: z4x5x4t212 &= mask13
# asm 1: and >z4x5x4t212=vec128#67,<z4x5x4t212=vec128#69,<mask13=vec128#22
# asm 2: and >z4x5x4t212=$69,<z4x5x4t212=$71,<mask13=$24
and $69,$71,$24

# qhasm: uint32323232 carry0 = z4x5x4t21  >> 13
# asm 1: rotmi >carry0=vec128#69,<z4x5x4t21=vec128#5,-13
# asm 2: rotmi >carry0=$71,<z4x5x4t21=$7,-13
rotmi $71,$7,-13

# qhasm: uint32323232 carry1 = z4x5x4t25  >> 13
# asm 1: rotmi >carry1=vec128#77,<z4x5x4t25=vec128#55,-13
# asm 2: rotmi >carry1=$79,<z4x5x4t25=$57,-13
rotmi $79,$57,-13

# qhasm: uint32323232 carry2 = z4x5x4t29  >> 13
# asm 1: rotmi >carry2=vec128#78,<z4x5x4t29=vec128#60,-13
# asm 2: rotmi >carry2=$80,<z4x5x4t29=$62,-13
rotmi $80,$62,-13

# qhasm: uint32323232 carry3 = z4x5x4t213 >> 13
# asm 1: rotmi >carry3=vec128#79,<z4x5x4t213=vec128#57,-13
# asm 2: rotmi >carry3=$81,<z4x5x4t213=$59,-13
rotmi $81,$59,-13

# qhasm: int32323232 z4x5x4t22  += carry0
# asm 1: a >z4x5x4t22=vec128#65,<z4x5x4t22=vec128#65,<carry0=vec128#69
# asm 2: a >z4x5x4t22=$67,<z4x5x4t22=$67,<carry0=$71
a $67,$67,$71

# qhasm: z4x5x4t21  &= mask13
# asm 1: and >z4x5x4t21=vec128#5,<z4x5x4t21=vec128#5,<mask13=vec128#22
# asm 2: and >z4x5x4t21=$7,<z4x5x4t21=$7,<mask13=$24
and $7,$7,$24

# qhasm: int32323232 z4x5x4t26  += carry1
# asm 1: a >z4x5x4t26=vec128#56,<z4x5x4t26=vec128#56,<carry1=vec128#77
# asm 2: a >z4x5x4t26=$58,<z4x5x4t26=$58,<carry1=$79
a $58,$58,$79

# qhasm: z4x5x4t25  &= mask13
# asm 1: and >z4x5x4t25=vec128#55,<z4x5x4t25=vec128#55,<mask13=vec128#22
# asm 2: and >z4x5x4t25=$57,<z4x5x4t25=$57,<mask13=$24
and $57,$57,$24

# qhasm: int32323232 z4x5x4t210 += carry2
# asm 1: a >z4x5x4t210=vec128#68,<z4x5x4t210=vec128#68,<carry2=vec128#78
# asm 2: a >z4x5x4t210=$70,<z4x5x4t210=$70,<carry2=$80
a $70,$70,$80

# qhasm: z4x5x4t29  &= mask13
# asm 1: and >z4x5x4t29=vec128#60,<z4x5x4t29=vec128#60,<mask13=vec128#22
# asm 2: and >z4x5x4t29=$62,<z4x5x4t29=$62,<mask13=$24
and $62,$62,$24

# qhasm: int32323232 z4x5x4t214 += carry3
# asm 1: a >z4x5x4t214=vec128#59,<z4x5x4t214=vec128#59,<carry3=vec128#79
# asm 2: a >z4x5x4t214=$61,<z4x5x4t214=$61,<carry3=$81
a $61,$61,$81

# qhasm: z4x5x4t213 &= mask13
# asm 1: and >z4x5x4t213=vec128#57,<z4x5x4t213=vec128#57,<mask13=vec128#22
# asm 2: and >z4x5x4t213=$59,<z4x5x4t213=$59,<mask13=$24
and $59,$59,$24

# qhasm: uint32323232 carry0 = z4x5x4t22  >> 13
# asm 1: rotmi >carry0=vec128#69,<z4x5x4t22=vec128#65,-13
# asm 2: rotmi >carry0=$71,<z4x5x4t22=$67,-13
rotmi $71,$67,-13

# qhasm: uint32323232 carry1 = z4x5x4t26  >> 13
# asm 1: rotmi >carry1=vec128#77,<z4x5x4t26=vec128#56,-13
# asm 2: rotmi >carry1=$79,<z4x5x4t26=$58,-13
rotmi $79,$58,-13

# qhasm: uint32323232 carry2 = z4x5x4t210 >> 13
# asm 1: rotmi >carry2=vec128#78,<z4x5x4t210=vec128#68,-13
# asm 2: rotmi >carry2=$80,<z4x5x4t210=$70,-13
rotmi $80,$70,-13

# qhasm: uint32323232 carry3 = z4x5x4t214 >> 13
# asm 1: rotmi >carry3=vec128#79,<z4x5x4t214=vec128#59,-13
# asm 2: rotmi >carry3=$81,<z4x5x4t214=$61,-13
rotmi $81,$61,-13

# qhasm: int32323232 z4x5x4t23  += carry0
# asm 1: a >z4x5x4t23=vec128#66,<z4x5x4t23=vec128#66,<carry0=vec128#69
# asm 2: a >z4x5x4t23=$68,<z4x5x4t23=$68,<carry0=$71
a $68,$68,$71

# qhasm: z4x5x4t22  &= mask13
# asm 1: and >z4x5x4t22=vec128#65,<z4x5x4t22=vec128#65,<mask13=vec128#22
# asm 2: and >z4x5x4t22=$67,<z4x5x4t22=$67,<mask13=$24
and $67,$67,$24

# qhasm: int32323232 z4x5x4t27  += carry1
# asm 1: a >z4x5x4t27=vec128#64,<z4x5x4t27=vec128#64,<carry1=vec128#77
# asm 2: a >z4x5x4t27=$66,<z4x5x4t27=$66,<carry1=$79
a $66,$66,$79

# qhasm: z4x5x4t26  &= mask13
# asm 1: and >z4x5x4t26=vec128#56,<z4x5x4t26=vec128#56,<mask13=vec128#22
# asm 2: and >z4x5x4t26=$58,<z4x5x4t26=$58,<mask13=$24
and $58,$58,$24

# qhasm: int32323232 z4x5x4t211 += carry2
# asm 1: a >z4x5x4t211=vec128#69,<z4x5x4t211=vec128#70,<carry2=vec128#78
# asm 2: a >z4x5x4t211=$71,<z4x5x4t211=$72,<carry2=$80
a $71,$72,$80

# qhasm: z4x5x4t210 &= mask13
# asm 1: and >z4x5x4t210=vec128#68,<z4x5x4t210=vec128#68,<mask13=vec128#22
# asm 2: and >z4x5x4t210=$70,<z4x5x4t210=$70,<mask13=$24
and $70,$70,$24

# qhasm: int32323232 z4x5x4t215 += carry3
# asm 1: a >z4x5x4t215=vec128#70,<z4x5x4t215=vec128#71,<carry3=vec128#79
# asm 2: a >z4x5x4t215=$72,<z4x5x4t215=$73,<carry3=$81
a $72,$73,$81

# qhasm: z4x5x4t214 &= mask13
# asm 1: and >z4x5x4t214=vec128#59,<z4x5x4t214=vec128#59,<mask13=vec128#22
# asm 2: and >z4x5x4t214=$61,<z4x5x4t214=$61,<mask13=$24
and $61,$61,$24

# qhasm: uint32323232 carry0 = z4x5x4t23  >> 12
# asm 1: rotmi >carry0=vec128#71,<z4x5x4t23=vec128#66,-12
# asm 2: rotmi >carry0=$73,<z4x5x4t23=$68,-12
rotmi $73,$68,-12

# qhasm: uint32323232 carry1 = z4x5x4t27  >> 12
# asm 1: rotmi >carry1=vec128#77,<z4x5x4t27=vec128#64,-12
# asm 2: rotmi >carry1=$79,<z4x5x4t27=$66,-12
rotmi $79,$66,-12

# qhasm: uint32323232 carry2 = z4x5x4t211 >> 12
# asm 1: rotmi >carry2=vec128#78,<z4x5x4t211=vec128#69,-12
# asm 2: rotmi >carry2=$80,<z4x5x4t211=$71,-12
rotmi $80,$71,-12

# qhasm: uint32323232 carry3 = z4x5x4t215 >> 12
# asm 1: rotmi >carry3=vec128#79,<z4x5x4t215=vec128#70,-12
# asm 2: rotmi >carry3=$81,<z4x5x4t215=$72,-12
rotmi $81,$72,-12

# qhasm: z4x5x4t23  &= mask12
# asm 1: and >z4x5x4t23=vec128#66,<z4x5x4t23=vec128#66,<mask12=vec128#21
# asm 2: and >z4x5x4t23=$68,<z4x5x4t23=$68,<mask12=$23
and $68,$68,$23

# qhasm: z4x5x4t27  &= mask12
# asm 1: and >z4x5x4t27=vec128#64,<z4x5x4t27=vec128#64,<mask12=vec128#21
# asm 2: and >z4x5x4t27=$66,<z4x5x4t27=$66,<mask12=$23
and $66,$66,$23

# qhasm: z4x5x4t211 &= mask12
# asm 1: and >z4x5x4t211=vec128#69,<z4x5x4t211=vec128#69,<mask12=vec128#21
# asm 2: and >z4x5x4t211=$71,<z4x5x4t211=$71,<mask12=$23
and $71,$71,$23

# qhasm: t20 = select bytes from z4x5x4t20 by selw3333
# asm 1: shufb >t20=vec128#80,<z4x5x4t20=vec128#76,<z4x5x4t20=vec128#76,<selw3333=vec128#30
# asm 2: shufb >t20=$82,<z4x5x4t20=$78,<z4x5x4t20=$78,<selw3333=$32
shufb $82,$78,$78,$32

# qhasm: z4x5x4t215 &= mask12
# asm 1: and >z4x5x4t215=vec128#70,<z4x5x4t215=vec128#70,<mask12=vec128#21
# asm 2: and >z4x5x4t215=$72,<z4x5x4t215=$72,<mask12=$23
and $72,$72,$23

# qhasm: t21 = select bytes from z4x5x4t21 by selw3333
# asm 1: shufb >t21=vec128#81,<z4x5x4t21=vec128#5,<z4x5x4t21=vec128#5,<selw3333=vec128#30
# asm 2: shufb >t21=$83,<z4x5x4t21=$7,<z4x5x4t21=$7,<selw3333=$32
shufb $83,$7,$7,$32

# qhasm: int32323232 z4x5x4t24  += carry0
# asm 1: a >z4x5x4t24=vec128#61,<z4x5x4t24=vec128#61,<carry0=vec128#71
# asm 2: a >z4x5x4t24=$63,<z4x5x4t24=$63,<carry0=$73
a $63,$63,$73

# qhasm: t22 = select bytes from z4x5x4t22 by selw3333
# asm 1: shufb >t22=vec128#71,<z4x5x4t22=vec128#65,<z4x5x4t22=vec128#65,<selw3333=vec128#30
# asm 2: shufb >t22=$73,<z4x5x4t22=$67,<z4x5x4t22=$67,<selw3333=$32
shufb $73,$67,$67,$32

# qhasm: int32323232 z4x5x4t28  += carry1
# asm 1: a >z4x5x4t28=vec128#62,<z4x5x4t28=vec128#62,<carry1=vec128#77
# asm 2: a >z4x5x4t28=$64,<z4x5x4t28=$64,<carry1=$79
a $64,$64,$79

# qhasm: t23 = select bytes from z4x5x4t23 by selw3333
# asm 1: shufb >t23=vec128#77,<z4x5x4t23=vec128#66,<z4x5x4t23=vec128#66,<selw3333=vec128#30
# asm 2: shufb >t23=$79,<z4x5x4t23=$68,<z4x5x4t23=$68,<selw3333=$32
shufb $79,$68,$68,$32

# qhasm: int32323232 z4x5x4t212 += carry2
# asm 1: a >z4x5x4t212=vec128#67,<z4x5x4t212=vec128#67,<carry2=vec128#78
# asm 2: a >z4x5x4t212=$69,<z4x5x4t212=$69,<carry2=$80
a $69,$69,$80

# qhasm: int32323232 z4x5x4t216 += carry3
# asm 1: a >z4x5x4t216=vec128#58,<z4x5x4t216=vec128#58,<carry3=vec128#79
# asm 2: a >z4x5x4t216=$60,<z4x5x4t216=$60,<carry3=$81
a $60,$60,$81

# qhasm: uint32323232 carry1 = z4x5x4t24  >> 13
# asm 1: rotmi >carry1=vec128#78,<z4x5x4t24=vec128#61,-13
# asm 2: rotmi >carry1=$80,<z4x5x4t24=$63,-13
rotmi $80,$63,-13

# qhasm: uint32323232 carry2 = z4x5x4t28  >> 13
# asm 1: rotmi >carry2=vec128#79,<z4x5x4t28=vec128#62,-13
# asm 2: rotmi >carry2=$81,<z4x5x4t28=$64,-13
rotmi $81,$64,-13

# qhasm: uint32323232 carry3 = z4x5x4t212 >> 13
# asm 1: rotmi >carry3=vec128#82,<z4x5x4t212=vec128#67,-13
# asm 2: rotmi >carry3=$84,<z4x5x4t212=$69,-13
rotmi $84,$69,-13

# qhasm: uint32323232 carry4 = z4x5x4t216 >> 13
# asm 1: rotmi >carry4=vec128#83,<z4x5x4t216=vec128#58,-13
# asm 2: rotmi >carry4=$85,<z4x5x4t216=$60,-13
rotmi $85,$60,-13

# qhasm: z4x5x4t24  &= mask13
# asm 1: and >z4x5x4t24=vec128#84,<z4x5x4t24=vec128#61,<mask13=vec128#22
# asm 2: and >z4x5x4t24=$86,<z4x5x4t24=$63,<mask13=$24
and $86,$63,$24

# qhasm: z4x5x4t28  &= mask13
# asm 1: and >z4x5x4t28=vec128#85,<z4x5x4t28=vec128#62,<mask13=vec128#22
# asm 2: and >z4x5x4t28=$87,<z4x5x4t28=$64,<mask13=$24
and $87,$64,$24

# qhasm: z4x5x4t212 &= mask13
# asm 1: and >z4x5x4t212=vec128#67,<z4x5x4t212=vec128#67,<mask13=vec128#22
# asm 2: and >z4x5x4t212=$69,<z4x5x4t212=$69,<mask13=$24
and $69,$69,$24

# qhasm: t24 = select bytes from z4x5x4t24 by selw3333
# asm 1: shufb >t24=vec128#86,<z4x5x4t24=vec128#84,<z4x5x4t24=vec128#84,<selw3333=vec128#30
# asm 2: shufb >t24=$88,<z4x5x4t24=$86,<z4x5x4t24=$86,<selw3333=$32
shufb $88,$86,$86,$32

# qhasm: z4x5x4t216 &= mask13
# asm 1: and >z4x5x4t216=vec128#87,<z4x5x4t216=vec128#58,<mask13=vec128#22
# asm 2: and >z4x5x4t216=$89,<z4x5x4t216=$60,<mask13=$24
and $89,$60,$24

# qhasm: t28 = select bytes from z4x5x4t28 by selw3333
# asm 1: shufb >t28=vec128#88,<z4x5x4t28=vec128#85,<z4x5x4t28=vec128#85,<selw3333=vec128#30
# asm 2: shufb >t28=$90,<z4x5x4t28=$87,<z4x5x4t28=$87,<selw3333=$32
shufb $90,$87,$87,$32

# qhasm: int32323232 z4x5x4t25  += carry1
# asm 1: a >z4x5x4t25=vec128#55,<z4x5x4t25=vec128#55,<carry1=vec128#78
# asm 2: a >z4x5x4t25=$57,<z4x5x4t25=$57,<carry1=$80
a $57,$57,$80

# qhasm: t212 = select bytes from z4x5x4t212 by selw3333
# asm 1: shufb >t212=vec128#78,<z4x5x4t212=vec128#67,<z4x5x4t212=vec128#67,<selw3333=vec128#30
# asm 2: shufb >t212=$80,<z4x5x4t212=$69,<z4x5x4t212=$69,<selw3333=$32
shufb $80,$69,$69,$32

# qhasm: int32323232 z4x5x4t29  += carry2
# asm 1: a >z4x5x4t29=vec128#58,<z4x5x4t29=vec128#60,<carry2=vec128#79
# asm 2: a >z4x5x4t29=$60,<z4x5x4t29=$62,<carry2=$81
a $60,$62,$81

# qhasm: t216 = select bytes from z4x5x4t216 by selw3333
# asm 1: shufb >t216=vec128#79,<z4x5x4t216=vec128#87,<z4x5x4t216=vec128#87,<selw3333=vec128#30
# asm 2: shufb >t216=$81,<z4x5x4t216=$89,<z4x5x4t216=$89,<selw3333=$32
shufb $81,$89,$89,$32

# qhasm: int32323232 z4x5x4t213 += carry3
# asm 1: a >z4x5x4t213=vec128#57,<z4x5x4t213=vec128#57,<carry3=vec128#82
# asm 2: a >z4x5x4t213=$59,<z4x5x4t213=$59,<carry3=$84
a $59,$59,$84

# qhasm: int32323232 z4x5x4t217 += carry4
# asm 1: a >z4x5x4t217=vec128#60,<z4x5x4t217=vec128#63,<carry4=vec128#83
# asm 2: a >z4x5x4t217=$62,<z4x5x4t217=$65,<carry4=$85
a $62,$65,$85

# qhasm: uint32323232 carry1 = z4x5x4t25  >> 13
# asm 1: rotmi >carry1=vec128#61,<z4x5x4t25=vec128#55,-13
# asm 2: rotmi >carry1=$63,<z4x5x4t25=$57,-13
rotmi $63,$57,-13

# qhasm: uint32323232 carry2 = z4x5x4t29  >> 13
# asm 1: rotmi >carry2=vec128#62,<z4x5x4t29=vec128#58,-13
# asm 2: rotmi >carry2=$64,<z4x5x4t29=$60,-13
rotmi $64,$60,-13

# qhasm: uint32323232 carry3 = z4x5x4t213 >> 13
# asm 1: rotmi >carry3=vec128#63,<z4x5x4t213=vec128#57,-13
# asm 2: rotmi >carry3=$65,<z4x5x4t213=$59,-13
rotmi $65,$59,-13

# qhasm: uint32323232 carry4 = z4x5x4t217 >> 13
# asm 1: rotmi >carry4=vec128#82,<z4x5x4t217=vec128#60,-13
# asm 2: rotmi >carry4=$84,<z4x5x4t217=$62,-13
rotmi $84,$62,-13

# qhasm: z4x5x4t25  &= mask13
# asm 1: and >z4x5x4t25=vec128#83,<z4x5x4t25=vec128#55,<mask13=vec128#22
# asm 2: and >z4x5x4t25=$85,<z4x5x4t25=$57,<mask13=$24
and $85,$57,$24

# qhasm: z4x5x4t29  &= mask13
# asm 1: and >z4x5x4t29=vec128#89,<z4x5x4t29=vec128#58,<mask13=vec128#22
# asm 2: and >z4x5x4t29=$91,<z4x5x4t29=$60,<mask13=$24
and $91,$60,$24

# qhasm: z4x5x4t213 &= mask13
# asm 1: and >z4x5x4t213=vec128#90,<z4x5x4t213=vec128#57,<mask13=vec128#22
# asm 2: and >z4x5x4t213=$92,<z4x5x4t213=$59,<mask13=$24
and $92,$59,$24

# qhasm: t25 = select bytes from z4x5x4t25 by selw3333
# asm 1: shufb >t25=vec128#91,<z4x5x4t25=vec128#83,<z4x5x4t25=vec128#83,<selw3333=vec128#30
# asm 2: shufb >t25=$93,<z4x5x4t25=$85,<z4x5x4t25=$85,<selw3333=$32
shufb $93,$85,$85,$32

# qhasm: z4x5x4t217 &= mask13
# asm 1: and >z4x5x4t217=vec128#92,<z4x5x4t217=vec128#60,<mask13=vec128#22
# asm 2: and >z4x5x4t217=$94,<z4x5x4t217=$62,<mask13=$24
and $94,$62,$24

# qhasm: t29 = select bytes from z4x5x4t29 by selw3333
# asm 1: shufb >t29=vec128#93,<z4x5x4t29=vec128#89,<z4x5x4t29=vec128#89,<selw3333=vec128#30
# asm 2: shufb >t29=$95,<z4x5x4t29=$91,<z4x5x4t29=$91,<selw3333=$32
shufb $95,$91,$91,$32

# qhasm: int32323232 z4x5x4t26  += carry1
# asm 1: a >z4x5x4t26=vec128#55,<z4x5x4t26=vec128#56,<carry1=vec128#61
# asm 2: a >z4x5x4t26=$57,<z4x5x4t26=$58,<carry1=$63
a $57,$58,$63

# qhasm: t213 = select bytes from z4x5x4t213 by selw3333
# asm 1: shufb >t213=vec128#94,<z4x5x4t213=vec128#90,<z4x5x4t213=vec128#90,<selw3333=vec128#30
# asm 2: shufb >t213=$96,<z4x5x4t213=$92,<z4x5x4t213=$92,<selw3333=$32
shufb $96,$92,$92,$32

# qhasm: int32323232 z4x5x4t210 += carry2
# asm 1: a >z4x5x4t210=vec128#56,<z4x5x4t210=vec128#68,<carry2=vec128#62
# asm 2: a >z4x5x4t210=$58,<z4x5x4t210=$70,<carry2=$64
a $58,$70,$64

# qhasm: t217 = select bytes from z4x5x4t217 by selw3333
# asm 1: shufb >t217=vec128#95,<z4x5x4t217=vec128#92,<z4x5x4t217=vec128#92,<selw3333=vec128#30
# asm 2: shufb >t217=$97,<z4x5x4t217=$94,<z4x5x4t217=$94,<selw3333=$32
shufb $97,$94,$94,$32

# qhasm: int32323232 z4x5x4t214 += carry3
# asm 1: a >z4x5x4t214=vec128#57,<z4x5x4t214=vec128#59,<carry3=vec128#63
# asm 2: a >z4x5x4t214=$59,<z4x5x4t214=$61,<carry3=$65
a $59,$61,$65

# qhasm: int32323232 z4x5x4t218 += carry4
# asm 1: a >z4x5x4t218=vec128#58,<z4x5x4t218=vec128#72,<carry4=vec128#82
# asm 2: a >z4x5x4t218=$60,<z4x5x4t218=$74,<carry4=$84
a $60,$74,$84

# qhasm: uint32323232 carry1 = z4x5x4t26  >> 13
# asm 1: rotmi >carry1=vec128#59,<z4x5x4t26=vec128#55,-13
# asm 2: rotmi >carry1=$61,<z4x5x4t26=$57,-13
rotmi $61,$57,-13

# qhasm: uint32323232 carry2 = z4x5x4t210 >> 13
# asm 1: rotmi >carry2=vec128#60,<z4x5x4t210=vec128#56,-13
# asm 2: rotmi >carry2=$62,<z4x5x4t210=$58,-13
rotmi $62,$58,-13

# qhasm: uint32323232 carry3 = z4x5x4t214 >> 13
# asm 1: rotmi >carry3=vec128#61,<z4x5x4t214=vec128#57,-13
# asm 2: rotmi >carry3=$63,<z4x5x4t214=$59,-13
rotmi $63,$59,-13

# qhasm: uint32323232 carry4 = z4x5x4t218  >> 13
# asm 1: rotmi >carry4=vec128#62,<z4x5x4t218=vec128#58,-13
# asm 2: rotmi >carry4=$64,<z4x5x4t218=$60,-13
rotmi $64,$60,-13

# qhasm: z4x5x4t26  &= mask13
# asm 1: and >z4x5x4t26=vec128#68,<z4x5x4t26=vec128#55,<mask13=vec128#22
# asm 2: and >z4x5x4t26=$70,<z4x5x4t26=$57,<mask13=$24
and $70,$57,$24

# qhasm: z4x5x4t210 &= mask13
# asm 1: and >z4x5x4t210=vec128#72,<z4x5x4t210=vec128#56,<mask13=vec128#22
# asm 2: and >z4x5x4t210=$74,<z4x5x4t210=$58,<mask13=$24
and $74,$58,$24

# qhasm: z4x5x4t214 &= mask13
# asm 1: and >z4x5x4t214=vec128#82,<z4x5x4t214=vec128#57,<mask13=vec128#22
# asm 2: and >z4x5x4t214=$84,<z4x5x4t214=$59,<mask13=$24
and $84,$59,$24

# qhasm: t26 = select bytes from z4x5x4t26 by selw3333
# asm 1: shufb >t26=vec128#96,<z4x5x4t26=vec128#68,<z4x5x4t26=vec128#68,<selw3333=vec128#30
# asm 2: shufb >t26=$98,<z4x5x4t26=$70,<z4x5x4t26=$70,<selw3333=$32
shufb $98,$70,$70,$32

# qhasm: z4x5x4t218  &= mask13
# asm 1: and >z4x5x4t218=vec128#97,<z4x5x4t218=vec128#58,<mask13=vec128#22
# asm 2: and >z4x5x4t218=$99,<z4x5x4t218=$60,<mask13=$24
and $99,$60,$24

# qhasm: t210 = select bytes from z4x5x4t210 by selw3333
# asm 1: shufb >t210=vec128#98,<z4x5x4t210=vec128#72,<z4x5x4t210=vec128#72,<selw3333=vec128#30
# asm 2: shufb >t210=$100,<z4x5x4t210=$74,<z4x5x4t210=$74,<selw3333=$32
shufb $100,$74,$74,$32

# qhasm: int32323232 z4x5x4t27  += carry1
# asm 1: a >z4x5x4t27=vec128#99,<z4x5x4t27=vec128#64,<carry1=vec128#59
# asm 2: a >z4x5x4t27=$101,<z4x5x4t27=$66,<carry1=$61
a $101,$66,$61

# qhasm: t214 = select bytes from z4x5x4t214 by selw3333
# asm 1: shufb >t214=vec128#100,<z4x5x4t214=vec128#82,<z4x5x4t214=vec128#82,<selw3333=vec128#30
# asm 2: shufb >t214=$102,<z4x5x4t214=$84,<z4x5x4t214=$84,<selw3333=$32
shufb $102,$84,$84,$32

# qhasm: int32323232 z4x5x4t211 += carry2
# asm 1: a >z4x5x4t211=vec128#69,<z4x5x4t211=vec128#69,<carry2=vec128#60
# asm 2: a >z4x5x4t211=$71,<z4x5x4t211=$71,<carry2=$62
a $71,$71,$62

# qhasm: t218 = select bytes from z4x5x4t218 by selw3333
# asm 1: shufb >t218=vec128#101,<z4x5x4t218=vec128#97,<z4x5x4t218=vec128#97,<selw3333=vec128#30
# asm 2: shufb >t218=$103,<z4x5x4t218=$99,<z4x5x4t218=$99,<selw3333=$32
shufb $103,$99,$99,$32

# qhasm: int32323232 z4x5x4t215 += carry3
# asm 1: a >z4x5x4t215=vec128#70,<z4x5x4t215=vec128#70,<carry3=vec128#61
# asm 2: a >z4x5x4t215=$72,<z4x5x4t215=$72,<carry3=$63
a $72,$72,$63

# qhasm: int32323232 z4x5x4t219 += carry4
# asm 1: a >z4x5x4t219=vec128#73,<z4x5x4t219=vec128#73,<carry4=vec128#62
# asm 2: a >z4x5x4t219=$75,<z4x5x4t219=$75,<carry4=$64
a $75,$75,$64

# qhasm: int32323232 tmp10 = (t21 & 0xffff) * (x1_03 & 0xffff)
# asm 1: mpy >tmp10=vec128#55,<t21=vec128#81,<x1_03=vec128#51
# asm 2: mpy >tmp10=$57,<t21=$83,<x1_03=$53
mpy $57,$83,$53

# qhasm: t27 = select bytes from z4x5x4t27 by selw3333
# asm 1: shufb >t27=vec128#102,<z4x5x4t27=vec128#99,<z4x5x4t27=vec128#99,<selw3333=vec128#30
# asm 2: shufb >t27=$104,<z4x5x4t27=$101,<z4x5x4t27=$101,<selw3333=$32
shufb $104,$101,$101,$32

# qhasm: int32323232 tmp11 = (t21 & 0xffff) * (x1_47 & 0xffff)
# asm 1: mpy >tmp11=vec128#56,<t21=vec128#81,<x1_47=vec128#52
# asm 2: mpy >tmp11=$58,<t21=$83,<x1_47=$54
mpy $58,$83,$54

# qhasm: t211 = select bytes from z4x5x4t211 by selw3333
# asm 1: shufb >t211=vec128#103,<z4x5x4t211=vec128#69,<z4x5x4t211=vec128#69,<selw3333=vec128#30
# asm 2: shufb >t211=$105,<z4x5x4t211=$71,<z4x5x4t211=$71,<selw3333=$32
shufb $105,$71,$71,$32

# qhasm: int32323232 tmp12 = (t21 & 0xffff) * (x1_811 & 0xffff)
# asm 1: mpy >tmp12=vec128#57,<t21=vec128#81,<x1_811=vec128#53
# asm 2: mpy >tmp12=$59,<t21=$83,<x1_811=$55
mpy $59,$83,$55

# qhasm: t215 = select bytes from z4x5x4t215 by selw3333
# asm 1: shufb >t215=vec128#104,<z4x5x4t215=vec128#70,<z4x5x4t215=vec128#70,<selw3333=vec128#30
# asm 2: shufb >t215=$106,<z4x5x4t215=$72,<z4x5x4t215=$72,<selw3333=$32
shufb $106,$72,$72,$32

# qhasm: int32323232 tmp13 = (t21 & 0xffff) * (x1_1215 & 0xffff)
# asm 1: mpy >tmp13=vec128#58,<t21=vec128#81,<x1_1215=vec128#54
# asm 2: mpy >tmp13=$60,<t21=$83,<x1_1215=$56
mpy $60,$83,$56

# qhasm: t219 = select bytes from z4x5x4t219 by selw3333
# asm 1: shufb >t219=vec128#105,<z4x5x4t219=vec128#73,<z4x5x4t219=vec128#73,<selw3333=vec128#30
# asm 2: shufb >t219=$107,<z4x5x4t219=$75,<z4x5x4t219=$75,<selw3333=$32
shufb $107,$75,$75,$32

# qhasm: int32323232 tmp14 = (t21 & 0xffff) * (x1_1619 & 0xffff)
# asm 1: mpy >tmp14=vec128#59,<t21=vec128#81,<x1_1619=vec128#3
# asm 2: mpy >tmp14=$61,<t21=$83,<x1_1619=$5
mpy $61,$83,$5

# qhasm: x2_03 = combine z4x5x4t20 and z4x5x4t21 by shuf2_01
# asm 1: shufb >x2_03=vec128#60,<z4x5x4t20=vec128#76,<z4x5x4t21=vec128#5,<shuf2_01=vec128#15
# asm 2: shufb >x2_03=$62,<z4x5x4t20=$78,<z4x5x4t21=$7,<shuf2_01=$17
shufb $62,$78,$7,$17

# qhasm: int32323232 z3_03 = (t20 & 0xffff) * (x1_03 & 0xffff)
# asm 1: mpy >z3_03=vec128#61,<t20=vec128#80,<x1_03=vec128#51
# asm 2: mpy >z3_03=$63,<t20=$82,<x1_03=$53
mpy $63,$82,$53

# qhasm: x2_47 = combine z4x5x4t24 and z4x5x4t25 by shuf2_01
# asm 1: shufb >x2_47=vec128#62,<z4x5x4t24=vec128#84,<z4x5x4t25=vec128#83,<shuf2_01=vec128#15
# asm 2: shufb >x2_47=$64,<z4x5x4t24=$86,<z4x5x4t25=$85,<shuf2_01=$17
shufb $64,$86,$85,$17

# qhasm: int32323232 z3_47 = (t20 & 0xffff) * (x1_47 & 0xffff)
# asm 1: mpy >z3_47=vec128#63,<t20=vec128#80,<x1_47=vec128#52
# asm 2: mpy >z3_47=$65,<t20=$82,<x1_47=$54
mpy $65,$82,$54

# qhasm: x2_811 = combine z4x5x4t28 and z4x5x4t29 by shuf2_01
# asm 1: shufb >x2_811=vec128#64,<z4x5x4t28=vec128#85,<z4x5x4t29=vec128#89,<shuf2_01=vec128#15
# asm 2: shufb >x2_811=$66,<z4x5x4t28=$87,<z4x5x4t29=$91,<shuf2_01=$17
shufb $66,$87,$91,$17

# qhasm: int32323232 z3_811 = (t20 & 0xffff) * (x1_811 & 0xffff)
# asm 1: mpy >z3_811=vec128#81,<t20=vec128#80,<x1_811=vec128#53
# asm 2: mpy >z3_811=$83,<t20=$82,<x1_811=$55
mpy $83,$82,$55

# qhasm: x2_1215 = combine z4x5x4t212 and z4x5x4t213 by shuf2_01
# asm 1: shufb >x2_1215=vec128#106,<z4x5x4t212=vec128#67,<z4x5x4t213=vec128#90,<shuf2_01=vec128#15
# asm 2: shufb >x2_1215=$108,<z4x5x4t212=$69,<z4x5x4t213=$92,<shuf2_01=$17
shufb $108,$69,$92,$17

# qhasm: int32323232 z3_1215 = (t20 & 0xffff) * (x1_1215 & 0xffff)
# asm 1: mpy >z3_1215=vec128#107,<t20=vec128#80,<x1_1215=vec128#54
# asm 2: mpy >z3_1215=$109,<t20=$82,<x1_1215=$56
mpy $109,$82,$56

# qhasm: x2_1619 = combine z4x5x4t216 and z4x5x4t217 by shuf2_01
# asm 1: shufb >x2_1619=vec128#108,<z4x5x4t216=vec128#87,<z4x5x4t217=vec128#92,<shuf2_01=vec128#15
# asm 2: shufb >x2_1619=$110,<z4x5x4t216=$89,<z4x5x4t217=$94,<shuf2_01=$17
shufb $110,$89,$94,$17

# qhasm: int32323232 z3_1619 = (t20 & 0xffff) * (x1_1619 & 0xffff)
# asm 1: mpy >z3_1619=vec128#80,<t20=vec128#80,<x1_1619=vec128#3
# asm 2: mpy >z3_1619=$82,<t20=$82,<x1_1619=$5
mpy $82,$82,$5

# qhasm: tmp10b = tmp10 >> (8 * 4) 
# asm 1: rotqmbyi >tmp10b=vec128#109,<tmp10=vec128#55,-4
# asm 2: rotqmbyi >tmp10b=$111,<tmp10=$57,-4
rotqmbyi $111,$57,-4

# qhasm: int32323232 tmp10a = tmp10 << 1
# asm 1: shli >tmp10a=vec128#55,<tmp10=vec128#55,1
# asm 2: shli >tmp10a=$57,<tmp10=$57,1
shli $57,$57,1

# qhasm: x2_03 = combine x2_03 and z4x5x4t22 by shuf2_2
# asm 1: shufb >x2_03=vec128#60,<x2_03=vec128#60,<z4x5x4t22=vec128#65,<shuf2_2=vec128#16
# asm 2: shufb >x2_03=$62,<x2_03=$62,<z4x5x4t22=$67,<shuf2_2=$18
shufb $62,$62,$67,$18

# qhasm: int32323232 tmp11a = tmp11 << 1
# asm 1: shli >tmp11a=vec128#110,<tmp11=vec128#56,1
# asm 2: shli >tmp11a=$112,<tmp11=$58,1
shli $112,$58,1

# qhasm: x2_47 = combine x2_47 and z4x5x4t26 by shuf2_2
# asm 1: shufb >x2_47=vec128#62,<x2_47=vec128#62,<z4x5x4t26=vec128#68,<shuf2_2=vec128#16
# asm 2: shufb >x2_47=$64,<x2_47=$64,<z4x5x4t26=$70,<shuf2_2=$18
shufb $64,$64,$70,$18

# qhasm: int32323232 tmp12a = tmp12 << 1
# asm 1: shli >tmp12a=vec128#111,<tmp12=vec128#57,1
# asm 2: shli >tmp12a=$113,<tmp12=$59,1
shli $113,$59,1

# qhasm: x2_811 = combine x2_811 and z4x5x4t210 by shuf2_2
# asm 1: shufb >x2_811=vec128#64,<x2_811=vec128#64,<z4x5x4t210=vec128#72,<shuf2_2=vec128#16
# asm 2: shufb >x2_811=$66,<x2_811=$66,<z4x5x4t210=$74,<shuf2_2=$18
shufb $66,$66,$74,$18

# qhasm: int32323232 tmp13a = tmp13 << 1
# asm 1: shli >tmp13a=vec128#112,<tmp13=vec128#58,1
# asm 2: shli >tmp13a=$114,<tmp13=$60,1
shli $114,$60,1

# qhasm: tmp14a = tmp14 << (8 * 12)
# asm 1: shlqbyi >tmp14a=vec128#113,<tmp14=vec128#59,12
# asm 2: shlqbyi >tmp14a=$115,<tmp14=$61,12
shlqbyi $115,$61,12

# qhasm: int32323232 tmp20 = (t22 & 0xffff) * (x1_03 & 0xffff)
# asm 1: mpy >tmp20=vec128#114,<t22=vec128#71,<x1_03=vec128#51
# asm 2: mpy >tmp20=$116,<t22=$73,<x1_03=$53
mpy $116,$73,$53

# qhasm: tmp11b = combine tmp10a and tmp11 by comb13
# asm 1: shufb >tmp11b=vec128#55,<tmp10a=vec128#55,<tmp11=vec128#56,<comb13=vec128#40
# asm 2: shufb >tmp11b=$57,<tmp10a=$57,<tmp11=$58,<comb13=$42
shufb $57,$57,$58,$42

# qhasm: int32323232 tmp21 = (t22 & 0xffff) * (x1_47 & 0xffff)
# asm 1: mpy >tmp21=vec128#115,<t22=vec128#71,<x1_47=vec128#52
# asm 2: mpy >tmp21=$117,<t22=$73,<x1_47=$54
mpy $117,$73,$54

# qhasm: tmp12b = combine tmp11a and tmp12 by comb13
# asm 1: shufb >tmp12b=vec128#56,<tmp11a=vec128#110,<tmp12=vec128#57,<comb13=vec128#40
# asm 2: shufb >tmp12b=$58,<tmp11a=$112,<tmp12=$59,<comb13=$42
shufb $58,$112,$59,$42

# qhasm: int32323232 tmp22 = (t22 & 0xffff) * (x1_811 & 0xffff)
# asm 1: mpy >tmp22=vec128#110,<t22=vec128#71,<x1_811=vec128#53
# asm 2: mpy >tmp22=$112,<t22=$73,<x1_811=$55
mpy $112,$73,$55

# qhasm: tmp13b = combine tmp12a and tmp13 by comb13
# asm 1: shufb >tmp13b=vec128#57,<tmp12a=vec128#111,<tmp13=vec128#58,<comb13=vec128#40
# asm 2: shufb >tmp13b=$59,<tmp12a=$113,<tmp13=$60,<comb13=$42
shufb $59,$113,$60,$42

# qhasm: int32323232 tmp23 = (t22 & 0xffff) * (x1_1215 & 0xffff)
# asm 1: mpy >tmp23=vec128#111,<t22=vec128#71,<x1_1215=vec128#54
# asm 2: mpy >tmp23=$113,<t22=$73,<x1_1215=$56
mpy $113,$73,$56

# qhasm: tmp14b = combine tmp13a and tmp14 by comb13
# asm 1: shufb >tmp14b=vec128#58,<tmp13a=vec128#112,<tmp14=vec128#59,<comb13=vec128#40
# asm 2: shufb >tmp14b=$60,<tmp13a=$114,<tmp14=$61,<comb13=$42
shufb $60,$114,$61,$42

# qhasm: int32323232 tmp24 = (t22 & 0xffff) * (x1_1619 & 0xffff)
# asm 1: mpy >tmp24=vec128#71,<t22=vec128#71,<x1_1619=vec128#3
# asm 2: mpy >tmp24=$73,<t22=$73,<x1_1619=$5
mpy $73,$73,$5

# qhasm: x2_1215 = combine x2_1215 and z4x5x4t214 by shuf2_2
# asm 1: shufb >x2_1215=vec128#59,<x2_1215=vec128#106,<z4x5x4t214=vec128#82,<shuf2_2=vec128#16
# asm 2: shufb >x2_1215=$61,<x2_1215=$108,<z4x5x4t214=$84,<shuf2_2=$18
shufb $61,$108,$84,$18

# qhasm: int32323232 z3_03 += tmp10b
# asm 1: a >z3_03=vec128#61,<z3_03=vec128#61,<tmp10b=vec128#109
# asm 2: a >z3_03=$63,<z3_03=$63,<tmp10b=$111
a $63,$63,$111

# qhasm: x2_1619 = combine x2_1619 and z4x5x4t218 by shuf2_2
# asm 1: shufb >x2_1619=vec128#106,<x2_1619=vec128#108,<z4x5x4t218=vec128#97,<shuf2_2=vec128#16
# asm 2: shufb >x2_1619=$108,<x2_1619=$110,<z4x5x4t218=$99,<shuf2_2=$18
shufb $108,$110,$99,$18

# qhasm: int32323232 z3_47 += tmp11b
# asm 1: a >z3_47=vec128#63,<z3_47=vec128#63,<tmp11b=vec128#55
# asm 2: a >z3_47=$65,<z3_47=$65,<tmp11b=$57
a $65,$65,$57

# qhasm: x2_03 = combine x2_03 and z4x5x4t23 by shuf2_3
# asm 1: shufb >x2_03=vec128#55,<x2_03=vec128#60,<z4x5x4t23=vec128#66,<shuf2_3=vec128#17
# asm 2: shufb >x2_03=$57,<x2_03=$62,<z4x5x4t23=$68,<shuf2_3=$19
shufb $57,$62,$68,$19

# qhasm: int32323232 z3_811 += tmp12b
# asm 1: a >z3_811=vec128#60,<z3_811=vec128#81,<tmp12b=vec128#56
# asm 2: a >z3_811=$62,<z3_811=$83,<tmp12b=$58
a $62,$83,$58

# qhasm: x2_47 = combine x2_47 and z4x5x4t27 by shuf2_3
# asm 1: shufb >x2_47=vec128#56,<x2_47=vec128#62,<z4x5x4t27=vec128#99,<shuf2_3=vec128#17
# asm 2: shufb >x2_47=$58,<x2_47=$64,<z4x5x4t27=$101,<shuf2_3=$19
shufb $58,$64,$101,$19

# qhasm: int32323232 z3_1215 += tmp13b
# asm 1: a >z3_1215=vec128#62,<z3_1215=vec128#107,<tmp13b=vec128#57
# asm 2: a >z3_1215=$64,<z3_1215=$109,<tmp13b=$59
a $64,$109,$59

# qhasm: x2_811 = combine x2_811 and z4x5x4t211 by shuf2_3
# asm 1: shufb >x2_811=vec128#57,<x2_811=vec128#64,<z4x5x4t211=vec128#69,<shuf2_3=vec128#17
# asm 2: shufb >x2_811=$59,<x2_811=$66,<z4x5x4t211=$71,<shuf2_3=$19
shufb $59,$66,$71,$19

# qhasm: int32323232 z3_1619 += tmp14b
# asm 1: a >z3_1619=vec128#64,<z3_1619=vec128#80,<tmp14b=vec128#58
# asm 2: a >z3_1619=$66,<z3_1619=$82,<tmp14b=$60
a $66,$82,$60

# qhasm: x2_1215 = combine x2_1215 and z4x5x4t215 by shuf2_3
# asm 1: shufb >x2_1215=vec128#58,<x2_1215=vec128#59,<z4x5x4t215=vec128#70,<shuf2_3=vec128#17
# asm 2: shufb >x2_1215=$60,<x2_1215=$61,<z4x5x4t215=$72,<shuf2_3=$19
shufb $60,$61,$72,$19

# qhasm: int32323232 z3_2023 = tmp14a << 1
# asm 1: shli >z3_2023=vec128#80,<tmp14a=vec128#113,1
# asm 2: shli >z3_2023=$82,<tmp14a=$115,1
shli $82,$115,1

# qhasm: tmp20b = tmp20 >> (8 * 8) 
# asm 1: rotqmbyi >tmp20b=vec128#81,<tmp20=vec128#114,-8
# asm 2: rotqmbyi >tmp20b=$83,<tmp20=$116,-8
rotqmbyi $83,$116,-8

# qhasm: int32323232 tmp20a = tmp20 << 1
# asm 1: shli >tmp20a=vec128#107,<tmp20=vec128#114,1
# asm 2: shli >tmp20a=$109,<tmp20=$116,1
shli $109,$116,1

# qhasm: x2_1619 = combine x2_1619 and z4x5x4t219 by shuf2_3
# asm 1: shufb >x2_1619=vec128#59,<x2_1619=vec128#106,<z4x5x4t219=vec128#73,<shuf2_3=vec128#17
# asm 2: shufb >x2_1619=$61,<x2_1619=$108,<z4x5x4t219=$75,<shuf2_3=$19
shufb $61,$108,$75,$19

# qhasm: int32323232 tmp21a = tmp21 << 1
# asm 1: shli >tmp21a=vec128#106,<tmp21=vec128#115,1
# asm 2: shli >tmp21a=$108,<tmp21=$117,1
shli $108,$117,1

# qhasm: z2_03 = combine z4x5x4t20 and z4x5x4t21 by shuf0_01
# asm 1: shufb >z2_03=vec128#108,<z4x5x4t20=vec128#76,<z4x5x4t21=vec128#5,<shuf0_01=vec128#9
# asm 2: shufb >z2_03=$110,<z4x5x4t20=$78,<z4x5x4t21=$7,<shuf0_01=$11
shufb $110,$78,$7,$11

# qhasm: int32323232 tmp22a = tmp22 << 1
# asm 1: shli >tmp22a=vec128#109,<tmp22=vec128#110,1
# asm 2: shli >tmp22a=$111,<tmp22=$112,1
shli $111,$112,1

# qhasm: tmp24a = tmp24 << (8 * 8)
# asm 1: shlqbyi >tmp24a=vec128#112,<tmp24=vec128#71,8
# asm 2: shlqbyi >tmp24a=$114,<tmp24=$73,8
shlqbyi $114,$73,8

# qhasm: int32323232 tmp23a = tmp23 << 1
# asm 1: shli >tmp23a=vec128#113,<tmp23=vec128#111,1
# asm 2: shli >tmp23a=$115,<tmp23=$113,1
shli $115,$113,1

# qhasm: z2_47 = combine z4x5x4t24 and z4x5x4t25 by shuf0_01
# asm 1: shufb >z2_47=vec128#114,<z4x5x4t24=vec128#84,<z4x5x4t25=vec128#83,<shuf0_01=vec128#9
# asm 2: shufb >z2_47=$116,<z4x5x4t24=$86,<z4x5x4t25=$85,<shuf0_01=$11
shufb $116,$86,$85,$11

# qhasm: int32323232 tmp30 = (t23 & 0xffff) * (x1_03 & 0xffff)
# asm 1: mpy >tmp30=vec128#116,<t23=vec128#77,<x1_03=vec128#51
# asm 2: mpy >tmp30=$118,<t23=$79,<x1_03=$53
mpy $118,$79,$53

# qhasm: z2_811 = combine z4x5x4t28 and z4x5x4t29 by shuf0_01
# asm 1: shufb >z2_811=vec128#117,<z4x5x4t28=vec128#85,<z4x5x4t29=vec128#89,<shuf0_01=vec128#9
# asm 2: shufb >z2_811=$119,<z4x5x4t28=$87,<z4x5x4t29=$91,<shuf0_01=$11
shufb $119,$87,$91,$11

# qhasm: int32323232 tmp31 = (t23 & 0xffff) * (x1_47 & 0xffff)
# asm 1: mpy >tmp31=vec128#118,<t23=vec128#77,<x1_47=vec128#52
# asm 2: mpy >tmp31=$120,<t23=$79,<x1_47=$54
mpy $120,$79,$54

# qhasm: tmp21b = combine tmp20a and tmp21 by comb22
# asm 1: shufb >tmp21b=vec128#107,<tmp20a=vec128#107,<tmp21=vec128#115,<comb22=vec128#41
# asm 2: shufb >tmp21b=$109,<tmp20a=$109,<tmp21=$117,<comb22=$43
shufb $109,$109,$117,$43

# qhasm: int32323232 tmp32 = (t23 & 0xffff) * (x1_811 & 0xffff)
# asm 1: mpy >tmp32=vec128#115,<t23=vec128#77,<x1_811=vec128#53
# asm 2: mpy >tmp32=$117,<t23=$79,<x1_811=$55
mpy $117,$79,$55

# qhasm: tmp22b = combine tmp21a and tmp22 by comb22
# asm 1: shufb >tmp22b=vec128#106,<tmp21a=vec128#106,<tmp22=vec128#110,<comb22=vec128#41
# asm 2: shufb >tmp22b=$108,<tmp21a=$108,<tmp22=$112,<comb22=$43
shufb $108,$108,$112,$43

# qhasm: int32323232 tmp33 = (t23 & 0xffff) * (x1_1215 & 0xffff)
# asm 1: mpy >tmp33=vec128#110,<t23=vec128#77,<x1_1215=vec128#54
# asm 2: mpy >tmp33=$112,<t23=$79,<x1_1215=$56
mpy $112,$79,$56

# qhasm: tmp23b = combine tmp22a and tmp23 by comb22
# asm 1: shufb >tmp23b=vec128#109,<tmp22a=vec128#109,<tmp23=vec128#111,<comb22=vec128#41
# asm 2: shufb >tmp23b=$111,<tmp22a=$111,<tmp23=$113,<comb22=$43
shufb $111,$111,$113,$43

# qhasm: int32323232 tmp34 = (t23 & 0xffff) * (x1_1619 & 0xffff)
# asm 1: mpy >tmp34=vec128#77,<t23=vec128#77,<x1_1619=vec128#3
# asm 2: mpy >tmp34=$79,<t23=$79,<x1_1619=$5
mpy $79,$79,$5

# qhasm: tmp24b = combine tmp23a and tmp24 by comb22
# asm 1: shufb >tmp24b=vec128#71,<tmp23a=vec128#113,<tmp24=vec128#71,<comb22=vec128#41
# asm 2: shufb >tmp24b=$73,<tmp23a=$115,<tmp24=$73,<comb22=$43
shufb $73,$115,$73,$43

# qhasm: int32323232 tmp24a <<= 1
# asm 1: shli >tmp24a=vec128#111,<tmp24a=vec128#112,1
# asm 2: shli >tmp24a=$113,<tmp24a=$114,1
shli $113,$114,1

# qhasm: z2_1215 = combine z4x5x4t212 and z4x5x4t213 by shuf0_01
# asm 1: shufb >z2_1215=vec128#112,<z4x5x4t212=vec128#67,<z4x5x4t213=vec128#90,<shuf0_01=vec128#9
# asm 2: shufb >z2_1215=$114,<z4x5x4t212=$69,<z4x5x4t213=$92,<shuf0_01=$11
shufb $114,$69,$92,$11

# qhasm: int32323232 z3_03 += tmp20b
# asm 1: a >z3_03=vec128#81,<z3_03=vec128#61,<tmp20b=vec128#81
# asm 2: a >z3_03=$83,<z3_03=$63,<tmp20b=$83
a $83,$63,$83

# qhasm: z2_1619 = combine z4x5x4t216 and z4x5x4t217 by shuf0_01
# asm 1: shufb >z2_1619=vec128#61,<z4x5x4t216=vec128#87,<z4x5x4t217=vec128#92,<shuf0_01=vec128#9
# asm 2: shufb >z2_1619=$63,<z4x5x4t216=$89,<z4x5x4t217=$94,<shuf0_01=$11
shufb $63,$89,$94,$11

# qhasm: int32323232 z3_47 += tmp21b
# asm 1: a >z3_47=vec128#107,<z3_47=vec128#63,<tmp21b=vec128#107
# asm 2: a >z3_47=$109,<z3_47=$65,<tmp21b=$109
a $109,$65,$109

# qhasm: z2_03 = combine z2_03 and z4x5x4t22 by shuf0_2
# asm 1: shufb >z2_03=vec128#63,<z2_03=vec128#108,<z4x5x4t22=vec128#65,<shuf0_2=vec128#10
# asm 2: shufb >z2_03=$65,<z2_03=$110,<z4x5x4t22=$67,<shuf0_2=$12
shufb $65,$110,$67,$12

# qhasm: int32323232 z3_811 += tmp22b
# asm 1: a >z3_811=vec128#106,<z3_811=vec128#60,<tmp22b=vec128#106
# asm 2: a >z3_811=$108,<z3_811=$62,<tmp22b=$108
a $108,$62,$108

# qhasm: z2_47 = combine z2_47 and z4x5x4t26 by shuf0_2
# asm 1: shufb >z2_47=vec128#108,<z2_47=vec128#114,<z4x5x4t26=vec128#68,<shuf0_2=vec128#10
# asm 2: shufb >z2_47=$110,<z2_47=$116,<z4x5x4t26=$70,<shuf0_2=$12
shufb $110,$116,$70,$12

# qhasm: int32323232 z3_1215 += tmp23b
# asm 1: a >z3_1215=vec128#109,<z3_1215=vec128#62,<tmp23b=vec128#109
# asm 2: a >z3_1215=$111,<z3_1215=$64,<tmp23b=$111
a $111,$64,$111

# qhasm: z2_811 = combine z2_811 and z4x5x4t210 by shuf0_2
# asm 1: shufb >z2_811=vec128#62,<z2_811=vec128#117,<z4x5x4t210=vec128#72,<shuf0_2=vec128#10
# asm 2: shufb >z2_811=$64,<z2_811=$119,<z4x5x4t210=$74,<shuf0_2=$12
shufb $64,$119,$74,$12

# qhasm: int32323232 z3_1619 += tmp24b
# asm 1: a >z3_1619=vec128#71,<z3_1619=vec128#64,<tmp24b=vec128#71
# asm 2: a >z3_1619=$73,<z3_1619=$66,<tmp24b=$73
a $73,$66,$73

# qhasm: z2_1215 = combine z2_1215 and z4x5x4t214 by shuf0_2
# asm 1: shufb >z2_1215=vec128#64,<z2_1215=vec128#112,<z4x5x4t214=vec128#82,<shuf0_2=vec128#10
# asm 2: shufb >z2_1215=$66,<z2_1215=$114,<z4x5x4t214=$84,<shuf0_2=$12
shufb $66,$114,$84,$12

# qhasm: int32323232 z3_2023 += tmp24a
# asm 1: a >z3_2023=vec128#80,<z3_2023=vec128#80,<tmp24a=vec128#111
# asm 2: a >z3_2023=$82,<z3_2023=$82,<tmp24a=$113
a $82,$82,$113

# qhasm: tmp30b = tmp30 >> (8 * 12) 
# asm 1: rotqmbyi >tmp30b=vec128#111,<tmp30=vec128#116,-12
# asm 2: rotqmbyi >tmp30b=$113,<tmp30=$118,-12
rotqmbyi $113,$118,-12

# qhasm: int32323232 tmp30a = tmp30 << 1
# asm 1: shli >tmp30a=vec128#112,<tmp30=vec128#116,1
# asm 2: shli >tmp30a=$114,<tmp30=$118,1
shli $114,$118,1

# qhasm: z2_1619 = combine z2_1619 and z4x5x4t218 by shuf0_2
# asm 1: shufb >z2_1619=vec128#113,<z2_1619=vec128#61,<z4x5x4t218=vec128#97,<shuf0_2=vec128#10
# asm 2: shufb >z2_1619=$115,<z2_1619=$63,<z4x5x4t218=$99,<shuf0_2=$12
shufb $115,$63,$99,$12

# qhasm: int32323232 tmp31a = tmp31 << 1
# asm 1: shli >tmp31a=vec128#114,<tmp31=vec128#118,1
# asm 2: shli >tmp31a=$116,<tmp31=$120,1
shli $116,$120,1

# qhasm: z2_03 = combine z2_03 and z4x5x4t23 by shuf0_3
# asm 1: shufb >z2_03=vec128#60,<z2_03=vec128#63,<z4x5x4t23=vec128#66,<shuf0_3=vec128#11
# asm 2: shufb >z2_03=$62,<z2_03=$65,<z4x5x4t23=$68,<shuf0_3=$13
shufb $62,$65,$68,$13

# qhasm: int32323232 tmp32a = tmp32 << 1
# asm 1: shli >tmp32a=vec128#63,<tmp32=vec128#115,1
# asm 2: shli >tmp32a=$65,<tmp32=$117,1
shli $65,$117,1

# qhasm: tmp34a = tmp34 << (8 * 4)
# asm 1: shlqbyi >tmp34a=vec128#116,<tmp34=vec128#77,4
# asm 2: shlqbyi >tmp34a=$118,<tmp34=$79,4
shlqbyi $118,$79,4

# qhasm: int32323232 tmp33a = tmp33 << 1
# asm 1: shli >tmp33a=vec128#117,<tmp33=vec128#110,1
# asm 2: shli >tmp33a=$119,<tmp33=$112,1
shli $119,$112,1

# qhasm: z2_47 = combine z2_47 and z4x5x4t27 by shuf0_3
# asm 1: shufb >z2_47=vec128#61,<z2_47=vec128#108,<z4x5x4t27=vec128#99,<shuf0_3=vec128#11
# asm 2: shufb >z2_47=$63,<z2_47=$110,<z4x5x4t27=$101,<shuf0_3=$13
shufb $63,$110,$101,$13

# qhasm: int32323232 tmp00 = (t24 & 0xffff) * (x1_03 & 0xffff)
# asm 1: mpy >tmp00=vec128#108,<t24=vec128#86,<x1_03=vec128#51
# asm 2: mpy >tmp00=$110,<t24=$88,<x1_03=$53
mpy $110,$88,$53

# qhasm: z2_811 = combine z2_811 and z4x5x4t211 by shuf0_3
# asm 1: shufb >z2_811=vec128#62,<z2_811=vec128#62,<z4x5x4t211=vec128#69,<shuf0_3=vec128#11
# asm 2: shufb >z2_811=$64,<z2_811=$64,<z4x5x4t211=$71,<shuf0_3=$13
shufb $64,$64,$71,$13

# qhasm: int32323232 tmp01 = (t24 & 0xffff) * (x1_47 & 0xffff)
# asm 1: mpy >tmp01=vec128#119,<t24=vec128#86,<x1_47=vec128#52
# asm 2: mpy >tmp01=$121,<t24=$88,<x1_47=$54
mpy $121,$88,$54

# qhasm: tmp31b = combine tmp30a and tmp31 by comb31
# asm 1: shufb >tmp31b=vec128#112,<tmp30a=vec128#112,<tmp31=vec128#118,<comb31=vec128#42
# asm 2: shufb >tmp31b=$114,<tmp30a=$114,<tmp31=$120,<comb31=$44
shufb $114,$114,$120,$44

# qhasm: int32323232 tmp02 = (t24 & 0xffff) * (x1_811 & 0xffff)
# asm 1: mpy >tmp02=vec128#118,<t24=vec128#86,<x1_811=vec128#53
# asm 2: mpy >tmp02=$120,<t24=$88,<x1_811=$55
mpy $120,$88,$55

# qhasm: tmp32b = combine tmp31a and tmp32 by comb31
# asm 1: shufb >tmp32b=vec128#114,<tmp31a=vec128#114,<tmp32=vec128#115,<comb31=vec128#42
# asm 2: shufb >tmp32b=$116,<tmp31a=$116,<tmp32=$117,<comb31=$44
shufb $116,$116,$117,$44

# qhasm: int32323232 tmp03 = (t24 & 0xffff) * (x1_1215 & 0xffff)
# asm 1: mpy >tmp03=vec128#115,<t24=vec128#86,<x1_1215=vec128#54
# asm 2: mpy >tmp03=$117,<t24=$88,<x1_1215=$56
mpy $117,$88,$56

# qhasm: tmp33b = combine tmp32a and tmp33 by comb31
# asm 1: shufb >tmp33b=vec128#110,<tmp32a=vec128#63,<tmp33=vec128#110,<comb31=vec128#42
# asm 2: shufb >tmp33b=$112,<tmp32a=$65,<tmp33=$112,<comb31=$44
shufb $112,$65,$112,$44

# qhasm: int32323232 tmp04 = (t24 & 0xffff) * (x1_1619 & 0xffff)
# asm 1: mpy >tmp04=vec128#86,<t24=vec128#86,<x1_1619=vec128#3
# asm 2: mpy >tmp04=$88,<t24=$88,<x1_1619=$5
mpy $88,$88,$5

# qhasm: tmp34b = combine tmp33a and tmp34 by comb31
# asm 1: shufb >tmp34b=vec128#77,<tmp33a=vec128#117,<tmp34=vec128#77,<comb31=vec128#42
# asm 2: shufb >tmp34b=$79,<tmp33a=$119,<tmp34=$79,<comb31=$44
shufb $79,$119,$79,$44

# qhasm: int32323232 tmp34a <<= 1
# asm 1: shli >tmp34a=vec128#116,<tmp34a=vec128#116,1
# asm 2: shli >tmp34a=$118,<tmp34a=$118,1
shli $118,$118,1

# qhasm: z2_1215 = combine z2_1215 and z4x5x4t215 by shuf0_3
# asm 1: shufb >z2_1215=vec128#63,<z2_1215=vec128#64,<z4x5x4t215=vec128#70,<shuf0_3=vec128#11
# asm 2: shufb >z2_1215=$65,<z2_1215=$66,<z4x5x4t215=$72,<shuf0_3=$13
shufb $65,$66,$72,$13

# qhasm: int32323232 z3_03 += tmp30b
# asm 1: a >z3_03=vec128#81,<z3_03=vec128#81,<tmp30b=vec128#111
# asm 2: a >z3_03=$83,<z3_03=$83,<tmp30b=$113
a $83,$83,$113

# qhasm: z2_1619 = combine z2_1619 and z4x5x4t219 by shuf0_3
# asm 1: shufb >z2_1619=vec128#64,<z2_1619=vec128#113,<z4x5x4t219=vec128#73,<shuf0_3=vec128#11
# asm 2: shufb >z2_1619=$66,<z2_1619=$115,<z4x5x4t219=$75,<shuf0_3=$13
shufb $66,$115,$75,$13

# qhasm: int32323232 z3_47 += tmp31b
# asm 1: a >z3_47=vec128#107,<z3_47=vec128#107,<tmp31b=vec128#112
# asm 2: a >z3_47=$109,<z3_47=$109,<tmp31b=$114
a $109,$109,$114

# qhasm: x3_03 = combine z4x5x4t20 and z4x5x4t21 by shuf1_01
# asm 1: shufb >x3_03=vec128#5,<z4x5x4t20=vec128#76,<z4x5x4t21=vec128#5,<shuf1_01=vec128#12
# asm 2: shufb >x3_03=$7,<z4x5x4t20=$78,<z4x5x4t21=$7,<shuf1_01=$14
shufb $7,$78,$7,$14

# qhasm: int32323232 z3_811 += tmp32b
# asm 1: a >z3_811=vec128#76,<z3_811=vec128#106,<tmp32b=vec128#114
# asm 2: a >z3_811=$78,<z3_811=$108,<tmp32b=$116
a $78,$108,$116

# qhasm: x3_47 = combine z4x5x4t24 and z4x5x4t25 by shuf1_01
# asm 1: shufb >x3_47=vec128#83,<z4x5x4t24=vec128#84,<z4x5x4t25=vec128#83,<shuf1_01=vec128#12
# asm 2: shufb >x3_47=$85,<z4x5x4t24=$86,<z4x5x4t25=$85,<shuf1_01=$14
shufb $85,$86,$85,$14

# qhasm: int32323232 z3_1215 += tmp33b
# asm 1: a >z3_1215=vec128#84,<z3_1215=vec128#109,<tmp33b=vec128#110
# asm 2: a >z3_1215=$86,<z3_1215=$111,<tmp33b=$112
a $86,$111,$112

# qhasm: x3_811 = combine z4x5x4t28 and z4x5x4t29 by shuf1_01
# asm 1: shufb >x3_811=vec128#85,<z4x5x4t28=vec128#85,<z4x5x4t29=vec128#89,<shuf1_01=vec128#12
# asm 2: shufb >x3_811=$87,<z4x5x4t28=$87,<z4x5x4t29=$91,<shuf1_01=$14
shufb $87,$87,$91,$14

# qhasm: int32323232 z3_1619 += tmp34b
# asm 1: a >z3_1619=vec128#71,<z3_1619=vec128#71,<tmp34b=vec128#77
# asm 2: a >z3_1619=$73,<z3_1619=$73,<tmp34b=$79
a $73,$73,$79

# qhasm: x3_1215 = combine z4x5x4t212 and z4x5x4t213 by shuf1_01
# asm 1: shufb >x3_1215=vec128#67,<z4x5x4t212=vec128#67,<z4x5x4t213=vec128#90,<shuf1_01=vec128#12
# asm 2: shufb >x3_1215=$69,<z4x5x4t212=$69,<z4x5x4t213=$92,<shuf1_01=$14
shufb $69,$69,$92,$14

# qhasm: int32323232 z3_2023 += tmp34a
# asm 1: a >z3_2023=vec128#77,<z3_2023=vec128#80,<tmp34a=vec128#116
# asm 2: a >z3_2023=$79,<z3_2023=$82,<tmp34a=$118
a $79,$82,$118

# qhasm: x3_1619 = combine z4x5x4t216 and z4x5x4t217 by shuf1_01
# asm 1: shufb >x3_1619=vec128#80,<z4x5x4t216=vec128#87,<z4x5x4t217=vec128#92,<shuf1_01=vec128#12
# asm 2: shufb >x3_1619=$82,<z4x5x4t216=$89,<z4x5x4t217=$94,<shuf1_01=$14
shufb $82,$89,$94,$14

# qhasm: int32323232 tmp10 = (t25 & 0xffff) * (x1_03 & 0xffff)
# asm 1: mpy >tmp10=vec128#87,<t25=vec128#91,<x1_03=vec128#51
# asm 2: mpy >tmp10=$89,<t25=$93,<x1_03=$53
mpy $89,$93,$53

# qhasm: x3_03 = combine x3_03 and z4x5x4t22 by shuf1_2
# asm 1: shufb >x3_03=vec128#5,<x3_03=vec128#5,<z4x5x4t22=vec128#65,<shuf1_2=vec128#13
# asm 2: shufb >x3_03=$7,<x3_03=$7,<z4x5x4t22=$67,<shuf1_2=$15
shufb $7,$7,$67,$15

# qhasm: int32323232 tmp11 = (t25 & 0xffff) * (x1_47 & 0xffff)
# asm 1: mpy >tmp11=vec128#89,<t25=vec128#91,<x1_47=vec128#52
# asm 2: mpy >tmp11=$91,<t25=$93,<x1_47=$54
mpy $91,$93,$54

# qhasm: x3_47 = combine x3_47 and z4x5x4t26 by shuf1_2
# asm 1: shufb >x3_47=vec128#68,<x3_47=vec128#83,<z4x5x4t26=vec128#68,<shuf1_2=vec128#13
# asm 2: shufb >x3_47=$70,<x3_47=$85,<z4x5x4t26=$70,<shuf1_2=$15
shufb $70,$85,$70,$15

# qhasm: int32323232 tmp12 = (t25 & 0xffff) * (x1_811 & 0xffff)
# asm 1: mpy >tmp12=vec128#83,<t25=vec128#91,<x1_811=vec128#53
# asm 2: mpy >tmp12=$85,<t25=$93,<x1_811=$55
mpy $85,$93,$55

# qhasm: x3_811 = combine x3_811 and z4x5x4t210 by shuf1_2
# asm 1: shufb >x3_811=vec128#72,<x3_811=vec128#85,<z4x5x4t210=vec128#72,<shuf1_2=vec128#13
# asm 2: shufb >x3_811=$74,<x3_811=$87,<z4x5x4t210=$74,<shuf1_2=$15
shufb $74,$87,$74,$15

# qhasm: int32323232 tmp13 = (t25 & 0xffff) * (x1_1215 & 0xffff)
# asm 1: mpy >tmp13=vec128#85,<t25=vec128#91,<x1_1215=vec128#54
# asm 2: mpy >tmp13=$87,<t25=$93,<x1_1215=$56
mpy $87,$93,$56

# qhasm: x3_1215 = combine x3_1215 and z4x5x4t214 by shuf1_2
# asm 1: shufb >x3_1215=vec128#82,<x3_1215=vec128#67,<z4x5x4t214=vec128#82,<shuf1_2=vec128#13
# asm 2: shufb >x3_1215=$84,<x3_1215=$69,<z4x5x4t214=$84,<shuf1_2=$15
shufb $84,$69,$84,$15

# qhasm: int32323232 tmp14 = (t25 & 0xffff) * (x1_1619 & 0xffff)
# asm 1: mpy >tmp14=vec128#90,<t25=vec128#91,<x1_1619=vec128#3
# asm 2: mpy >tmp14=$92,<t25=$93,<x1_1619=$5
mpy $92,$93,$5

# qhasm: x3_1619 = combine x3_1619 and z4x5x4t218 by shuf1_2
# asm 1: shufb >x3_1619=vec128#80,<x3_1619=vec128#80,<z4x5x4t218=vec128#97,<shuf1_2=vec128#13
# asm 2: shufb >x3_1619=$82,<x3_1619=$82,<z4x5x4t218=$99,<shuf1_2=$15
shufb $82,$82,$99,$15

# qhasm: int32323232 z3_47 += tmp00
# asm 1: a >z3_47=vec128#91,<z3_47=vec128#107,<tmp00=vec128#108
# asm 2: a >z3_47=$93,<z3_47=$109,<tmp00=$110
a $93,$109,$110

# qhasm: x3_03 = combine x3_03 and z4x5x4t23 by shuf1_3
# asm 1: shufb >x3_03=vec128#65,<x3_03=vec128#5,<z4x5x4t23=vec128#66,<shuf1_3=vec128#14
# asm 2: shufb >x3_03=$67,<x3_03=$7,<z4x5x4t23=$68,<shuf1_3=$16
shufb $67,$7,$68,$16

# qhasm: int32323232 z3_811 += tmp01
# asm 1: a >z3_811=vec128#5,<z3_811=vec128#76,<tmp01=vec128#119
# asm 2: a >z3_811=$7,<z3_811=$78,<tmp01=$121
a $7,$78,$121

# qhasm: x3_47 = combine x3_47 and z4x5x4t27 by shuf1_3
# asm 1: shufb >x3_47=vec128#66,<x3_47=vec128#68,<z4x5x4t27=vec128#99,<shuf1_3=vec128#14
# asm 2: shufb >x3_47=$68,<x3_47=$70,<z4x5x4t27=$101,<shuf1_3=$16
shufb $68,$70,$101,$16

# qhasm: int32323232 z3_1215 += tmp02
# asm 1: a >z3_1215=vec128#76,<z3_1215=vec128#84,<tmp02=vec128#118
# asm 2: a >z3_1215=$78,<z3_1215=$86,<tmp02=$120
a $78,$86,$120

# qhasm: x3_811 = combine x3_811 and z4x5x4t211 by shuf1_3
# asm 1: shufb >x3_811=vec128#67,<x3_811=vec128#72,<z4x5x4t211=vec128#69,<shuf1_3=vec128#14
# asm 2: shufb >x3_811=$69,<x3_811=$74,<z4x5x4t211=$71,<shuf1_3=$16
shufb $69,$74,$71,$16

# qhasm: int32323232 z3_1619 += tmp03
# asm 1: a >z3_1619=vec128#71,<z3_1619=vec128#71,<tmp03=vec128#115
# asm 2: a >z3_1619=$73,<z3_1619=$73,<tmp03=$117
a $73,$73,$117

# qhasm: tmp10b = tmp10 >> (8 * 4) 
# asm 1: rotqmbyi >tmp10b=vec128#72,<tmp10=vec128#87,-4
# asm 2: rotqmbyi >tmp10b=$74,<tmp10=$89,-4
rotqmbyi $74,$89,-4

# qhasm: int32323232 z3_2023 += tmp04
# asm 1: a >z3_2023=vec128#77,<z3_2023=vec128#77,<tmp04=vec128#86
# asm 2: a >z3_2023=$79,<z3_2023=$79,<tmp04=$88
a $79,$79,$88

# qhasm: x3_1215 = combine x3_1215 and z4x5x4t215 by shuf1_3
# asm 1: shufb >x3_1215=vec128#68,<x3_1215=vec128#82,<z4x5x4t215=vec128#70,<shuf1_3=vec128#14
# asm 2: shufb >x3_1215=$70,<x3_1215=$84,<z4x5x4t215=$72,<shuf1_3=$16
shufb $70,$84,$72,$16

# qhasm: int32323232 tmp10a = tmp10 << 1
# asm 1: shli >tmp10a=vec128#70,<tmp10=vec128#87,1
# asm 2: shli >tmp10a=$72,<tmp10=$89,1
shli $72,$89,1

# qhasm: x3_1619 = combine x3_1619 and z4x5x4t219 by shuf1_3
# asm 1: shufb >x3_1619=vec128#69,<x3_1619=vec128#80,<z4x5x4t219=vec128#73,<shuf1_3=vec128#14
# asm 2: shufb >x3_1619=$71,<x3_1619=$82,<z4x5x4t219=$75,<shuf1_3=$16
shufb $71,$82,$75,$16

# qhasm: int32323232 tmp11a = tmp11 << 1
# asm 1: shli >tmp11a=vec128#73,<tmp11=vec128#89,1
# asm 2: shli >tmp11a=$75,<tmp11=$91,1
shli $75,$91,1

# qhasm: int32323232 tmp12a = tmp12 << 1
# asm 1: shli >tmp12a=vec128#80,<tmp12=vec128#83,1
# asm 2: shli >tmp12a=$82,<tmp12=$85,1
shli $82,$85,1

# qhasm: int32323232 tmp13a = tmp13 << 1
# asm 1: shli >tmp13a=vec128#82,<tmp13=vec128#85,1
# asm 2: shli >tmp13a=$84,<tmp13=$87,1
shli $84,$87,1

# qhasm: tmp14a = tmp14 << (8 * 12)
# asm 1: shlqbyi >tmp14a=vec128#84,<tmp14=vec128#90,12
# asm 2: shlqbyi >tmp14a=$86,<tmp14=$92,12
shlqbyi $86,$92,12

# qhasm: int32323232 tmp20 = (t26 & 0xffff) * (x1_03 & 0xffff)
# asm 1: mpy >tmp20=vec128#86,<t26=vec128#96,<x1_03=vec128#51
# asm 2: mpy >tmp20=$88,<t26=$98,<x1_03=$53
mpy $88,$98,$53

# qhasm: tmp11b = combine tmp10a and tmp11 by comb13
# asm 1: shufb >tmp11b=vec128#70,<tmp10a=vec128#70,<tmp11=vec128#89,<comb13=vec128#40
# asm 2: shufb >tmp11b=$72,<tmp10a=$72,<tmp11=$91,<comb13=$42
shufb $72,$72,$91,$42

# qhasm: int32323232 tmp21 = (t26 & 0xffff) * (x1_47 & 0xffff)
# asm 1: mpy >tmp21=vec128#87,<t26=vec128#96,<x1_47=vec128#52
# asm 2: mpy >tmp21=$89,<t26=$98,<x1_47=$54
mpy $89,$98,$54

# qhasm: tmp12b = combine tmp11a and tmp12 by comb13
# asm 1: shufb >tmp12b=vec128#73,<tmp11a=vec128#73,<tmp12=vec128#83,<comb13=vec128#40
# asm 2: shufb >tmp12b=$75,<tmp11a=$75,<tmp12=$85,<comb13=$42
shufb $75,$75,$85,$42

# qhasm: int32323232 tmp22 = (t26 & 0xffff) * (x1_811 & 0xffff)
# asm 1: mpy >tmp22=vec128#83,<t26=vec128#96,<x1_811=vec128#53
# asm 2: mpy >tmp22=$85,<t26=$98,<x1_811=$55
mpy $85,$98,$55

# qhasm: tmp13b = combine tmp12a and tmp13 by comb13
# asm 1: shufb >tmp13b=vec128#80,<tmp12a=vec128#80,<tmp13=vec128#85,<comb13=vec128#40
# asm 2: shufb >tmp13b=$82,<tmp12a=$82,<tmp13=$87,<comb13=$42
shufb $82,$82,$87,$42

# qhasm: int32323232 tmp23 = (t26 & 0xffff) * (x1_1215 & 0xffff)
# asm 1: mpy >tmp23=vec128#85,<t26=vec128#96,<x1_1215=vec128#54
# asm 2: mpy >tmp23=$87,<t26=$98,<x1_1215=$56
mpy $87,$98,$56

# qhasm: tmp14b = combine tmp13a and tmp14 by comb13
# asm 1: shufb >tmp14b=vec128#82,<tmp13a=vec128#82,<tmp14=vec128#90,<comb13=vec128#40
# asm 2: shufb >tmp14b=$84,<tmp13a=$84,<tmp14=$92,<comb13=$42
shufb $84,$84,$92,$42

# qhasm: int32323232 tmp24 = (t26 & 0xffff) * (x1_1619 & 0xffff)
# asm 1: mpy >tmp24=vec128#89,<t26=vec128#96,<x1_1619=vec128#3
# asm 2: mpy >tmp24=$91,<t26=$98,<x1_1619=$5
mpy $91,$98,$5

# qhasm: int32323232 z3_47 += tmp10b
# asm 1: a >z3_47=vec128#72,<z3_47=vec128#91,<tmp10b=vec128#72
# asm 2: a >z3_47=$74,<z3_47=$93,<tmp10b=$74
a $74,$93,$74

# qhasm: int32323232 z3_811 += tmp11b
# asm 1: a >z3_811=vec128#5,<z3_811=vec128#5,<tmp11b=vec128#70
# asm 2: a >z3_811=$7,<z3_811=$7,<tmp11b=$72
a $7,$7,$72

# qhasm: int32323232 z3_1215 += tmp12b
# asm 1: a >z3_1215=vec128#70,<z3_1215=vec128#76,<tmp12b=vec128#73
# asm 2: a >z3_1215=$72,<z3_1215=$78,<tmp12b=$75
a $72,$78,$75

# qhasm: int32323232 z3_1619 += tmp13b
# asm 1: a >z3_1619=vec128#71,<z3_1619=vec128#71,<tmp13b=vec128#80
# asm 2: a >z3_1619=$73,<z3_1619=$73,<tmp13b=$82
a $73,$73,$82

# qhasm: int32323232 z3_2023 += tmp14b
# asm 1: a >z3_2023=vec128#73,<z3_2023=vec128#77,<tmp14b=vec128#82
# asm 2: a >z3_2023=$75,<z3_2023=$79,<tmp14b=$84
a $75,$79,$84

# qhasm: int32323232 z3_2427 = tmp14a << 1
# asm 1: shli >z3_2427=vec128#76,<tmp14a=vec128#84,1
# asm 2: shli >z3_2427=$78,<tmp14a=$86,1
shli $78,$86,1

# qhasm: tmp20b = tmp20 >> (8 * 8) 
# asm 1: rotqmbyi >tmp20b=vec128#77,<tmp20=vec128#86,-8
# asm 2: rotqmbyi >tmp20b=$79,<tmp20=$88,-8
rotqmbyi $79,$88,-8

# qhasm: int32323232 tmp20a = tmp20 << 1
# asm 1: shli >tmp20a=vec128#80,<tmp20=vec128#86,1
# asm 2: shli >tmp20a=$82,<tmp20=$88,1
shli $82,$88,1

# qhasm: int32323232 tmp21a = tmp21 << 1
# asm 1: shli >tmp21a=vec128#82,<tmp21=vec128#87,1
# asm 2: shli >tmp21a=$84,<tmp21=$89,1
shli $84,$89,1

# qhasm: int32323232 tmp22a = tmp22 << 1
# asm 1: shli >tmp22a=vec128#84,<tmp22=vec128#83,1
# asm 2: shli >tmp22a=$86,<tmp22=$85,1
shli $86,$85,1

# qhasm: tmp24a = tmp24 << (8 * 8)
# asm 1: shlqbyi >tmp24a=vec128#86,<tmp24=vec128#89,8
# asm 2: shlqbyi >tmp24a=$88,<tmp24=$91,8
shlqbyi $88,$91,8

# qhasm: int32323232 tmp23a = tmp23 << 1
# asm 1: shli >tmp23a=vec128#90,<tmp23=vec128#85,1
# asm 2: shli >tmp23a=$92,<tmp23=$87,1
shli $92,$87,1

# qhasm: int32323232 tmp30 = (t27 & 0xffff) * (x1_03 & 0xffff)
# asm 1: mpy >tmp30=vec128#91,<t27=vec128#102,<x1_03=vec128#51
# asm 2: mpy >tmp30=$93,<t27=$104,<x1_03=$53
mpy $93,$104,$53

# qhasm: int32323232 tmp31 = (t27 & 0xffff) * (x1_47 & 0xffff)
# asm 1: mpy >tmp31=vec128#92,<t27=vec128#102,<x1_47=vec128#52
# asm 2: mpy >tmp31=$94,<t27=$104,<x1_47=$54
mpy $94,$104,$54

# qhasm: tmp21b = combine tmp20a and tmp21 by comb22
# asm 1: shufb >tmp21b=vec128#80,<tmp20a=vec128#80,<tmp21=vec128#87,<comb22=vec128#41
# asm 2: shufb >tmp21b=$82,<tmp20a=$82,<tmp21=$89,<comb22=$43
shufb $82,$82,$89,$43

# qhasm: int32323232 tmp32 = (t27 & 0xffff) * (x1_811 & 0xffff)
# asm 1: mpy >tmp32=vec128#87,<t27=vec128#102,<x1_811=vec128#53
# asm 2: mpy >tmp32=$89,<t27=$104,<x1_811=$55
mpy $89,$104,$55

# qhasm: tmp22b = combine tmp21a and tmp22 by comb22
# asm 1: shufb >tmp22b=vec128#82,<tmp21a=vec128#82,<tmp22=vec128#83,<comb22=vec128#41
# asm 2: shufb >tmp22b=$84,<tmp21a=$84,<tmp22=$85,<comb22=$43
shufb $84,$84,$85,$43

# qhasm: int32323232 tmp33 = (t27 & 0xffff) * (x1_1215 & 0xffff)
# asm 1: mpy >tmp33=vec128#83,<t27=vec128#102,<x1_1215=vec128#54
# asm 2: mpy >tmp33=$85,<t27=$104,<x1_1215=$56
mpy $85,$104,$56

# qhasm: tmp23b = combine tmp22a and tmp23 by comb22
# asm 1: shufb >tmp23b=vec128#84,<tmp22a=vec128#84,<tmp23=vec128#85,<comb22=vec128#41
# asm 2: shufb >tmp23b=$86,<tmp22a=$86,<tmp23=$87,<comb22=$43
shufb $86,$86,$87,$43

# qhasm: int32323232 tmp34 = (t27 & 0xffff) * (x1_1619 & 0xffff)
# asm 1: mpy >tmp34=vec128#85,<t27=vec128#102,<x1_1619=vec128#3
# asm 2: mpy >tmp34=$87,<t27=$104,<x1_1619=$5
mpy $87,$104,$5

# qhasm: tmp24b = combine tmp23a and tmp24 by comb22
# asm 1: shufb >tmp24b=vec128#89,<tmp23a=vec128#90,<tmp24=vec128#89,<comb22=vec128#41
# asm 2: shufb >tmp24b=$91,<tmp23a=$92,<tmp24=$91,<comb22=$43
shufb $91,$92,$91,$43

# qhasm: int32323232 tmp24a <<= 1
# asm 1: shli >tmp24a=vec128#86,<tmp24a=vec128#86,1
# asm 2: shli >tmp24a=$88,<tmp24a=$88,1
shli $88,$88,1

# qhasm: int32323232 z3_47 += tmp20b
# asm 1: a >z3_47=vec128#72,<z3_47=vec128#72,<tmp20b=vec128#77
# asm 2: a >z3_47=$74,<z3_47=$74,<tmp20b=$79
a $74,$74,$79

# qhasm: int32323232 z3_811 += tmp21b
# asm 1: a >z3_811=vec128#5,<z3_811=vec128#5,<tmp21b=vec128#80
# asm 2: a >z3_811=$7,<z3_811=$7,<tmp21b=$82
a $7,$7,$82

# qhasm: int32323232 z3_1215 += tmp22b
# asm 1: a >z3_1215=vec128#70,<z3_1215=vec128#70,<tmp22b=vec128#82
# asm 2: a >z3_1215=$72,<z3_1215=$72,<tmp22b=$84
a $72,$72,$84

# qhasm: int32323232 z3_1619 += tmp23b
# asm 1: a >z3_1619=vec128#71,<z3_1619=vec128#71,<tmp23b=vec128#84
# asm 2: a >z3_1619=$73,<z3_1619=$73,<tmp23b=$86
a $73,$73,$86

# qhasm: int32323232 z3_2023 += tmp24b
# asm 1: a >z3_2023=vec128#73,<z3_2023=vec128#73,<tmp24b=vec128#89
# asm 2: a >z3_2023=$75,<z3_2023=$75,<tmp24b=$91
a $75,$75,$91

# qhasm: int32323232 z3_2427 += tmp24a
# asm 1: a >z3_2427=vec128#76,<z3_2427=vec128#76,<tmp24a=vec128#86
# asm 2: a >z3_2427=$78,<z3_2427=$78,<tmp24a=$88
a $78,$78,$88

# qhasm: tmp30b = tmp30 >> (8 * 12) 
# asm 1: rotqmbyi >tmp30b=vec128#77,<tmp30=vec128#91,-12
# asm 2: rotqmbyi >tmp30b=$79,<tmp30=$93,-12
rotqmbyi $79,$93,-12

# qhasm: int32323232 tmp30a = tmp30 << 1
# asm 1: shli >tmp30a=vec128#80,<tmp30=vec128#91,1
# asm 2: shli >tmp30a=$82,<tmp30=$93,1
shli $82,$93,1

# qhasm: int32323232 tmp31a = tmp31 << 1
# asm 1: shli >tmp31a=vec128#82,<tmp31=vec128#92,1
# asm 2: shli >tmp31a=$84,<tmp31=$94,1
shli $84,$94,1

# qhasm: int32323232 tmp32a = tmp32 << 1
# asm 1: shli >tmp32a=vec128#84,<tmp32=vec128#87,1
# asm 2: shli >tmp32a=$86,<tmp32=$89,1
shli $86,$89,1

# qhasm: tmp34a = tmp34 << (8 * 4)
# asm 1: shlqbyi >tmp34a=vec128#86,<tmp34=vec128#85,4
# asm 2: shlqbyi >tmp34a=$88,<tmp34=$87,4
shlqbyi $88,$87,4

# qhasm: int32323232 tmp33a = tmp33 << 1
# asm 1: shli >tmp33a=vec128#89,<tmp33=vec128#83,1
# asm 2: shli >tmp33a=$91,<tmp33=$85,1
shli $91,$85,1

# qhasm: int32323232 tmp00 = (t28 & 0xffff) * (x1_03 & 0xffff)
# asm 1: mpy >tmp00=vec128#90,<t28=vec128#88,<x1_03=vec128#51
# asm 2: mpy >tmp00=$92,<t28=$90,<x1_03=$53
mpy $92,$90,$53

# qhasm: int32323232 tmp01 = (t28 & 0xffff) * (x1_47 & 0xffff)
# asm 1: mpy >tmp01=vec128#91,<t28=vec128#88,<x1_47=vec128#52
# asm 2: mpy >tmp01=$93,<t28=$90,<x1_47=$54
mpy $93,$90,$54

# qhasm: tmp31b = combine tmp30a and tmp31 by comb31
# asm 1: shufb >tmp31b=vec128#80,<tmp30a=vec128#80,<tmp31=vec128#92,<comb31=vec128#42
# asm 2: shufb >tmp31b=$82,<tmp30a=$82,<tmp31=$94,<comb31=$44
shufb $82,$82,$94,$44

# qhasm: int32323232 tmp02 = (t28 & 0xffff) * (x1_811 & 0xffff)
# asm 1: mpy >tmp02=vec128#92,<t28=vec128#88,<x1_811=vec128#53
# asm 2: mpy >tmp02=$94,<t28=$90,<x1_811=$55
mpy $94,$90,$55

# qhasm: tmp32b = combine tmp31a and tmp32 by comb31
# asm 1: shufb >tmp32b=vec128#82,<tmp31a=vec128#82,<tmp32=vec128#87,<comb31=vec128#42
# asm 2: shufb >tmp32b=$84,<tmp31a=$84,<tmp32=$89,<comb31=$44
shufb $84,$84,$89,$44

# qhasm: int32323232 tmp03 = (t28 & 0xffff) * (x1_1215 & 0xffff)
# asm 1: mpy >tmp03=vec128#87,<t28=vec128#88,<x1_1215=vec128#54
# asm 2: mpy >tmp03=$89,<t28=$90,<x1_1215=$56
mpy $89,$90,$56

# qhasm: tmp33b = combine tmp32a and tmp33 by comb31
# asm 1: shufb >tmp33b=vec128#83,<tmp32a=vec128#84,<tmp33=vec128#83,<comb31=vec128#42
# asm 2: shufb >tmp33b=$85,<tmp32a=$86,<tmp33=$85,<comb31=$44
shufb $85,$86,$85,$44

# qhasm: int32323232 tmp04 = (t28 & 0xffff) * (x1_1619 & 0xffff)
# asm 1: mpy >tmp04=vec128#84,<t28=vec128#88,<x1_1619=vec128#3
# asm 2: mpy >tmp04=$86,<t28=$90,<x1_1619=$5
mpy $86,$90,$5

# qhasm: tmp34b = combine tmp33a and tmp34 by comb31
# asm 1: shufb >tmp34b=vec128#85,<tmp33a=vec128#89,<tmp34=vec128#85,<comb31=vec128#42
# asm 2: shufb >tmp34b=$87,<tmp33a=$91,<tmp34=$87,<comb31=$44
shufb $87,$91,$87,$44

# qhasm: int32323232 tmp34a <<= 1
# asm 1: shli >tmp34a=vec128#86,<tmp34a=vec128#86,1
# asm 2: shli >tmp34a=$88,<tmp34a=$88,1
shli $88,$88,1

# qhasm: int32323232 z3_47 += tmp30b
# asm 1: a >z3_47=vec128#72,<z3_47=vec128#72,<tmp30b=vec128#77
# asm 2: a >z3_47=$74,<z3_47=$74,<tmp30b=$79
a $74,$74,$79

# qhasm: int32323232 z3_811 += tmp31b
# asm 1: a >z3_811=vec128#5,<z3_811=vec128#5,<tmp31b=vec128#80
# asm 2: a >z3_811=$7,<z3_811=$7,<tmp31b=$82
a $7,$7,$82

# qhasm: int32323232 z3_1215 += tmp32b
# asm 1: a >z3_1215=vec128#70,<z3_1215=vec128#70,<tmp32b=vec128#82
# asm 2: a >z3_1215=$72,<z3_1215=$72,<tmp32b=$84
a $72,$72,$84

# qhasm: int32323232 z3_1619 += tmp33b
# asm 1: a >z3_1619=vec128#71,<z3_1619=vec128#71,<tmp33b=vec128#83
# asm 2: a >z3_1619=$73,<z3_1619=$73,<tmp33b=$85
a $73,$73,$85

# qhasm: int32323232 z3_2023 += tmp34b
# asm 1: a >z3_2023=vec128#73,<z3_2023=vec128#73,<tmp34b=vec128#85
# asm 2: a >z3_2023=$75,<z3_2023=$75,<tmp34b=$87
a $75,$75,$87

# qhasm: int32323232 z3_2427 += tmp34a
# asm 1: a >z3_2427=vec128#76,<z3_2427=vec128#76,<tmp34a=vec128#86
# asm 2: a >z3_2427=$78,<z3_2427=$78,<tmp34a=$88
a $78,$78,$88

# qhasm: int32323232 tmp10 = (t29 & 0xffff) * (x1_03 & 0xffff)
# asm 1: mpy >tmp10=vec128#77,<t29=vec128#93,<x1_03=vec128#51
# asm 2: mpy >tmp10=$79,<t29=$95,<x1_03=$53
mpy $79,$95,$53

# qhasm: int32323232 tmp11 = (t29 & 0xffff) * (x1_47 & 0xffff)
# asm 1: mpy >tmp11=vec128#80,<t29=vec128#93,<x1_47=vec128#52
# asm 2: mpy >tmp11=$82,<t29=$95,<x1_47=$54
mpy $82,$95,$54

# qhasm: int32323232 tmp12 = (t29 & 0xffff) * (x1_811 & 0xffff)
# asm 1: mpy >tmp12=vec128#82,<t29=vec128#93,<x1_811=vec128#53
# asm 2: mpy >tmp12=$84,<t29=$95,<x1_811=$55
mpy $84,$95,$55

# qhasm: int32323232 tmp13 = (t29 & 0xffff) * (x1_1215 & 0xffff)
# asm 1: mpy >tmp13=vec128#83,<t29=vec128#93,<x1_1215=vec128#54
# asm 2: mpy >tmp13=$85,<t29=$95,<x1_1215=$56
mpy $85,$95,$56

# qhasm: int32323232 tmp14 = (t29 & 0xffff) * (x1_1619 & 0xffff)
# asm 1: mpy >tmp14=vec128#85,<t29=vec128#93,<x1_1619=vec128#3
# asm 2: mpy >tmp14=$87,<t29=$95,<x1_1619=$5
mpy $87,$95,$5

# qhasm: int32323232 z3_811 += tmp00
# asm 1: a >z3_811=vec128#5,<z3_811=vec128#5,<tmp00=vec128#90
# asm 2: a >z3_811=$7,<z3_811=$7,<tmp00=$92
a $7,$7,$92

# qhasm: int32323232 z3_1215 += tmp01
# asm 1: a >z3_1215=vec128#70,<z3_1215=vec128#70,<tmp01=vec128#91
# asm 2: a >z3_1215=$72,<z3_1215=$72,<tmp01=$93
a $72,$72,$93

# qhasm: int32323232 z3_1619 += tmp02
# asm 1: a >z3_1619=vec128#71,<z3_1619=vec128#71,<tmp02=vec128#92
# asm 2: a >z3_1619=$73,<z3_1619=$73,<tmp02=$94
a $73,$73,$94

# qhasm: int32323232 z3_2023 += tmp03
# asm 1: a >z3_2023=vec128#73,<z3_2023=vec128#73,<tmp03=vec128#87
# asm 2: a >z3_2023=$75,<z3_2023=$75,<tmp03=$89
a $75,$75,$89

# qhasm: int32323232 z3_2427 += tmp04
# asm 1: a >z3_2427=vec128#76,<z3_2427=vec128#76,<tmp04=vec128#84
# asm 2: a >z3_2427=$78,<z3_2427=$78,<tmp04=$86
a $78,$78,$86

# qhasm: tmp10b = tmp10 >> (8 * 4) 
# asm 1: rotqmbyi >tmp10b=vec128#84,<tmp10=vec128#77,-4
# asm 2: rotqmbyi >tmp10b=$86,<tmp10=$79,-4
rotqmbyi $86,$79,-4

# qhasm: int32323232 tmp10a = tmp10 << 1
# asm 1: shli >tmp10a=vec128#77,<tmp10=vec128#77,1
# asm 2: shli >tmp10a=$79,<tmp10=$79,1
shli $79,$79,1

# qhasm: int32323232 tmp11a = tmp11 << 1
# asm 1: shli >tmp11a=vec128#86,<tmp11=vec128#80,1
# asm 2: shli >tmp11a=$88,<tmp11=$82,1
shli $88,$82,1

# qhasm: int32323232 tmp12a = tmp12 << 1
# asm 1: shli >tmp12a=vec128#87,<tmp12=vec128#82,1
# asm 2: shli >tmp12a=$89,<tmp12=$84,1
shli $89,$84,1

# qhasm: tmp14a = tmp14 << (8 * 12)
# asm 1: shlqbyi >tmp14a=vec128#88,<tmp14=vec128#85,12
# asm 2: shlqbyi >tmp14a=$90,<tmp14=$87,12
shlqbyi $90,$87,12

# qhasm: int32323232 tmp13a = tmp13 << 1
# asm 1: shli >tmp13a=vec128#89,<tmp13=vec128#83,1
# asm 2: shli >tmp13a=$91,<tmp13=$85,1
shli $91,$85,1

# qhasm: int32323232 tmp20 = (t210 & 0xffff) * (x1_03 & 0xffff)
# asm 1: mpy >tmp20=vec128#90,<t210=vec128#98,<x1_03=vec128#51
# asm 2: mpy >tmp20=$92,<t210=$100,<x1_03=$53
mpy $92,$100,$53

# qhasm: int32323232 tmp21 = (t210 & 0xffff) * (x1_47 & 0xffff)
# asm 1: mpy >tmp21=vec128#91,<t210=vec128#98,<x1_47=vec128#52
# asm 2: mpy >tmp21=$93,<t210=$100,<x1_47=$54
mpy $93,$100,$54

# qhasm: tmp11b = combine tmp10a and tmp11 by comb13
# asm 1: shufb >tmp11b=vec128#77,<tmp10a=vec128#77,<tmp11=vec128#80,<comb13=vec128#40
# asm 2: shufb >tmp11b=$79,<tmp10a=$79,<tmp11=$82,<comb13=$42
shufb $79,$79,$82,$42

# qhasm: int32323232 tmp22 = (t210 & 0xffff) * (x1_811 & 0xffff)
# asm 1: mpy >tmp22=vec128#80,<t210=vec128#98,<x1_811=vec128#53
# asm 2: mpy >tmp22=$82,<t210=$100,<x1_811=$55
mpy $82,$100,$55

# qhasm: tmp12b = combine tmp11a and tmp12 by comb13
# asm 1: shufb >tmp12b=vec128#82,<tmp11a=vec128#86,<tmp12=vec128#82,<comb13=vec128#40
# asm 2: shufb >tmp12b=$84,<tmp11a=$88,<tmp12=$84,<comb13=$42
shufb $84,$88,$84,$42

# qhasm: int32323232 tmp23 = (t210 & 0xffff) * (x1_1215 & 0xffff)
# asm 1: mpy >tmp23=vec128#86,<t210=vec128#98,<x1_1215=vec128#54
# asm 2: mpy >tmp23=$88,<t210=$100,<x1_1215=$56
mpy $88,$100,$56

# qhasm: tmp13b = combine tmp12a and tmp13 by comb13
# asm 1: shufb >tmp13b=vec128#83,<tmp12a=vec128#87,<tmp13=vec128#83,<comb13=vec128#40
# asm 2: shufb >tmp13b=$85,<tmp12a=$89,<tmp13=$85,<comb13=$42
shufb $85,$89,$85,$42

# qhasm: int32323232 tmp24 = (t210 & 0xffff) * (x1_1619 & 0xffff)
# asm 1: mpy >tmp24=vec128#87,<t210=vec128#98,<x1_1619=vec128#3
# asm 2: mpy >tmp24=$89,<t210=$100,<x1_1619=$5
mpy $89,$100,$5

# qhasm: tmp14b = combine tmp13a and tmp14 by comb13
# asm 1: shufb >tmp14b=vec128#85,<tmp13a=vec128#89,<tmp14=vec128#85,<comb13=vec128#40
# asm 2: shufb >tmp14b=$87,<tmp13a=$91,<tmp14=$87,<comb13=$42
shufb $87,$91,$87,$42

# qhasm: int32323232 z3_811 += tmp10b
# asm 1: a >z3_811=vec128#5,<z3_811=vec128#5,<tmp10b=vec128#84
# asm 2: a >z3_811=$7,<z3_811=$7,<tmp10b=$86
a $7,$7,$86

# qhasm: int32323232 z3_1215 += tmp11b
# asm 1: a >z3_1215=vec128#70,<z3_1215=vec128#70,<tmp11b=vec128#77
# asm 2: a >z3_1215=$72,<z3_1215=$72,<tmp11b=$79
a $72,$72,$79

# qhasm: int32323232 z3_1619 += tmp12b
# asm 1: a >z3_1619=vec128#71,<z3_1619=vec128#71,<tmp12b=vec128#82
# asm 2: a >z3_1619=$73,<z3_1619=$73,<tmp12b=$84
a $73,$73,$84

# qhasm: int32323232 z3_2023 += tmp13b
# asm 1: a >z3_2023=vec128#73,<z3_2023=vec128#73,<tmp13b=vec128#83
# asm 2: a >z3_2023=$75,<z3_2023=$75,<tmp13b=$85
a $75,$75,$85

# qhasm: int32323232 z3_2427 += tmp14b
# asm 1: a >z3_2427=vec128#76,<z3_2427=vec128#76,<tmp14b=vec128#85
# asm 2: a >z3_2427=$78,<z3_2427=$78,<tmp14b=$87
a $78,$78,$87

# qhasm: tmp20b = tmp20 >> (8 * 8) 
# asm 1: rotqmbyi >tmp20b=vec128#77,<tmp20=vec128#90,-8
# asm 2: rotqmbyi >tmp20b=$79,<tmp20=$92,-8
rotqmbyi $79,$92,-8

# qhasm: int32323232 z3_2831 = tmp14a << 1
# asm 1: shli >z3_2831=vec128#82,<tmp14a=vec128#88,1
# asm 2: shli >z3_2831=$84,<tmp14a=$90,1
shli $84,$90,1

# qhasm: int32323232 tmp20a = tmp20 << 1
# asm 1: shli >tmp20a=vec128#83,<tmp20=vec128#90,1
# asm 2: shli >tmp20a=$85,<tmp20=$92,1
shli $85,$92,1

# qhasm: int32323232 tmp21a = tmp21 << 1
# asm 1: shli >tmp21a=vec128#84,<tmp21=vec128#91,1
# asm 2: shli >tmp21a=$86,<tmp21=$93,1
shli $86,$93,1

# qhasm: int32323232 tmp22a = tmp22 << 1
# asm 1: shli >tmp22a=vec128#85,<tmp22=vec128#80,1
# asm 2: shli >tmp22a=$87,<tmp22=$82,1
shli $87,$82,1

# qhasm: int32323232 tmp23a = tmp23 << 1
# asm 1: shli >tmp23a=vec128#88,<tmp23=vec128#86,1
# asm 2: shli >tmp23a=$90,<tmp23=$88,1
shli $90,$88,1

# qhasm: tmp24a = tmp24 << (8 * 8)
# asm 1: shlqbyi >tmp24a=vec128#89,<tmp24=vec128#87,8
# asm 2: shlqbyi >tmp24a=$91,<tmp24=$89,8
shlqbyi $91,$89,8

# qhasm: int32323232 tmp30 = (t211 & 0xffff) * (x1_03 & 0xffff)
# asm 1: mpy >tmp30=vec128#90,<t211=vec128#103,<x1_03=vec128#51
# asm 2: mpy >tmp30=$92,<t211=$105,<x1_03=$53
mpy $92,$105,$53

# qhasm: tmp21b = combine tmp20a and tmp21 by comb22
# asm 1: shufb >tmp21b=vec128#83,<tmp20a=vec128#83,<tmp21=vec128#91,<comb22=vec128#41
# asm 2: shufb >tmp21b=$85,<tmp20a=$85,<tmp21=$93,<comb22=$43
shufb $85,$85,$93,$43

# qhasm: int32323232 tmp31 = (t211 & 0xffff) * (x1_47 & 0xffff)
# asm 1: mpy >tmp31=vec128#91,<t211=vec128#103,<x1_47=vec128#52
# asm 2: mpy >tmp31=$93,<t211=$105,<x1_47=$54
mpy $93,$105,$54

# qhasm: tmp22b = combine tmp21a and tmp22 by comb22
# asm 1: shufb >tmp22b=vec128#80,<tmp21a=vec128#84,<tmp22=vec128#80,<comb22=vec128#41
# asm 2: shufb >tmp22b=$82,<tmp21a=$86,<tmp22=$82,<comb22=$43
shufb $82,$86,$82,$43

# qhasm: int32323232 tmp32 = (t211 & 0xffff) * (x1_811 & 0xffff)
# asm 1: mpy >tmp32=vec128#84,<t211=vec128#103,<x1_811=vec128#53
# asm 2: mpy >tmp32=$86,<t211=$105,<x1_811=$55
mpy $86,$105,$55

# qhasm: tmp23b = combine tmp22a and tmp23 by comb22
# asm 1: shufb >tmp23b=vec128#85,<tmp22a=vec128#85,<tmp23=vec128#86,<comb22=vec128#41
# asm 2: shufb >tmp23b=$87,<tmp22a=$87,<tmp23=$88,<comb22=$43
shufb $87,$87,$88,$43

# qhasm: int32323232 tmp33 = (t211 & 0xffff) * (x1_1215 & 0xffff)
# asm 1: mpy >tmp33=vec128#86,<t211=vec128#103,<x1_1215=vec128#54
# asm 2: mpy >tmp33=$88,<t211=$105,<x1_1215=$56
mpy $88,$105,$56

# qhasm: tmp24b = combine tmp23a and tmp24 by comb22
# asm 1: shufb >tmp24b=vec128#87,<tmp23a=vec128#88,<tmp24=vec128#87,<comb22=vec128#41
# asm 2: shufb >tmp24b=$89,<tmp23a=$90,<tmp24=$89,<comb22=$43
shufb $89,$90,$89,$43

# qhasm: int32323232 tmp34 = (t211 & 0xffff) * (x1_1619 & 0xffff)
# asm 1: mpy >tmp34=vec128#88,<t211=vec128#103,<x1_1619=vec128#3
# asm 2: mpy >tmp34=$90,<t211=$105,<x1_1619=$5
mpy $90,$105,$5

# qhasm: int32323232 tmp24a <<= 1
# asm 1: shli >tmp24a=vec128#89,<tmp24a=vec128#89,1
# asm 2: shli >tmp24a=$91,<tmp24a=$91,1
shli $91,$91,1

# qhasm: int32323232 z3_811 += tmp20b
# asm 1: a >z3_811=vec128#5,<z3_811=vec128#5,<tmp20b=vec128#77
# asm 2: a >z3_811=$7,<z3_811=$7,<tmp20b=$79
a $7,$7,$79

# qhasm: int32323232 z3_1215 += tmp21b
# asm 1: a >z3_1215=vec128#70,<z3_1215=vec128#70,<tmp21b=vec128#83
# asm 2: a >z3_1215=$72,<z3_1215=$72,<tmp21b=$85
a $72,$72,$85

# qhasm: int32323232 z3_1619 += tmp22b
# asm 1: a >z3_1619=vec128#71,<z3_1619=vec128#71,<tmp22b=vec128#80
# asm 2: a >z3_1619=$73,<z3_1619=$73,<tmp22b=$82
a $73,$73,$82

# qhasm: int32323232 z3_2023 += tmp23b
# asm 1: a >z3_2023=vec128#73,<z3_2023=vec128#73,<tmp23b=vec128#85
# asm 2: a >z3_2023=$75,<z3_2023=$75,<tmp23b=$87
a $75,$75,$87

# qhasm: int32323232 z3_2427 += tmp24b
# asm 1: a >z3_2427=vec128#76,<z3_2427=vec128#76,<tmp24b=vec128#87
# asm 2: a >z3_2427=$78,<z3_2427=$78,<tmp24b=$89
a $78,$78,$89

# qhasm: tmp30b = tmp30 >> (8 * 12) 
# asm 1: rotqmbyi >tmp30b=vec128#77,<tmp30=vec128#90,-12
# asm 2: rotqmbyi >tmp30b=$79,<tmp30=$92,-12
rotqmbyi $79,$92,-12

# qhasm: int32323232 z3_2831 += tmp24a
# asm 1: a >z3_2831=vec128#80,<z3_2831=vec128#82,<tmp24a=vec128#89
# asm 2: a >z3_2831=$82,<z3_2831=$84,<tmp24a=$91
a $82,$84,$91

# qhasm: int32323232 tmp30a = tmp30 << 1
# asm 1: shli >tmp30a=vec128#82,<tmp30=vec128#90,1
# asm 2: shli >tmp30a=$84,<tmp30=$92,1
shli $84,$92,1

# qhasm: int32323232 tmp31a = tmp31 << 1
# asm 1: shli >tmp31a=vec128#83,<tmp31=vec128#91,1
# asm 2: shli >tmp31a=$85,<tmp31=$93,1
shli $85,$93,1

# qhasm: int32323232 tmp32a = tmp32 << 1
# asm 1: shli >tmp32a=vec128#85,<tmp32=vec128#84,1
# asm 2: shli >tmp32a=$87,<tmp32=$86,1
shli $87,$86,1

# qhasm: int32323232 tmp33a = tmp33 << 1
# asm 1: shli >tmp33a=vec128#87,<tmp33=vec128#86,1
# asm 2: shli >tmp33a=$89,<tmp33=$88,1
shli $89,$88,1

# qhasm: tmp34a = tmp34 << (8 * 4)
# asm 1: shlqbyi >tmp34a=vec128#89,<tmp34=vec128#88,4
# asm 2: shlqbyi >tmp34a=$91,<tmp34=$90,4
shlqbyi $91,$90,4

# qhasm: int32323232 tmp00 = (t212 & 0xffff) * (x1_03 & 0xffff)
# asm 1: mpy >tmp00=vec128#90,<t212=vec128#78,<x1_03=vec128#51
# asm 2: mpy >tmp00=$92,<t212=$80,<x1_03=$53
mpy $92,$80,$53

# qhasm: tmp31b = combine tmp30a and tmp31 by comb31
# asm 1: shufb >tmp31b=vec128#82,<tmp30a=vec128#82,<tmp31=vec128#91,<comb31=vec128#42
# asm 2: shufb >tmp31b=$84,<tmp30a=$84,<tmp31=$93,<comb31=$44
shufb $84,$84,$93,$44

# qhasm: int32323232 tmp01 = (t212 & 0xffff) * (x1_47 & 0xffff)
# asm 1: mpy >tmp01=vec128#91,<t212=vec128#78,<x1_47=vec128#52
# asm 2: mpy >tmp01=$93,<t212=$80,<x1_47=$54
mpy $93,$80,$54

# qhasm: tmp32b = combine tmp31a and tmp32 by comb31
# asm 1: shufb >tmp32b=vec128#83,<tmp31a=vec128#83,<tmp32=vec128#84,<comb31=vec128#42
# asm 2: shufb >tmp32b=$85,<tmp31a=$85,<tmp32=$86,<comb31=$44
shufb $85,$85,$86,$44

# qhasm: int32323232 tmp02 = (t212 & 0xffff) * (x1_811 & 0xffff)
# asm 1: mpy >tmp02=vec128#84,<t212=vec128#78,<x1_811=vec128#53
# asm 2: mpy >tmp02=$86,<t212=$80,<x1_811=$55
mpy $86,$80,$55

# qhasm: tmp33b = combine tmp32a and tmp33 by comb31
# asm 1: shufb >tmp33b=vec128#85,<tmp32a=vec128#85,<tmp33=vec128#86,<comb31=vec128#42
# asm 2: shufb >tmp33b=$87,<tmp32a=$87,<tmp33=$88,<comb31=$44
shufb $87,$87,$88,$44

# qhasm: int32323232 tmp03 = (t212 & 0xffff) * (x1_1215 & 0xffff)
# asm 1: mpy >tmp03=vec128#86,<t212=vec128#78,<x1_1215=vec128#54
# asm 2: mpy >tmp03=$88,<t212=$80,<x1_1215=$56
mpy $88,$80,$56

# qhasm: tmp34b = combine tmp33a and tmp34 by comb31
# asm 1: shufb >tmp34b=vec128#87,<tmp33a=vec128#87,<tmp34=vec128#88,<comb31=vec128#42
# asm 2: shufb >tmp34b=$89,<tmp33a=$89,<tmp34=$90,<comb31=$44
shufb $89,$89,$90,$44

# qhasm: int32323232 tmp04 = (t212 & 0xffff) * (x1_1619 & 0xffff)
# asm 1: mpy >tmp04=vec128#78,<t212=vec128#78,<x1_1619=vec128#3
# asm 2: mpy >tmp04=$80,<t212=$80,<x1_1619=$5
mpy $80,$80,$5

# qhasm: int32323232 tmp34a <<= 1
# asm 1: shli >tmp34a=vec128#88,<tmp34a=vec128#89,1
# asm 2: shli >tmp34a=$90,<tmp34a=$91,1
shli $90,$91,1

# qhasm: int32323232 z3_811 += tmp30b
# asm 1: a >z3_811=vec128#5,<z3_811=vec128#5,<tmp30b=vec128#77
# asm 2: a >z3_811=$7,<z3_811=$7,<tmp30b=$79
a $7,$7,$79

# qhasm: int32323232 z3_1215 += tmp31b
# asm 1: a >z3_1215=vec128#70,<z3_1215=vec128#70,<tmp31b=vec128#82
# asm 2: a >z3_1215=$72,<z3_1215=$72,<tmp31b=$84
a $72,$72,$84

# qhasm: int32323232 z3_1619 += tmp32b
# asm 1: a >z3_1619=vec128#71,<z3_1619=vec128#71,<tmp32b=vec128#83
# asm 2: a >z3_1619=$73,<z3_1619=$73,<tmp32b=$85
a $73,$73,$85

# qhasm: int32323232 z3_2023 += tmp33b
# asm 1: a >z3_2023=vec128#73,<z3_2023=vec128#73,<tmp33b=vec128#85
# asm 2: a >z3_2023=$75,<z3_2023=$75,<tmp33b=$87
a $75,$75,$87

# qhasm: int32323232 z3_2427 += tmp34b
# asm 1: a >z3_2427=vec128#76,<z3_2427=vec128#76,<tmp34b=vec128#87
# asm 2: a >z3_2427=$78,<z3_2427=$78,<tmp34b=$89
a $78,$78,$89

# qhasm: int32323232 z3_2831 += tmp34a
# asm 1: a >z3_2831=vec128#77,<z3_2831=vec128#80,<tmp34a=vec128#88
# asm 2: a >z3_2831=$79,<z3_2831=$82,<tmp34a=$90
a $79,$82,$90

# qhasm: int32323232 tmp10 = (t213 & 0xffff) * (x1_03 & 0xffff)
# asm 1: mpy >tmp10=vec128#80,<t213=vec128#94,<x1_03=vec128#51
# asm 2: mpy >tmp10=$82,<t213=$96,<x1_03=$53
mpy $82,$96,$53

# qhasm: int32323232 tmp11 = (t213 & 0xffff) * (x1_47 & 0xffff)
# asm 1: mpy >tmp11=vec128#82,<t213=vec128#94,<x1_47=vec128#52
# asm 2: mpy >tmp11=$84,<t213=$96,<x1_47=$54
mpy $84,$96,$54

# qhasm: int32323232 tmp12 = (t213 & 0xffff) * (x1_811 & 0xffff)
# asm 1: mpy >tmp12=vec128#83,<t213=vec128#94,<x1_811=vec128#53
# asm 2: mpy >tmp12=$85,<t213=$96,<x1_811=$55
mpy $85,$96,$55

# qhasm: int32323232 tmp13 = (t213 & 0xffff) * (x1_1215 & 0xffff)
# asm 1: mpy >tmp13=vec128#85,<t213=vec128#94,<x1_1215=vec128#54
# asm 2: mpy >tmp13=$87,<t213=$96,<x1_1215=$56
mpy $87,$96,$56

# qhasm: int32323232 tmp14 = (t213 & 0xffff) * (x1_1619 & 0xffff)
# asm 1: mpy >tmp14=vec128#87,<t213=vec128#94,<x1_1619=vec128#3
# asm 2: mpy >tmp14=$89,<t213=$96,<x1_1619=$5
mpy $89,$96,$5

# qhasm: int32323232 z3_1215 += tmp00
# asm 1: a >z3_1215=vec128#70,<z3_1215=vec128#70,<tmp00=vec128#90
# asm 2: a >z3_1215=$72,<z3_1215=$72,<tmp00=$92
a $72,$72,$92

# qhasm: int32323232 z3_1619 += tmp01
# asm 1: a >z3_1619=vec128#71,<z3_1619=vec128#71,<tmp01=vec128#91
# asm 2: a >z3_1619=$73,<z3_1619=$73,<tmp01=$93
a $73,$73,$93

# qhasm: int32323232 z3_2023 += tmp02
# asm 1: a >z3_2023=vec128#73,<z3_2023=vec128#73,<tmp02=vec128#84
# asm 2: a >z3_2023=$75,<z3_2023=$75,<tmp02=$86
a $75,$75,$86

# qhasm: int32323232 z3_2427 += tmp03
# asm 1: a >z3_2427=vec128#76,<z3_2427=vec128#76,<tmp03=vec128#86
# asm 2: a >z3_2427=$78,<z3_2427=$78,<tmp03=$88
a $78,$78,$88

# qhasm: tmp10b = tmp10 >> (8 * 4) 
# asm 1: rotqmbyi >tmp10b=vec128#84,<tmp10=vec128#80,-4
# asm 2: rotqmbyi >tmp10b=$86,<tmp10=$82,-4
rotqmbyi $86,$82,-4

# qhasm: int32323232 z3_2831 += tmp04
# asm 1: a >z3_2831=vec128#77,<z3_2831=vec128#77,<tmp04=vec128#78
# asm 2: a >z3_2831=$79,<z3_2831=$79,<tmp04=$80
a $79,$79,$80

# qhasm: int32323232 tmp10a = tmp10 << 1
# asm 1: shli >tmp10a=vec128#78,<tmp10=vec128#80,1
# asm 2: shli >tmp10a=$80,<tmp10=$82,1
shli $80,$82,1

# qhasm: int32323232 tmp11a = tmp11 << 1
# asm 1: shli >tmp11a=vec128#80,<tmp11=vec128#82,1
# asm 2: shli >tmp11a=$82,<tmp11=$84,1
shli $82,$84,1

# qhasm: int32323232 tmp12a = tmp12 << 1
# asm 1: shli >tmp12a=vec128#86,<tmp12=vec128#83,1
# asm 2: shli >tmp12a=$88,<tmp12=$85,1
shli $88,$85,1

# qhasm: int32323232 tmp13a = tmp13 << 1
# asm 1: shli >tmp13a=vec128#88,<tmp13=vec128#85,1
# asm 2: shli >tmp13a=$90,<tmp13=$87,1
shli $90,$87,1

# qhasm: tmp14a = tmp14 << (8 * 12)
# asm 1: shlqbyi >tmp14a=vec128#89,<tmp14=vec128#87,12
# asm 2: shlqbyi >tmp14a=$91,<tmp14=$89,12
shlqbyi $91,$89,12

# qhasm: int32323232 tmp20 = (t214 & 0xffff) * (x1_03 & 0xffff)
# asm 1: mpy >tmp20=vec128#90,<t214=vec128#100,<x1_03=vec128#51
# asm 2: mpy >tmp20=$92,<t214=$102,<x1_03=$53
mpy $92,$102,$53

# qhasm: tmp11b = combine tmp10a and tmp11 by comb13
# asm 1: shufb >tmp11b=vec128#78,<tmp10a=vec128#78,<tmp11=vec128#82,<comb13=vec128#40
# asm 2: shufb >tmp11b=$80,<tmp10a=$80,<tmp11=$84,<comb13=$42
shufb $80,$80,$84,$42

# qhasm: int32323232 tmp21 = (t214 & 0xffff) * (x1_47 & 0xffff)
# asm 1: mpy >tmp21=vec128#82,<t214=vec128#100,<x1_47=vec128#52
# asm 2: mpy >tmp21=$84,<t214=$102,<x1_47=$54
mpy $84,$102,$54

# qhasm: tmp12b = combine tmp11a and tmp12 by comb13
# asm 1: shufb >tmp12b=vec128#80,<tmp11a=vec128#80,<tmp12=vec128#83,<comb13=vec128#40
# asm 2: shufb >tmp12b=$82,<tmp11a=$82,<tmp12=$85,<comb13=$42
shufb $82,$82,$85,$42

# qhasm: int32323232 tmp22 = (t214 & 0xffff) * (x1_811 & 0xffff)
# asm 1: mpy >tmp22=vec128#83,<t214=vec128#100,<x1_811=vec128#53
# asm 2: mpy >tmp22=$85,<t214=$102,<x1_811=$55
mpy $85,$102,$55

# qhasm: tmp13b = combine tmp12a and tmp13 by comb13
# asm 1: shufb >tmp13b=vec128#85,<tmp12a=vec128#86,<tmp13=vec128#85,<comb13=vec128#40
# asm 2: shufb >tmp13b=$87,<tmp12a=$88,<tmp13=$87,<comb13=$42
shufb $87,$88,$87,$42

# qhasm: int32323232 tmp23 = (t214 & 0xffff) * (x1_1215 & 0xffff)
# asm 1: mpy >tmp23=vec128#86,<t214=vec128#100,<x1_1215=vec128#54
# asm 2: mpy >tmp23=$88,<t214=$102,<x1_1215=$56
mpy $88,$102,$56

# qhasm: tmp14b = combine tmp13a and tmp14 by comb13
# asm 1: shufb >tmp14b=vec128#87,<tmp13a=vec128#88,<tmp14=vec128#87,<comb13=vec128#40
# asm 2: shufb >tmp14b=$89,<tmp13a=$90,<tmp14=$89,<comb13=$42
shufb $89,$90,$89,$42

# qhasm: int32323232 tmp24 = (t214 & 0xffff) * (x1_1619 & 0xffff)
# asm 1: mpy >tmp24=vec128#88,<t214=vec128#100,<x1_1619=vec128#3
# asm 2: mpy >tmp24=$90,<t214=$102,<x1_1619=$5
mpy $90,$102,$5

# qhasm: int32323232 z3_1215 += tmp10b
# asm 1: a >z3_1215=vec128#70,<z3_1215=vec128#70,<tmp10b=vec128#84
# asm 2: a >z3_1215=$72,<z3_1215=$72,<tmp10b=$86
a $72,$72,$86

# qhasm: int32323232 z3_1619 += tmp11b
# asm 1: a >z3_1619=vec128#71,<z3_1619=vec128#71,<tmp11b=vec128#78
# asm 2: a >z3_1619=$73,<z3_1619=$73,<tmp11b=$80
a $73,$73,$80

# qhasm: int32323232 z3_2023 += tmp12b
# asm 1: a >z3_2023=vec128#73,<z3_2023=vec128#73,<tmp12b=vec128#80
# asm 2: a >z3_2023=$75,<z3_2023=$75,<tmp12b=$82
a $75,$75,$82

# qhasm: int32323232 z3_2427 += tmp13b
# asm 1: a >z3_2427=vec128#76,<z3_2427=vec128#76,<tmp13b=vec128#85
# asm 2: a >z3_2427=$78,<z3_2427=$78,<tmp13b=$87
a $78,$78,$87

# qhasm: int32323232 z3_2831 += tmp14b
# asm 1: a >z3_2831=vec128#77,<z3_2831=vec128#77,<tmp14b=vec128#87
# asm 2: a >z3_2831=$79,<z3_2831=$79,<tmp14b=$89
a $79,$79,$89

# qhasm: int32323232 z3_3235 = tmp14a << 1
# asm 1: shli >z3_3235=vec128#78,<tmp14a=vec128#89,1
# asm 2: shli >z3_3235=$80,<tmp14a=$91,1
shli $80,$91,1

# qhasm: tmp20b = tmp20 >> (8 * 8) 
# asm 1: rotqmbyi >tmp20b=vec128#80,<tmp20=vec128#90,-8
# asm 2: rotqmbyi >tmp20b=$82,<tmp20=$92,-8
rotqmbyi $82,$92,-8

# qhasm: int32323232 tmp20a = tmp20 << 1
# asm 1: shli >tmp20a=vec128#84,<tmp20=vec128#90,1
# asm 2: shli >tmp20a=$86,<tmp20=$92,1
shli $86,$92,1

# qhasm: int32323232 tmp21a = tmp21 << 1
# asm 1: shli >tmp21a=vec128#85,<tmp21=vec128#82,1
# asm 2: shli >tmp21a=$87,<tmp21=$84,1
shli $87,$84,1

# qhasm: int32323232 tmp22a = tmp22 << 1
# asm 1: shli >tmp22a=vec128#87,<tmp22=vec128#83,1
# asm 2: shli >tmp22a=$89,<tmp22=$85,1
shli $89,$85,1

# qhasm: tmp24a = tmp24 << (8 * 8)
# asm 1: shlqbyi >tmp24a=vec128#89,<tmp24=vec128#88,8
# asm 2: shlqbyi >tmp24a=$91,<tmp24=$90,8
shlqbyi $91,$90,8

# qhasm: int32323232 tmp23a = tmp23 << 1
# asm 1: shli >tmp23a=vec128#90,<tmp23=vec128#86,1
# asm 2: shli >tmp23a=$92,<tmp23=$88,1
shli $92,$88,1

# qhasm: int32323232 tmp30 = (t215 & 0xffff) * (x1_03 & 0xffff)
# asm 1: mpy >tmp30=vec128#91,<t215=vec128#104,<x1_03=vec128#51
# asm 2: mpy >tmp30=$93,<t215=$106,<x1_03=$53
mpy $93,$106,$53

# qhasm: int32323232 tmp31 = (t215 & 0xffff) * (x1_47 & 0xffff)
# asm 1: mpy >tmp31=vec128#92,<t215=vec128#104,<x1_47=vec128#52
# asm 2: mpy >tmp31=$94,<t215=$106,<x1_47=$54
mpy $94,$106,$54

# qhasm: tmp21b = combine tmp20a and tmp21 by comb22
# asm 1: shufb >tmp21b=vec128#82,<tmp20a=vec128#84,<tmp21=vec128#82,<comb22=vec128#41
# asm 2: shufb >tmp21b=$84,<tmp20a=$86,<tmp21=$84,<comb22=$43
shufb $84,$86,$84,$43

# qhasm: int32323232 tmp32 = (t215 & 0xffff) * (x1_811 & 0xffff)
# asm 1: mpy >tmp32=vec128#84,<t215=vec128#104,<x1_811=vec128#53
# asm 2: mpy >tmp32=$86,<t215=$106,<x1_811=$55
mpy $86,$106,$55

# qhasm: tmp22b = combine tmp21a and tmp22 by comb22
# asm 1: shufb >tmp22b=vec128#83,<tmp21a=vec128#85,<tmp22=vec128#83,<comb22=vec128#41
# asm 2: shufb >tmp22b=$85,<tmp21a=$87,<tmp22=$85,<comb22=$43
shufb $85,$87,$85,$43

# qhasm: int32323232 tmp33 = (t215 & 0xffff) * (x1_1215 & 0xffff)
# asm 1: mpy >tmp33=vec128#85,<t215=vec128#104,<x1_1215=vec128#54
# asm 2: mpy >tmp33=$87,<t215=$106,<x1_1215=$56
mpy $87,$106,$56

# qhasm: tmp23b = combine tmp22a and tmp23 by comb22
# asm 1: shufb >tmp23b=vec128#86,<tmp22a=vec128#87,<tmp23=vec128#86,<comb22=vec128#41
# asm 2: shufb >tmp23b=$88,<tmp22a=$89,<tmp23=$88,<comb22=$43
shufb $88,$89,$88,$43

# qhasm: int32323232 tmp34 = (t215 & 0xffff) * (x1_1619 & 0xffff)
# asm 1: mpy >tmp34=vec128#87,<t215=vec128#104,<x1_1619=vec128#3
# asm 2: mpy >tmp34=$89,<t215=$106,<x1_1619=$5
mpy $89,$106,$5

# qhasm: tmp24b = combine tmp23a and tmp24 by comb22
# asm 1: shufb >tmp24b=vec128#88,<tmp23a=vec128#90,<tmp24=vec128#88,<comb22=vec128#41
# asm 2: shufb >tmp24b=$90,<tmp23a=$92,<tmp24=$90,<comb22=$43
shufb $90,$92,$90,$43

# qhasm: int32323232 tmp24a <<= 1
# asm 1: shli >tmp24a=vec128#89,<tmp24a=vec128#89,1
# asm 2: shli >tmp24a=$91,<tmp24a=$91,1
shli $91,$91,1

# qhasm: int32323232 z3_1215 += tmp20b
# asm 1: a >z3_1215=vec128#70,<z3_1215=vec128#70,<tmp20b=vec128#80
# asm 2: a >z3_1215=$72,<z3_1215=$72,<tmp20b=$82
a $72,$72,$82

# qhasm: int32323232 z3_1619 += tmp21b
# asm 1: a >z3_1619=vec128#71,<z3_1619=vec128#71,<tmp21b=vec128#82
# asm 2: a >z3_1619=$73,<z3_1619=$73,<tmp21b=$84
a $73,$73,$84

# qhasm: int32323232 z3_2023 += tmp22b
# asm 1: a >z3_2023=vec128#73,<z3_2023=vec128#73,<tmp22b=vec128#83
# asm 2: a >z3_2023=$75,<z3_2023=$75,<tmp22b=$85
a $75,$75,$85

# qhasm: int32323232 z3_2427 += tmp23b
# asm 1: a >z3_2427=vec128#76,<z3_2427=vec128#76,<tmp23b=vec128#86
# asm 2: a >z3_2427=$78,<z3_2427=$78,<tmp23b=$88
a $78,$78,$88

# qhasm: int32323232 z3_2831 += tmp24b
# asm 1: a >z3_2831=vec128#77,<z3_2831=vec128#77,<tmp24b=vec128#88
# asm 2: a >z3_2831=$79,<z3_2831=$79,<tmp24b=$90
a $79,$79,$90

# qhasm: int32323232 z3_3235 += tmp24a
# asm 1: a >z3_3235=vec128#78,<z3_3235=vec128#78,<tmp24a=vec128#89
# asm 2: a >z3_3235=$80,<z3_3235=$80,<tmp24a=$91
a $80,$80,$91

# qhasm: tmp30b = tmp30 >> (8 * 12) 
# asm 1: rotqmbyi >tmp30b=vec128#80,<tmp30=vec128#91,-12
# asm 2: rotqmbyi >tmp30b=$82,<tmp30=$93,-12
rotqmbyi $82,$93,-12

# qhasm: int32323232 tmp30a = tmp30 << 1
# asm 1: shli >tmp30a=vec128#82,<tmp30=vec128#91,1
# asm 2: shli >tmp30a=$84,<tmp30=$93,1
shli $84,$93,1

# qhasm: int32323232 tmp31a = tmp31 << 1
# asm 1: shli >tmp31a=vec128#83,<tmp31=vec128#92,1
# asm 2: shli >tmp31a=$85,<tmp31=$94,1
shli $85,$94,1

# qhasm: int32323232 tmp32a = tmp32 << 1
# asm 1: shli >tmp32a=vec128#86,<tmp32=vec128#84,1
# asm 2: shli >tmp32a=$88,<tmp32=$86,1
shli $88,$86,1

# qhasm: tmp34a = tmp34 << (8 * 4)
# asm 1: shlqbyi >tmp34a=vec128#88,<tmp34=vec128#87,4
# asm 2: shlqbyi >tmp34a=$90,<tmp34=$89,4
shlqbyi $90,$89,4

# qhasm: int32323232 tmp33a = tmp33 << 1
# asm 1: shli >tmp33a=vec128#89,<tmp33=vec128#85,1
# asm 2: shli >tmp33a=$91,<tmp33=$87,1
shli $91,$87,1

# qhasm: int32323232 tmp00 = (t216 & 0xffff) * (x1_03 & 0xffff)
# asm 1: mpy >tmp00=vec128#90,<t216=vec128#79,<x1_03=vec128#51
# asm 2: mpy >tmp00=$92,<t216=$81,<x1_03=$53
mpy $92,$81,$53

# qhasm: int32323232 tmp01 = (t216 & 0xffff) * (x1_47 & 0xffff)
# asm 1: mpy >tmp01=vec128#91,<t216=vec128#79,<x1_47=vec128#52
# asm 2: mpy >tmp01=$93,<t216=$81,<x1_47=$54
mpy $93,$81,$54

# qhasm: tmp31b = combine tmp30a and tmp31 by comb31
# asm 1: shufb >tmp31b=vec128#82,<tmp30a=vec128#82,<tmp31=vec128#92,<comb31=vec128#42
# asm 2: shufb >tmp31b=$84,<tmp30a=$84,<tmp31=$94,<comb31=$44
shufb $84,$84,$94,$44

# qhasm: int32323232 tmp02 = (t216 & 0xffff) * (x1_811 & 0xffff)
# asm 1: mpy >tmp02=vec128#92,<t216=vec128#79,<x1_811=vec128#53
# asm 2: mpy >tmp02=$94,<t216=$81,<x1_811=$55
mpy $94,$81,$55

# qhasm: tmp32b = combine tmp31a and tmp32 by comb31
# asm 1: shufb >tmp32b=vec128#83,<tmp31a=vec128#83,<tmp32=vec128#84,<comb31=vec128#42
# asm 2: shufb >tmp32b=$85,<tmp31a=$85,<tmp32=$86,<comb31=$44
shufb $85,$85,$86,$44

# qhasm: int32323232 tmp03 = (t216 & 0xffff) * (x1_1215 & 0xffff)
# asm 1: mpy >tmp03=vec128#84,<t216=vec128#79,<x1_1215=vec128#54
# asm 2: mpy >tmp03=$86,<t216=$81,<x1_1215=$56
mpy $86,$81,$56

# qhasm: tmp33b = combine tmp32a and tmp33 by comb31
# asm 1: shufb >tmp33b=vec128#85,<tmp32a=vec128#86,<tmp33=vec128#85,<comb31=vec128#42
# asm 2: shufb >tmp33b=$87,<tmp32a=$88,<tmp33=$87,<comb31=$44
shufb $87,$88,$87,$44

# qhasm: int32323232 tmp04 = (t216 & 0xffff) * (x1_1619 & 0xffff)
# asm 1: mpy >tmp04=vec128#79,<t216=vec128#79,<x1_1619=vec128#3
# asm 2: mpy >tmp04=$81,<t216=$81,<x1_1619=$5
mpy $81,$81,$5

# qhasm: tmp34b = combine tmp33a and tmp34 by comb31
# asm 1: shufb >tmp34b=vec128#86,<tmp33a=vec128#89,<tmp34=vec128#87,<comb31=vec128#42
# asm 2: shufb >tmp34b=$88,<tmp33a=$91,<tmp34=$89,<comb31=$44
shufb $88,$91,$89,$44

# qhasm: int32323232 tmp34a <<= 1
# asm 1: shli >tmp34a=vec128#87,<tmp34a=vec128#88,1
# asm 2: shli >tmp34a=$89,<tmp34a=$90,1
shli $89,$90,1

# qhasm: int32323232 z3_1215 += tmp30b
# asm 1: a >z3_1215=vec128#70,<z3_1215=vec128#70,<tmp30b=vec128#80
# asm 2: a >z3_1215=$72,<z3_1215=$72,<tmp30b=$82
a $72,$72,$82

# qhasm: int32323232 z3_1619 += tmp31b
# asm 1: a >z3_1619=vec128#71,<z3_1619=vec128#71,<tmp31b=vec128#82
# asm 2: a >z3_1619=$73,<z3_1619=$73,<tmp31b=$84
a $73,$73,$84

# qhasm: int32323232 z3_2023 += tmp32b
# asm 1: a >z3_2023=vec128#73,<z3_2023=vec128#73,<tmp32b=vec128#83
# asm 2: a >z3_2023=$75,<z3_2023=$75,<tmp32b=$85
a $75,$75,$85

# qhasm: int32323232 z3_2427 += tmp33b
# asm 1: a >z3_2427=vec128#76,<z3_2427=vec128#76,<tmp33b=vec128#85
# asm 2: a >z3_2427=$78,<z3_2427=$78,<tmp33b=$87
a $78,$78,$87

# qhasm: int32323232 z3_2831 += tmp34b
# asm 1: a >z3_2831=vec128#77,<z3_2831=vec128#77,<tmp34b=vec128#86
# asm 2: a >z3_2831=$79,<z3_2831=$79,<tmp34b=$88
a $79,$79,$88

# qhasm: int32323232 z3_3235 += tmp34a
# asm 1: a >z3_3235=vec128#78,<z3_3235=vec128#78,<tmp34a=vec128#87
# asm 2: a >z3_3235=$80,<z3_3235=$80,<tmp34a=$89
a $80,$80,$89

# qhasm: int32323232 tmp10 = (t217 & 0xffff) * (x1_03 & 0xffff)
# asm 1: mpy >tmp10=vec128#80,<t217=vec128#95,<x1_03=vec128#51
# asm 2: mpy >tmp10=$82,<t217=$97,<x1_03=$53
mpy $82,$97,$53

# qhasm: int32323232 tmp11 = (t217 & 0xffff) * (x1_47 & 0xffff)
# asm 1: mpy >tmp11=vec128#82,<t217=vec128#95,<x1_47=vec128#52
# asm 2: mpy >tmp11=$84,<t217=$97,<x1_47=$54
mpy $84,$97,$54

# qhasm: int32323232 tmp12 = (t217 & 0xffff) * (x1_811 & 0xffff)
# asm 1: mpy >tmp12=vec128#83,<t217=vec128#95,<x1_811=vec128#53
# asm 2: mpy >tmp12=$85,<t217=$97,<x1_811=$55
mpy $85,$97,$55

# qhasm: int32323232 tmp13 = (t217 & 0xffff) * (x1_1215 & 0xffff)
# asm 1: mpy >tmp13=vec128#85,<t217=vec128#95,<x1_1215=vec128#54
# asm 2: mpy >tmp13=$87,<t217=$97,<x1_1215=$56
mpy $87,$97,$56

# qhasm: int32323232 tmp14 = (t217 & 0xffff) * (x1_1619 & 0xffff)
# asm 1: mpy >tmp14=vec128#86,<t217=vec128#95,<x1_1619=vec128#3
# asm 2: mpy >tmp14=$88,<t217=$97,<x1_1619=$5
mpy $88,$97,$5

# qhasm: int32323232 z3_1619 += tmp00
# asm 1: a >z3_1619=vec128#71,<z3_1619=vec128#71,<tmp00=vec128#90
# asm 2: a >z3_1619=$73,<z3_1619=$73,<tmp00=$92
a $73,$73,$92

# qhasm: int32323232 z3_2023 += tmp01
# asm 1: a >z3_2023=vec128#73,<z3_2023=vec128#73,<tmp01=vec128#91
# asm 2: a >z3_2023=$75,<z3_2023=$75,<tmp01=$93
a $75,$75,$93

# qhasm: int32323232 z3_2427 += tmp02
# asm 1: a >z3_2427=vec128#76,<z3_2427=vec128#76,<tmp02=vec128#92
# asm 2: a >z3_2427=$78,<z3_2427=$78,<tmp02=$94
a $78,$78,$94

# qhasm: int32323232 z3_2831 += tmp03
# asm 1: a >z3_2831=vec128#77,<z3_2831=vec128#77,<tmp03=vec128#84
# asm 2: a >z3_2831=$79,<z3_2831=$79,<tmp03=$86
a $79,$79,$86

# qhasm: int32323232 z3_3235 += tmp04
# asm 1: a >z3_3235=vec128#78,<z3_3235=vec128#78,<tmp04=vec128#79
# asm 2: a >z3_3235=$80,<z3_3235=$80,<tmp04=$81
a $80,$80,$81

# qhasm: tmp10b = tmp10 >> (8 * 4) 
# asm 1: rotqmbyi >tmp10b=vec128#79,<tmp10=vec128#80,-4
# asm 2: rotqmbyi >tmp10b=$81,<tmp10=$82,-4
rotqmbyi $81,$82,-4

# qhasm: int32323232 tmp10a = tmp10 << 1
# asm 1: shli >tmp10a=vec128#80,<tmp10=vec128#80,1
# asm 2: shli >tmp10a=$82,<tmp10=$82,1
shli $82,$82,1

# qhasm: int32323232 tmp11a = tmp11 << 1
# asm 1: shli >tmp11a=vec128#84,<tmp11=vec128#82,1
# asm 2: shli >tmp11a=$86,<tmp11=$84,1
shli $86,$84,1

# qhasm: int32323232 tmp12a = tmp12 << 1
# asm 1: shli >tmp12a=vec128#87,<tmp12=vec128#83,1
# asm 2: shli >tmp12a=$89,<tmp12=$85,1
shli $89,$85,1

# qhasm: tmp14a = tmp14 << (8 * 12)
# asm 1: shlqbyi >tmp14a=vec128#88,<tmp14=vec128#86,12
# asm 2: shlqbyi >tmp14a=$90,<tmp14=$88,12
shlqbyi $90,$88,12

# qhasm: int32323232 tmp13a = tmp13 << 1
# asm 1: shli >tmp13a=vec128#89,<tmp13=vec128#85,1
# asm 2: shli >tmp13a=$91,<tmp13=$87,1
shli $91,$87,1

# qhasm: int32323232 tmp20 = (t218 & 0xffff) * (x1_03 & 0xffff)
# asm 1: mpy >tmp20=vec128#90,<t218=vec128#101,<x1_03=vec128#51
# asm 2: mpy >tmp20=$92,<t218=$103,<x1_03=$53
mpy $92,$103,$53

# qhasm: int32323232 tmp21 = (t218 & 0xffff) * (x1_47 & 0xffff)
# asm 1: mpy >tmp21=vec128#91,<t218=vec128#101,<x1_47=vec128#52
# asm 2: mpy >tmp21=$93,<t218=$103,<x1_47=$54
mpy $93,$103,$54

# qhasm: tmp11b = combine tmp10a and tmp11 by comb13
# asm 1: shufb >tmp11b=vec128#80,<tmp10a=vec128#80,<tmp11=vec128#82,<comb13=vec128#40
# asm 2: shufb >tmp11b=$82,<tmp10a=$82,<tmp11=$84,<comb13=$42
shufb $82,$82,$84,$42

# qhasm: int32323232 tmp22 = (t218 & 0xffff) * (x1_811 & 0xffff)
# asm 1: mpy >tmp22=vec128#82,<t218=vec128#101,<x1_811=vec128#53
# asm 2: mpy >tmp22=$84,<t218=$103,<x1_811=$55
mpy $84,$103,$55

# qhasm: tmp12b = combine tmp11a and tmp12 by comb13
# asm 1: shufb >tmp12b=vec128#83,<tmp11a=vec128#84,<tmp12=vec128#83,<comb13=vec128#40
# asm 2: shufb >tmp12b=$85,<tmp11a=$86,<tmp12=$85,<comb13=$42
shufb $85,$86,$85,$42

# qhasm: int32323232 tmp23 = (t218 & 0xffff) * (x1_1215 & 0xffff)
# asm 1: mpy >tmp23=vec128#84,<t218=vec128#101,<x1_1215=vec128#54
# asm 2: mpy >tmp23=$86,<t218=$103,<x1_1215=$56
mpy $86,$103,$56

# qhasm: tmp13b = combine tmp12a and tmp13 by comb13
# asm 1: shufb >tmp13b=vec128#85,<tmp12a=vec128#87,<tmp13=vec128#85,<comb13=vec128#40
# asm 2: shufb >tmp13b=$87,<tmp12a=$89,<tmp13=$87,<comb13=$42
shufb $87,$89,$87,$42

# qhasm: int32323232 tmp24 = (t218 & 0xffff) * (x1_1619 & 0xffff)
# asm 1: mpy >tmp24=vec128#87,<t218=vec128#101,<x1_1619=vec128#3
# asm 2: mpy >tmp24=$89,<t218=$103,<x1_1619=$5
mpy $89,$103,$5

# qhasm: tmp14b = combine tmp13a and tmp14 by comb13
# asm 1: shufb >tmp14b=vec128#86,<tmp13a=vec128#89,<tmp14=vec128#86,<comb13=vec128#40
# asm 2: shufb >tmp14b=$88,<tmp13a=$91,<tmp14=$88,<comb13=$42
shufb $88,$91,$88,$42

# qhasm: int32323232 z3_1619 += tmp10b
# asm 1: a >z3_1619=vec128#71,<z3_1619=vec128#71,<tmp10b=vec128#79
# asm 2: a >z3_1619=$73,<z3_1619=$73,<tmp10b=$81
a $73,$73,$81

# qhasm: int32323232 z3_2023 += tmp11b
# asm 1: a >z3_2023=vec128#73,<z3_2023=vec128#73,<tmp11b=vec128#80
# asm 2: a >z3_2023=$75,<z3_2023=$75,<tmp11b=$82
a $75,$75,$82

# qhasm: int32323232 z3_2427 += tmp12b
# asm 1: a >z3_2427=vec128#76,<z3_2427=vec128#76,<tmp12b=vec128#83
# asm 2: a >z3_2427=$78,<z3_2427=$78,<tmp12b=$85
a $78,$78,$85

# qhasm: int32323232 z3_2831 += tmp13b
# asm 1: a >z3_2831=vec128#77,<z3_2831=vec128#77,<tmp13b=vec128#85
# asm 2: a >z3_2831=$79,<z3_2831=$79,<tmp13b=$87
a $79,$79,$87

# qhasm: int32323232 z3_3235 += tmp14b
# asm 1: a >z3_3235=vec128#78,<z3_3235=vec128#78,<tmp14b=vec128#86
# asm 2: a >z3_3235=$80,<z3_3235=$80,<tmp14b=$88
a $80,$80,$88

# qhasm: tmp20b = tmp20 >> (8 * 8) 
# asm 1: rotqmbyi >tmp20b=vec128#79,<tmp20=vec128#90,-8
# asm 2: rotqmbyi >tmp20b=$81,<tmp20=$92,-8
rotqmbyi $81,$92,-8

# qhasm: int32323232 z3_3639 = tmp14a << 1
# asm 1: shli >z3_3639=vec128#80,<tmp14a=vec128#88,1
# asm 2: shli >z3_3639=$82,<tmp14a=$90,1
shli $82,$90,1

# qhasm: int32323232 tmp20a = tmp20 << 1
# asm 1: shli >tmp20a=vec128#83,<tmp20=vec128#90,1
# asm 2: shli >tmp20a=$85,<tmp20=$92,1
shli $85,$92,1

# qhasm: int32323232 tmp21a = tmp21 << 1
# asm 1: shli >tmp21a=vec128#85,<tmp21=vec128#91,1
# asm 2: shli >tmp21a=$87,<tmp21=$93,1
shli $87,$93,1

# qhasm: int32323232 tmp22a = tmp22 << 1
# asm 1: shli >tmp22a=vec128#86,<tmp22=vec128#82,1
# asm 2: shli >tmp22a=$88,<tmp22=$84,1
shli $88,$84,1

# qhasm: int32323232 tmp23a = tmp23 << 1
# asm 1: shli >tmp23a=vec128#88,<tmp23=vec128#84,1
# asm 2: shli >tmp23a=$90,<tmp23=$86,1
shli $90,$86,1

# qhasm: tmp24a = tmp24 << (8 * 8)
# asm 1: shlqbyi >tmp24a=vec128#89,<tmp24=vec128#87,8
# asm 2: shlqbyi >tmp24a=$91,<tmp24=$89,8
shlqbyi $91,$89,8

# qhasm: int32323232 tmp30 = (t219 & 0xffff) * (x1_03 & 0xffff)
# asm 1: mpy >tmp30=vec128#90,<t219=vec128#105,<x1_03=vec128#51
# asm 2: mpy >tmp30=$92,<t219=$107,<x1_03=$53
mpy $92,$107,$53

# qhasm: tmp21b = combine tmp20a and tmp21 by comb22
# asm 1: shufb >tmp21b=vec128#83,<tmp20a=vec128#83,<tmp21=vec128#91,<comb22=vec128#41
# asm 2: shufb >tmp21b=$85,<tmp20a=$85,<tmp21=$93,<comb22=$43
shufb $85,$85,$93,$43

# qhasm: int32323232 tmp31 = (t219 & 0xffff) * (x1_47 & 0xffff)
# asm 1: mpy >tmp31=vec128#91,<t219=vec128#105,<x1_47=vec128#52
# asm 2: mpy >tmp31=$93,<t219=$107,<x1_47=$54
mpy $93,$107,$54

# qhasm: tmp22b = combine tmp21a and tmp22 by comb22
# asm 1: shufb >tmp22b=vec128#82,<tmp21a=vec128#85,<tmp22=vec128#82,<comb22=vec128#41
# asm 2: shufb >tmp22b=$84,<tmp21a=$87,<tmp22=$84,<comb22=$43
shufb $84,$87,$84,$43

# qhasm: int32323232 tmp32 = (t219 & 0xffff) * (x1_811 & 0xffff)
# asm 1: mpy >tmp32=vec128#85,<t219=vec128#105,<x1_811=vec128#53
# asm 2: mpy >tmp32=$87,<t219=$107,<x1_811=$55
mpy $87,$107,$55

# qhasm: tmp23b = combine tmp22a and tmp23 by comb22
# asm 1: shufb >tmp23b=vec128#84,<tmp22a=vec128#86,<tmp23=vec128#84,<comb22=vec128#41
# asm 2: shufb >tmp23b=$86,<tmp22a=$88,<tmp23=$86,<comb22=$43
shufb $86,$88,$86,$43

# qhasm: int32323232 tmp33 = (t219 & 0xffff) * (x1_1215 & 0xffff)
# asm 1: mpy >tmp33=vec128#86,<t219=vec128#105,<x1_1215=vec128#54
# asm 2: mpy >tmp33=$88,<t219=$107,<x1_1215=$56
mpy $88,$107,$56

# qhasm: tmp24b = combine tmp23a and tmp24 by comb22
# asm 1: shufb >tmp24b=vec128#87,<tmp23a=vec128#88,<tmp24=vec128#87,<comb22=vec128#41
# asm 2: shufb >tmp24b=$89,<tmp23a=$90,<tmp24=$89,<comb22=$43
shufb $89,$90,$89,$43

# qhasm: int32323232 tmp34 = (t219 & 0xffff) * (x1_1619 & 0xffff)
# asm 1: mpy >tmp34=vec128#88,<t219=vec128#105,<x1_1619=vec128#3
# asm 2: mpy >tmp34=$90,<t219=$107,<x1_1619=$5
mpy $90,$107,$5

# qhasm: int32323232 tmp24a <<= 1
# asm 1: shli >tmp24a=vec128#89,<tmp24a=vec128#89,1
# asm 2: shli >tmp24a=$91,<tmp24a=$91,1
shli $91,$91,1

# qhasm: int32323232 z3_1619 += tmp20b
# asm 1: a >z3_1619=vec128#71,<z3_1619=vec128#71,<tmp20b=vec128#79
# asm 2: a >z3_1619=$73,<z3_1619=$73,<tmp20b=$81
a $73,$73,$81

# qhasm: int32323232 z3_2023 += tmp21b
# asm 1: a >z3_2023=vec128#73,<z3_2023=vec128#73,<tmp21b=vec128#83
# asm 2: a >z3_2023=$75,<z3_2023=$75,<tmp21b=$85
a $75,$75,$85

# qhasm: int32323232 tmp30a = tmp30 << 1
# asm 1: shli >tmp30a=vec128#79,<tmp30=vec128#90,1
# asm 2: shli >tmp30a=$81,<tmp30=$92,1
shli $81,$92,1

# qhasm: tmp30b = tmp30 >> (8 * 12) 
# asm 1: rotqmbyi >tmp30b=vec128#83,<tmp30=vec128#90,-12
# asm 2: rotqmbyi >tmp30b=$85,<tmp30=$92,-12
rotqmbyi $85,$92,-12

# qhasm: int32323232 tmp31a = tmp31 << 1
# asm 1: shli >tmp31a=vec128#90,<tmp31=vec128#91,1
# asm 2: shli >tmp31a=$92,<tmp31=$93,1
shli $92,$93,1

# qhasm: int32323232 tmp32a = tmp32 << 1
# asm 1: shli >tmp32a=vec128#92,<tmp32=vec128#85,1
# asm 2: shli >tmp32a=$94,<tmp32=$87,1
shli $94,$87,1

# qhasm: int32323232 tmp33a = tmp33 << 1
# asm 1: shli >tmp33a=vec128#93,<tmp33=vec128#86,1
# asm 2: shli >tmp33a=$95,<tmp33=$88,1
shli $95,$88,1

# qhasm: tmp34a = tmp34 << (8 * 4)
# asm 1: shlqbyi >tmp34a=vec128#94,<tmp34=vec128#88,4
# asm 2: shlqbyi >tmp34a=$96,<tmp34=$90,4
shlqbyi $96,$90,4

# qhasm: int32323232 z3_2427 += tmp22b
# asm 1: a >z3_2427=vec128#76,<z3_2427=vec128#76,<tmp22b=vec128#82
# asm 2: a >z3_2427=$78,<z3_2427=$78,<tmp22b=$84
a $78,$78,$84

# qhasm: tmp31b = combine tmp30a and tmp31 by comb31
# asm 1: shufb >tmp31b=vec128#79,<tmp30a=vec128#79,<tmp31=vec128#91,<comb31=vec128#42
# asm 2: shufb >tmp31b=$81,<tmp30a=$81,<tmp31=$93,<comb31=$44
shufb $81,$81,$93,$44

# qhasm: int32323232 z3_2831 += tmp23b
# asm 1: a >z3_2831=vec128#77,<z3_2831=vec128#77,<tmp23b=vec128#84
# asm 2: a >z3_2831=$79,<z3_2831=$79,<tmp23b=$86
a $79,$79,$86

# qhasm: tmp32b = combine tmp31a and tmp32 by comb31
# asm 1: shufb >tmp32b=vec128#82,<tmp31a=vec128#90,<tmp32=vec128#85,<comb31=vec128#42
# asm 2: shufb >tmp32b=$84,<tmp31a=$92,<tmp32=$87,<comb31=$44
shufb $84,$92,$87,$44

# qhasm: int32323232 z3_3235 += tmp24b
# asm 1: a >z3_3235=vec128#78,<z3_3235=vec128#78,<tmp24b=vec128#87
# asm 2: a >z3_3235=$80,<z3_3235=$80,<tmp24b=$89
a $80,$80,$89

# qhasm: tmp33b = combine tmp32a and tmp33 by comb31
# asm 1: shufb >tmp33b=vec128#84,<tmp32a=vec128#92,<tmp33=vec128#86,<comb31=vec128#42
# asm 2: shufb >tmp33b=$86,<tmp32a=$94,<tmp33=$88,<comb31=$44
shufb $86,$94,$88,$44

# qhasm: int32323232 z3_3639 += tmp24a
# asm 1: a >z3_3639=vec128#80,<z3_3639=vec128#80,<tmp24a=vec128#89
# asm 2: a >z3_3639=$82,<z3_3639=$82,<tmp24a=$91
a $82,$82,$91

# qhasm: tmp34b = combine tmp33a and tmp34 by comb31
# asm 1: shufb >tmp34b=vec128#85,<tmp33a=vec128#93,<tmp34=vec128#88,<comb31=vec128#42
# asm 2: shufb >tmp34b=$87,<tmp33a=$95,<tmp34=$90,<comb31=$44
shufb $87,$95,$90,$44

# qhasm: int32323232 tmp34a <<= 1
# asm 1: shli >tmp34a=vec128#86,<tmp34a=vec128#94,1
# asm 2: shli >tmp34a=$88,<tmp34a=$96,1
shli $88,$96,1

# qhasm: int32323232 z3_1619 += tmp30b
# asm 1: a >z3_1619=vec128#71,<z3_1619=vec128#71,<tmp30b=vec128#83
# asm 2: a >z3_1619=$73,<z3_1619=$73,<tmp30b=$85
a $73,$73,$85

# qhasm: int32323232 z3_2023 += tmp31b
# asm 1: a >z3_2023=vec128#73,<z3_2023=vec128#73,<tmp31b=vec128#79
# asm 2: a >z3_2023=$75,<z3_2023=$75,<tmp31b=$81
a $75,$75,$81

# qhasm: int32323232 z3_2427 += tmp32b
# asm 1: a >z3_2427=vec128#76,<z3_2427=vec128#76,<tmp32b=vec128#82
# asm 2: a >z3_2427=$78,<z3_2427=$78,<tmp32b=$84
a $78,$78,$84

# qhasm: int32323232 z3_2831 += tmp33b
# asm 1: a >z3_2831=vec128#77,<z3_2831=vec128#77,<tmp33b=vec128#84
# asm 2: a >z3_2831=$79,<z3_2831=$79,<tmp33b=$86
a $79,$79,$86

# qhasm: int32323232 z3_3235 += tmp34b
# asm 1: a >z3_3235=vec128#78,<z3_3235=vec128#78,<tmp34b=vec128#85
# asm 2: a >z3_3235=$80,<z3_3235=$80,<tmp34b=$87
a $80,$80,$87

# qhasm: int32323232 z3_3639 += tmp34a
# asm 1: a >z3_3639=vec128#79,<z3_3639=vec128#80,<tmp34a=vec128#86
# asm 2: a >z3_3639=$81,<z3_3639=$82,<tmp34a=$88
a $81,$82,$88

# qhasm: carry0 = select bytes from z3_2023 by sel01
# asm 1: shufb >carry0=vec128#80,<z3_2023=vec128#73,<z3_2023=vec128#73,<sel01=vec128#31
# asm 2: shufb >carry0=$82,<z3_2023=$75,<z3_2023=$75,<sel01=$33
shufb $82,$75,$75,$33

# qhasm: carry1 = select bytes from z3_2427 by sel01
# asm 1: shufb >carry1=vec128#82,<z3_2427=vec128#76,<z3_2427=vec128#76,<sel01=vec128#31
# asm 2: shufb >carry1=$84,<z3_2427=$78,<z3_2427=$78,<sel01=$33
shufb $84,$78,$78,$33

# qhasm: carry2 = select bytes from z3_2831 by sel01
# asm 1: shufb >carry2=vec128#83,<z3_2831=vec128#77,<z3_2831=vec128#77,<sel01=vec128#31
# asm 2: shufb >carry2=$85,<z3_2831=$79,<z3_2831=$79,<sel01=$33
shufb $85,$79,$79,$33

# qhasm: carry3 = select bytes from z3_3235 by sel01
# asm 1: shufb >carry3=vec128#84,<z3_3235=vec128#78,<z3_3235=vec128#78,<sel01=vec128#31
# asm 2: shufb >carry3=$86,<z3_3235=$80,<z3_3235=$80,<sel01=$33
shufb $86,$80,$80,$33

# qhasm: uint32323232 carry0 >>= 13
# asm 1: rotmi >carry0=vec128#80,<carry0=vec128#80,-13
# asm 2: rotmi >carry0=$82,<carry0=$82,-13
rotmi $82,$82,-13

# qhasm: uint32323232 carry1 >>= 13
# asm 1: rotmi >carry1=vec128#82,<carry1=vec128#82,-13
# asm 2: rotmi >carry1=$84,<carry1=$84,-13
rotmi $84,$84,-13

# qhasm: uint32323232 carry2 >>= 13
# asm 1: rotmi >carry2=vec128#83,<carry2=vec128#83,-13
# asm 2: rotmi >carry2=$85,<carry2=$85,-13
rotmi $85,$85,-13

# qhasm: uint32323232 carry3 >>= 13
# asm 1: rotmi >carry3=vec128#84,<carry3=vec128#84,-13
# asm 2: rotmi >carry3=$86,<carry3=$86,-13
rotmi $86,$86,-13

# qhasm: int32323232 z3_2023 += carry0
# asm 1: a >z3_2023=vec128#73,<z3_2023=vec128#73,<carry0=vec128#80
# asm 2: a >z3_2023=$75,<z3_2023=$75,<carry0=$82
a $75,$75,$82

# qhasm: int32323232 z3_2427 += carry1
# asm 1: a >z3_2427=vec128#76,<z3_2427=vec128#76,<carry1=vec128#82
# asm 2: a >z3_2427=$78,<z3_2427=$78,<carry1=$84
a $78,$78,$84

# qhasm: int32323232 z3_2831 += carry2
# asm 1: a >z3_2831=vec128#77,<z3_2831=vec128#77,<carry2=vec128#83
# asm 2: a >z3_2831=$79,<z3_2831=$79,<carry2=$85
a $79,$79,$85

# qhasm: int32323232 z3_3235 += carry3
# asm 1: a >z3_3235=vec128#78,<z3_3235=vec128#78,<carry3=vec128#84
# asm 2: a >z3_3235=$80,<z3_3235=$80,<carry3=$86
a $80,$80,$86

# qhasm: carry0 = select bytes from z3_2023 by sel12
# asm 1: shufb >carry0=vec128#80,<z3_2023=vec128#73,<z3_2023=vec128#73,<sel12=vec128#32
# asm 2: shufb >carry0=$82,<z3_2023=$75,<z3_2023=$75,<sel12=$34
shufb $82,$75,$75,$34

# qhasm: carry1 = select bytes from z3_2427 by sel12
# asm 1: shufb >carry1=vec128#82,<z3_2427=vec128#76,<z3_2427=vec128#76,<sel12=vec128#32
# asm 2: shufb >carry1=$84,<z3_2427=$78,<z3_2427=$78,<sel12=$34
shufb $84,$78,$78,$34

# qhasm: carry2 = select bytes from z3_2831 by sel12
# asm 1: shufb >carry2=vec128#83,<z3_2831=vec128#77,<z3_2831=vec128#77,<sel12=vec128#32
# asm 2: shufb >carry2=$85,<z3_2831=$79,<z3_2831=$79,<sel12=$34
shufb $85,$79,$79,$34

# qhasm: carry3 = select bytes from z3_3235 by sel12
# asm 1: shufb >carry3=vec128#84,<z3_3235=vec128#78,<z3_3235=vec128#78,<sel12=vec128#32
# asm 2: shufb >carry3=$86,<z3_3235=$80,<z3_3235=$80,<sel12=$34
shufb $86,$80,$80,$34

# qhasm: uint32323232 carry0 >>= 13
# asm 1: rotmi >carry0=vec128#80,<carry0=vec128#80,-13
# asm 2: rotmi >carry0=$82,<carry0=$82,-13
rotmi $82,$82,-13

# qhasm: uint32323232 carry1 >>= 13
# asm 1: rotmi >carry1=vec128#82,<carry1=vec128#82,-13
# asm 2: rotmi >carry1=$84,<carry1=$84,-13
rotmi $84,$84,-13

# qhasm: uint32323232 carry2 >>= 13
# asm 1: rotmi >carry2=vec128#83,<carry2=vec128#83,-13
# asm 2: rotmi >carry2=$85,<carry2=$85,-13
rotmi $85,$85,-13

# qhasm: uint32323232 carry3 >>= 13
# asm 1: rotmi >carry3=vec128#84,<carry3=vec128#84,-13
# asm 2: rotmi >carry3=$86,<carry3=$86,-13
rotmi $86,$86,-13

# qhasm: int32323232 z3_2023 += carry0
# asm 1: a >z3_2023=vec128#73,<z3_2023=vec128#73,<carry0=vec128#80
# asm 2: a >z3_2023=$75,<z3_2023=$75,<carry0=$82
a $75,$75,$82

# qhasm: int32323232 z3_2427 += carry1
# asm 1: a >z3_2427=vec128#76,<z3_2427=vec128#76,<carry1=vec128#82
# asm 2: a >z3_2427=$78,<z3_2427=$78,<carry1=$84
a $78,$78,$84

# qhasm: int32323232 z3_2831 += carry2
# asm 1: a >z3_2831=vec128#77,<z3_2831=vec128#77,<carry2=vec128#83
# asm 2: a >z3_2831=$79,<z3_2831=$79,<carry2=$85
a $79,$79,$85

# qhasm: int32323232 z3_3235 += carry3
# asm 1: a >z3_3235=vec128#78,<z3_3235=vec128#78,<carry3=vec128#84
# asm 2: a >z3_3235=$80,<z3_3235=$80,<carry3=$86
a $80,$80,$86

# qhasm: carry0 = select bytes from z3_2023 by sel23
# asm 1: shufb >carry0=vec128#80,<z3_2023=vec128#73,<z3_2023=vec128#73,<sel23=vec128#33
# asm 2: shufb >carry0=$82,<z3_2023=$75,<z3_2023=$75,<sel23=$35
shufb $82,$75,$75,$35

# qhasm: carry1 = select bytes from z3_2427 by sel23
# asm 1: shufb >carry1=vec128#82,<z3_2427=vec128#76,<z3_2427=vec128#76,<sel23=vec128#33
# asm 2: shufb >carry1=$84,<z3_2427=$78,<z3_2427=$78,<sel23=$35
shufb $84,$78,$78,$35

# qhasm: carry2 = select bytes from z3_2831 by sel23
# asm 1: shufb >carry2=vec128#83,<z3_2831=vec128#77,<z3_2831=vec128#77,<sel23=vec128#33
# asm 2: shufb >carry2=$85,<z3_2831=$79,<z3_2831=$79,<sel23=$35
shufb $85,$79,$79,$35

# qhasm: carry3 = select bytes from z3_3235 by sel23
# asm 1: shufb >carry3=vec128#84,<z3_3235=vec128#78,<z3_3235=vec128#78,<sel23=vec128#33
# asm 2: shufb >carry3=$86,<z3_3235=$80,<z3_3235=$80,<sel23=$35
shufb $86,$80,$80,$35

# qhasm: uint32323232 carry0 >>= 13
# asm 1: rotmi >carry0=vec128#80,<carry0=vec128#80,-13
# asm 2: rotmi >carry0=$82,<carry0=$82,-13
rotmi $82,$82,-13

# qhasm: uint32323232 carry1 >>= 13
# asm 1: rotmi >carry1=vec128#82,<carry1=vec128#82,-13
# asm 2: rotmi >carry1=$84,<carry1=$84,-13
rotmi $84,$84,-13

# qhasm: uint32323232 carry2 >>= 13
# asm 1: rotmi >carry2=vec128#83,<carry2=vec128#83,-13
# asm 2: rotmi >carry2=$85,<carry2=$85,-13
rotmi $85,$85,-13

# qhasm: uint32323232 carry3 >>= 13
# asm 1: rotmi >carry3=vec128#84,<carry3=vec128#84,-13
# asm 2: rotmi >carry3=$86,<carry3=$86,-13
rotmi $86,$86,-13

# qhasm: int32323232 z3_2023 += carry0
# asm 1: a >z3_2023=vec128#73,<z3_2023=vec128#73,<carry0=vec128#80
# asm 2: a >z3_2023=$75,<z3_2023=$75,<carry0=$82
a $75,$75,$82

# qhasm: int32323232 z3_2427 += carry1
# asm 1: a >z3_2427=vec128#76,<z3_2427=vec128#76,<carry1=vec128#82
# asm 2: a >z3_2427=$78,<z3_2427=$78,<carry1=$84
a $78,$78,$84

# qhasm: int32323232 z3_2831 += carry2
# asm 1: a >z3_2831=vec128#77,<z3_2831=vec128#77,<carry2=vec128#83
# asm 2: a >z3_2831=$79,<z3_2831=$79,<carry2=$85
a $79,$79,$85

# qhasm: int32323232 z3_3235 += carry3
# asm 1: a >z3_3235=vec128#78,<z3_3235=vec128#78,<carry3=vec128#84
# asm 2: a >z3_3235=$80,<z3_3235=$80,<carry3=$86
a $80,$80,$86

# qhasm: carry0 = select bytes from z3_2023 by sel30
# asm 1: shufb >carry0=vec128#80,<z3_2023=vec128#73,<z3_2023=vec128#73,<sel30=vec128#34
# asm 2: shufb >carry0=$82,<z3_2023=$75,<z3_2023=$75,<sel30=$36
shufb $82,$75,$75,$36

# qhasm: carry1 = select bytes from z3_2427 by sel30
# asm 1: shufb >carry1=vec128#82,<z3_2427=vec128#76,<z3_2427=vec128#76,<sel30=vec128#34
# asm 2: shufb >carry1=$84,<z3_2427=$78,<z3_2427=$78,<sel30=$36
shufb $84,$78,$78,$36

# qhasm: carry2 = select bytes from z3_2831 by sel30
# asm 1: shufb >carry2=vec128#83,<z3_2831=vec128#77,<z3_2831=vec128#77,<sel30=vec128#34
# asm 2: shufb >carry2=$85,<z3_2831=$79,<z3_2831=$79,<sel30=$36
shufb $85,$79,$79,$36

# qhasm: carry3 = select bytes from z3_3235 by sel30
# asm 1: shufb >carry3=vec128#84,<z3_3235=vec128#78,<z3_3235=vec128#78,<sel30=vec128#34
# asm 2: shufb >carry3=$86,<z3_3235=$80,<z3_3235=$80,<sel30=$36
shufb $86,$80,$80,$36

# qhasm: uint32323232 carry0 >>= 12
# asm 1: rotmi >carry0=vec128#80,<carry0=vec128#80,-12
# asm 2: rotmi >carry0=$82,<carry0=$82,-12
rotmi $82,$82,-12

# qhasm: uint32323232 carry1 >>= 12
# asm 1: rotmi >carry1=vec128#82,<carry1=vec128#82,-12
# asm 2: rotmi >carry1=$84,<carry1=$84,-12
rotmi $84,$84,-12

# qhasm: uint32323232 carry2 >>= 12
# asm 1: rotmi >carry2=vec128#83,<carry2=vec128#83,-12
# asm 2: rotmi >carry2=$85,<carry2=$85,-12
rotmi $85,$85,-12

# qhasm: uint32323232 carry3 >>= 12
# asm 1: rotmi >carry3=vec128#84,<carry3=vec128#84,-12
# asm 2: rotmi >carry3=$86,<carry3=$86,-12
rotmi $86,$86,-12

# qhasm: z3_2023 &= redcoeffmask
# asm 1: and >z3_2023=vec128#73,<z3_2023=vec128#73,<redcoeffmask=vec128#37
# asm 2: and >z3_2023=$75,<z3_2023=$75,<redcoeffmask=$39
and $75,$75,$39

# qhasm: z3_2427 &= redcoeffmask
# asm 1: and >z3_2427=vec128#76,<z3_2427=vec128#76,<redcoeffmask=vec128#37
# asm 2: and >z3_2427=$78,<z3_2427=$78,<redcoeffmask=$39
and $78,$78,$39

# qhasm: z3_2831 &= redcoeffmask
# asm 1: and >z3_2831=vec128#77,<z3_2831=vec128#77,<redcoeffmask=vec128#37
# asm 2: and >z3_2831=$79,<z3_2831=$79,<redcoeffmask=$39
and $79,$79,$39

# qhasm: z3_3235 &= redcoeffmask
# asm 1: and >z3_3235=vec128#78,<z3_3235=vec128#78,<redcoeffmask=vec128#37
# asm 2: and >z3_3235=$80,<z3_3235=$80,<redcoeffmask=$39
and $80,$80,$39

# qhasm: int32323232 z3_2427 += carry0
# asm 1: a >z3_2427=vec128#76,<z3_2427=vec128#76,<carry0=vec128#80
# asm 2: a >z3_2427=$78,<z3_2427=$78,<carry0=$82
a $78,$78,$82

# qhasm: int32323232 z3_2831 += carry1
# asm 1: a >z3_2831=vec128#77,<z3_2831=vec128#77,<carry1=vec128#82
# asm 2: a >z3_2831=$79,<z3_2831=$79,<carry1=$84
a $79,$79,$84

# qhasm: int32323232 z3_3235 += carry2
# asm 1: a >z3_3235=vec128#78,<z3_3235=vec128#78,<carry2=vec128#83
# asm 2: a >z3_3235=$80,<z3_3235=$80,<carry2=$85
a $80,$80,$85

# qhasm: int32323232 z3_3639 += carry3
# asm 1: a >z3_3639=vec128#79,<z3_3639=vec128#79,<carry3=vec128#84
# asm 2: a >z3_3639=$81,<z3_3639=$81,<carry3=$86
a $81,$81,$86

# qhasm: carry1 = select bytes from z3_2427 by sel01
# asm 1: shufb >carry1=vec128#80,<z3_2427=vec128#76,<z3_2427=vec128#76,<sel01=vec128#31
# asm 2: shufb >carry1=$82,<z3_2427=$78,<z3_2427=$78,<sel01=$33
shufb $82,$78,$78,$33

# qhasm: carry2 = select bytes from z3_2831 by sel01
# asm 1: shufb >carry2=vec128#82,<z3_2831=vec128#77,<z3_2831=vec128#77,<sel01=vec128#31
# asm 2: shufb >carry2=$84,<z3_2831=$79,<z3_2831=$79,<sel01=$33
shufb $84,$79,$79,$33

# qhasm: carry3 = select bytes from z3_3235 by sel01
# asm 1: shufb >carry3=vec128#83,<z3_3235=vec128#78,<z3_3235=vec128#78,<sel01=vec128#31
# asm 2: shufb >carry3=$85,<z3_3235=$80,<z3_3235=$80,<sel01=$33
shufb $85,$80,$80,$33

# qhasm: carry4 = select bytes from z3_3639 by sel01
# asm 1: shufb >carry4=vec128#84,<z3_3639=vec128#79,<z3_3639=vec128#79,<sel01=vec128#31
# asm 2: shufb >carry4=$86,<z3_3639=$81,<z3_3639=$81,<sel01=$33
shufb $86,$81,$81,$33

# qhasm: uint32323232 carry1 >>= 13
# asm 1: rotmi >carry1=vec128#80,<carry1=vec128#80,-13
# asm 2: rotmi >carry1=$82,<carry1=$82,-13
rotmi $82,$82,-13

# qhasm: uint32323232 carry2 >>= 13
# asm 1: rotmi >carry2=vec128#82,<carry2=vec128#82,-13
# asm 2: rotmi >carry2=$84,<carry2=$84,-13
rotmi $84,$84,-13

# qhasm: uint32323232 carry3 >>= 13
# asm 1: rotmi >carry3=vec128#83,<carry3=vec128#83,-13
# asm 2: rotmi >carry3=$85,<carry3=$85,-13
rotmi $85,$85,-13

# qhasm: uint32323232 carry4 >>= 13
# asm 1: rotmi >carry4=vec128#84,<carry4=vec128#84,-13
# asm 2: rotmi >carry4=$86,<carry4=$86,-13
rotmi $86,$86,-13

# qhasm: int32323232 z3_2427 += carry1
# asm 1: a >z3_2427=vec128#76,<z3_2427=vec128#76,<carry1=vec128#80
# asm 2: a >z3_2427=$78,<z3_2427=$78,<carry1=$82
a $78,$78,$82

# qhasm: int32323232 z3_2831 += carry2
# asm 1: a >z3_2831=vec128#77,<z3_2831=vec128#77,<carry2=vec128#82
# asm 2: a >z3_2831=$79,<z3_2831=$79,<carry2=$84
a $79,$79,$84

# qhasm: int32323232 z3_3235 += carry3
# asm 1: a >z3_3235=vec128#78,<z3_3235=vec128#78,<carry3=vec128#83
# asm 2: a >z3_3235=$80,<z3_3235=$80,<carry3=$85
a $80,$80,$85

# qhasm: int32323232 z3_3639 += carry4
# asm 1: a >z3_3639=vec128#79,<z3_3639=vec128#79,<carry4=vec128#84
# asm 2: a >z3_3639=$81,<z3_3639=$81,<carry4=$86
a $81,$81,$86

# qhasm: carry1 = select bytes from z3_2427 by sel12
# asm 1: shufb >carry1=vec128#80,<z3_2427=vec128#76,<z3_2427=vec128#76,<sel12=vec128#32
# asm 2: shufb >carry1=$82,<z3_2427=$78,<z3_2427=$78,<sel12=$34
shufb $82,$78,$78,$34

# qhasm: carry2 = select bytes from z3_2831 by sel12
# asm 1: shufb >carry2=vec128#82,<z3_2831=vec128#77,<z3_2831=vec128#77,<sel12=vec128#32
# asm 2: shufb >carry2=$84,<z3_2831=$79,<z3_2831=$79,<sel12=$34
shufb $84,$79,$79,$34

# qhasm: carry3 = select bytes from z3_3235 by sel12
# asm 1: shufb >carry3=vec128#83,<z3_3235=vec128#78,<z3_3235=vec128#78,<sel12=vec128#32
# asm 2: shufb >carry3=$85,<z3_3235=$80,<z3_3235=$80,<sel12=$34
shufb $85,$80,$80,$34

# qhasm: carry4 = select bytes from z3_3639 by sel12
# asm 1: shufb >carry4=vec128#84,<z3_3639=vec128#79,<z3_3639=vec128#79,<sel12=vec128#32
# asm 2: shufb >carry4=$86,<z3_3639=$81,<z3_3639=$81,<sel12=$34
shufb $86,$81,$81,$34

# qhasm: uint32323232 carry1 >>= 13
# asm 1: rotmi >carry1=vec128#80,<carry1=vec128#80,-13
# asm 2: rotmi >carry1=$82,<carry1=$82,-13
rotmi $82,$82,-13

# qhasm: uint32323232 carry2 >>= 13
# asm 1: rotmi >carry2=vec128#82,<carry2=vec128#82,-13
# asm 2: rotmi >carry2=$84,<carry2=$84,-13
rotmi $84,$84,-13

# qhasm: uint32323232 carry3 >>= 13
# asm 1: rotmi >carry3=vec128#83,<carry3=vec128#83,-13
# asm 2: rotmi >carry3=$85,<carry3=$85,-13
rotmi $85,$85,-13

# qhasm: uint32323232 carry4 >>= 13
# asm 1: rotmi >carry4=vec128#84,<carry4=vec128#84,-13
# asm 2: rotmi >carry4=$86,<carry4=$86,-13
rotmi $86,$86,-13

# qhasm: int32323232 z3_2427 += carry1
# asm 1: a >z3_2427=vec128#76,<z3_2427=vec128#76,<carry1=vec128#80
# asm 2: a >z3_2427=$78,<z3_2427=$78,<carry1=$82
a $78,$78,$82

# qhasm: int32323232 z3_2831 += carry2
# asm 1: a >z3_2831=vec128#77,<z3_2831=vec128#77,<carry2=vec128#82
# asm 2: a >z3_2831=$79,<z3_2831=$79,<carry2=$84
a $79,$79,$84

# qhasm: int32323232 z3_3235 += carry3
# asm 1: a >z3_3235=vec128#78,<z3_3235=vec128#78,<carry3=vec128#83
# asm 2: a >z3_3235=$80,<z3_3235=$80,<carry3=$85
a $80,$80,$85

# qhasm: int32323232 z3_3639 += carry4
# asm 1: a >z3_3639=vec128#79,<z3_3639=vec128#79,<carry4=vec128#84
# asm 2: a >z3_3639=$81,<z3_3639=$81,<carry4=$86
a $81,$81,$86

# qhasm: carry1 = select bytes from z3_2427 by sel23
# asm 1: shufb >carry1=vec128#80,<z3_2427=vec128#76,<z3_2427=vec128#76,<sel23=vec128#33
# asm 2: shufb >carry1=$82,<z3_2427=$78,<z3_2427=$78,<sel23=$35
shufb $82,$78,$78,$35

# qhasm: carry2 = select bytes from z3_2831 by sel23
# asm 1: shufb >carry2=vec128#82,<z3_2831=vec128#77,<z3_2831=vec128#77,<sel23=vec128#33
# asm 2: shufb >carry2=$84,<z3_2831=$79,<z3_2831=$79,<sel23=$35
shufb $84,$79,$79,$35

# qhasm: carry3 = select bytes from z3_3235 by sel23
# asm 1: shufb >carry3=vec128#83,<z3_3235=vec128#78,<z3_3235=vec128#78,<sel23=vec128#33
# asm 2: shufb >carry3=$85,<z3_3235=$80,<z3_3235=$80,<sel23=$35
shufb $85,$80,$80,$35

# qhasm: carry4 = select bytes from z3_3639 by sel23
# asm 1: shufb >carry4=vec128#84,<z3_3639=vec128#79,<z3_3639=vec128#79,<sel23=vec128#33
# asm 2: shufb >carry4=$86,<z3_3639=$81,<z3_3639=$81,<sel23=$35
shufb $86,$81,$81,$35

# qhasm: uint32323232 carry1 >>= 13
# asm 1: rotmi >carry1=vec128#80,<carry1=vec128#80,-13
# asm 2: rotmi >carry1=$82,<carry1=$82,-13
rotmi $82,$82,-13

# qhasm: uint32323232 carry2 >>= 13
# asm 1: rotmi >carry2=vec128#82,<carry2=vec128#82,-13
# asm 2: rotmi >carry2=$84,<carry2=$84,-13
rotmi $84,$84,-13

# qhasm: uint32323232 carry3 >>= 13
# asm 1: rotmi >carry3=vec128#83,<carry3=vec128#83,-13
# asm 2: rotmi >carry3=$85,<carry3=$85,-13
rotmi $85,$85,-13

# qhasm: uint32323232 carry4 >>= 13
# asm 1: rotmi >carry4=vec128#84,<carry4=vec128#84,-13
# asm 2: rotmi >carry4=$86,<carry4=$86,-13
rotmi $86,$86,-13

# qhasm: int32323232 z3_2427 += carry1
# asm 1: a >z3_2427=vec128#76,<z3_2427=vec128#76,<carry1=vec128#80
# asm 2: a >z3_2427=$78,<z3_2427=$78,<carry1=$82
a $78,$78,$82

# qhasm: int32323232 z3_2831 += carry2
# asm 1: a >z3_2831=vec128#77,<z3_2831=vec128#77,<carry2=vec128#82
# asm 2: a >z3_2831=$79,<z3_2831=$79,<carry2=$84
a $79,$79,$84

# qhasm: int32323232 z3_3235 += carry3
# asm 1: a >z3_3235=vec128#78,<z3_3235=vec128#78,<carry3=vec128#83
# asm 2: a >z3_3235=$80,<z3_3235=$80,<carry3=$85
a $80,$80,$85

# qhasm: int32323232 z3_3639 += carry4
# asm 1: a >z3_3639=vec128#79,<z3_3639=vec128#79,<carry4=vec128#84
# asm 2: a >z3_3639=$81,<z3_3639=$81,<carry4=$86
a $81,$81,$86

# qhasm: carry1 = select bytes from z3_2427 by sel30
# asm 1: shufb >carry1=vec128#80,<z3_2427=vec128#76,<z3_2427=vec128#76,<sel30=vec128#34
# asm 2: shufb >carry1=$82,<z3_2427=$78,<z3_2427=$78,<sel30=$36
shufb $82,$78,$78,$36

# qhasm: carry2 = select bytes from z3_2831 by sel30
# asm 1: shufb >carry2=vec128#82,<z3_2831=vec128#77,<z3_2831=vec128#77,<sel30=vec128#34
# asm 2: shufb >carry2=$84,<z3_2831=$79,<z3_2831=$79,<sel30=$36
shufb $84,$79,$79,$36

# qhasm: carry3 = select bytes from z3_3235 by sel30
# asm 1: shufb >carry3=vec128#83,<z3_3235=vec128#78,<z3_3235=vec128#78,<sel30=vec128#34
# asm 2: shufb >carry3=$85,<z3_3235=$80,<z3_3235=$80,<sel30=$36
shufb $85,$80,$80,$36

# qhasm: uint32323232 carry1 >>= 12
# asm 1: rotmi >carry1=vec128#80,<carry1=vec128#80,-12
# asm 2: rotmi >carry1=$82,<carry1=$82,-12
rotmi $82,$82,-12

# qhasm: uint32323232 carry2 >>= 12
# asm 1: rotmi >carry2=vec128#82,<carry2=vec128#82,-12
# asm 2: rotmi >carry2=$84,<carry2=$84,-12
rotmi $84,$84,-12

# qhasm: uint32323232 carry3 >>= 12
# asm 1: rotmi >carry3=vec128#83,<carry3=vec128#83,-12
# asm 2: rotmi >carry3=$85,<carry3=$85,-12
rotmi $85,$85,-12

# qhasm: z3_2427 &= redcoeffmask
# asm 1: and >z3_2427=vec128#76,<z3_2427=vec128#76,<redcoeffmask=vec128#37
# asm 2: and >z3_2427=$78,<z3_2427=$78,<redcoeffmask=$39
and $78,$78,$39

# qhasm: z3_2831 &= redcoeffmask
# asm 1: and >z3_2831=vec128#77,<z3_2831=vec128#77,<redcoeffmask=vec128#37
# asm 2: and >z3_2831=$79,<z3_2831=$79,<redcoeffmask=$39
and $79,$79,$39

# qhasm: z3_3235 &= redcoeffmask
# asm 1: and >z3_3235=vec128#78,<z3_3235=vec128#78,<redcoeffmask=vec128#37
# asm 2: and >z3_3235=$80,<z3_3235=$80,<redcoeffmask=$39
and $80,$80,$39

# qhasm: z3_3639 &= redcoeffmaskend
# asm 1: and >z3_3639=vec128#79,<z3_3639=vec128#79,<redcoeffmaskend=vec128#38
# asm 2: and >z3_3639=$81,<z3_3639=$81,<redcoeffmaskend=$40
and $81,$81,$40

# qhasm: int32323232 z3_2831 += carry1
# asm 1: a >z3_2831=vec128#77,<z3_2831=vec128#77,<carry1=vec128#80
# asm 2: a >z3_2831=$79,<z3_2831=$79,<carry1=$82
a $79,$79,$82

# qhasm: int32323232 z3_3235 += carry2
# asm 1: a >z3_3235=vec128#78,<z3_3235=vec128#78,<carry2=vec128#82
# asm 2: a >z3_3235=$80,<z3_3235=$80,<carry2=$84
a $80,$80,$84

# qhasm: int32323232 z3_3639 += carry3
# asm 1: a >z3_3639=vec128#79,<z3_3639=vec128#79,<carry3=vec128#83
# asm 2: a >z3_3639=$81,<z3_3639=$81,<carry3=$85
a $81,$81,$85

# qhasm: uint32323232 red0 = (z3_2023 & 0xffff) * 19
# asm 1: mpyui >red0=vec128#73,<z3_2023=vec128#73,19
# asm 2: mpyui >red0=$75,<z3_2023=$75,19
mpyui $75,$75,19

# qhasm: uint32323232 red4 = (z3_3639 & 0xffff) * 19
# asm 1: mpyui >red4=vec128#79,<z3_3639=vec128#79,19
# asm 2: mpyui >red4=$81,<z3_3639=$81,19
mpyui $81,$81,19

# qhasm: uint32323232 red1 = (z3_2427 & 0xffff) * 19
# asm 1: mpyui >red1=vec128#76,<z3_2427=vec128#76,19
# asm 2: mpyui >red1=$78,<z3_2427=$78,19
mpyui $78,$78,19

# qhasm: uint32323232 red2 = (z3_2831 & 0xffff) * 19
# asm 1: mpyui >red2=vec128#77,<z3_2831=vec128#77,19
# asm 2: mpyui >red2=$79,<z3_2831=$79,19
mpyui $79,$79,19

# qhasm: uint32323232 red3 = (z3_3235 & 0xffff) * 19
# asm 1: mpyui >red3=vec128#78,<z3_3235=vec128#78,19
# asm 2: mpyui >red3=$80,<z3_3235=$80,19
mpyui $80,$80,19

# qhasm: int32323232 z3_03 += red0
# asm 1: a >z3_03=vec128#73,<z3_03=vec128#81,<red0=vec128#73
# asm 2: a >z3_03=$75,<z3_03=$83,<red0=$75
a $75,$83,$75

# qhasm: int32323232 z3_1619 += red4
# asm 1: a >z3_1619=vec128#71,<z3_1619=vec128#71,<red4=vec128#79
# asm 2: a >z3_1619=$73,<z3_1619=$73,<red4=$81
a $73,$73,$81

# qhasm: int32323232 z3_47 += red1
# asm 1: a >z3_47=vec128#72,<z3_47=vec128#72,<red1=vec128#76
# asm 2: a >z3_47=$74,<z3_47=$74,<red1=$78
a $74,$74,$78

# qhasm: int32323232 z3_811 += red2
# asm 1: a >z3_811=vec128#5,<z3_811=vec128#5,<red2=vec128#77
# asm 2: a >z3_811=$7,<z3_811=$7,<red2=$79
a $7,$7,$79

# qhasm: int32323232 z3_1215 += red3
# asm 1: a >z3_1215=vec128#70,<z3_1215=vec128#70,<red3=vec128#78
# asm 2: a >z3_1215=$72,<z3_1215=$72,<red3=$80
a $72,$72,$80

# qhasm: carry = select bytes from z3_1619 by sel01
# asm 1: shufb >carry=vec128#76,<z3_1619=vec128#71,<z3_1619=vec128#71,<sel01=vec128#31
# asm 2: shufb >carry=$78,<z3_1619=$73,<z3_1619=$73,<sel01=$33
shufb $78,$73,$73,$33

# qhasm: uint32323232 carry >>= 13
# asm 1: rotmi >carry=vec128#76,<carry=vec128#76,-13
# asm 2: rotmi >carry=$78,<carry=$78,-13
rotmi $78,$78,-13

# qhasm: int32323232 z3_1619 += carry
# asm 1: a >z3_1619=vec128#71,<z3_1619=vec128#71,<carry=vec128#76
# asm 2: a >z3_1619=$73,<z3_1619=$73,<carry=$78
a $73,$73,$78

# qhasm: carry = select bytes from z3_1619 by sel12
# asm 1: shufb >carry=vec128#76,<z3_1619=vec128#71,<z3_1619=vec128#71,<sel12=vec128#32
# asm 2: shufb >carry=$78,<z3_1619=$73,<z3_1619=$73,<sel12=$34
shufb $78,$73,$73,$34

# qhasm: uint32323232 carry >>= 13
# asm 1: rotmi >carry=vec128#76,<carry=vec128#76,-13
# asm 2: rotmi >carry=$78,<carry=$78,-13
rotmi $78,$78,-13

# qhasm: int32323232 z3_1619 += carry
# asm 1: a >z3_1619=vec128#71,<z3_1619=vec128#71,<carry=vec128#76
# asm 2: a >z3_1619=$73,<z3_1619=$73,<carry=$78
a $73,$73,$78

# qhasm: carry = select bytes from z3_1619 by sel23
# asm 1: shufb >carry=vec128#76,<z3_1619=vec128#71,<z3_1619=vec128#71,<sel23=vec128#33
# asm 2: shufb >carry=$78,<z3_1619=$73,<z3_1619=$73,<sel23=$35
shufb $78,$73,$73,$35

# qhasm: uint32323232 carry >>= 13
# asm 1: rotmi >carry=vec128#76,<carry=vec128#76,-13
# asm 2: rotmi >carry=$78,<carry=$78,-13
rotmi $78,$78,-13

# qhasm: int32323232 z3_1619 += carry
# asm 1: a >z3_1619=vec128#71,<z3_1619=vec128#71,<carry=vec128#76
# asm 2: a >z3_1619=$73,<z3_1619=$73,<carry=$78
a $73,$73,$78

# qhasm: carry = select bytes from z3_1619 by sel30
# asm 1: shufb >carry=vec128#76,<z3_1619=vec128#71,<z3_1619=vec128#71,<sel30=vec128#34
# asm 2: shufb >carry=$78,<z3_1619=$73,<z3_1619=$73,<sel30=$36
shufb $78,$73,$73,$36

# qhasm: uint32323232 carry >>= 12
# asm 1: rotmi >carry=vec128#76,<carry=vec128#76,-12
# asm 2: rotmi >carry=$78,<carry=$78,-12
rotmi $78,$78,-12

# qhasm: int32323232 red = carry << 4
# asm 1: shli >red=vec128#77,<carry=vec128#76,4
# asm 2: shli >red=$79,<carry=$78,4
shli $79,$78,4

# qhasm: int32323232 red = red + carry 
# asm 1: a >red=vec128#77,<red=vec128#77,<carry=vec128#76
# asm 2: a >red=$79,<red=$79,<carry=$78
a $79,$79,$78

# qhasm: int32323232 red = red + carry 
# asm 1: a >red=vec128#77,<red=vec128#77,<carry=vec128#76
# asm 2: a >red=$79,<red=$79,<carry=$78
a $79,$79,$78

# qhasm: int32323232 red = red + carry 
# asm 1: a >red=vec128#76,<red=vec128#77,<carry=vec128#76
# asm 2: a >red=$78,<red=$79,<carry=$78
a $78,$79,$78

# qhasm: int32323232 z3_03 += red
# asm 1: a >z3_03=vec128#73,<z3_03=vec128#73,<red=vec128#76
# asm 2: a >z3_03=$75,<z3_03=$75,<red=$78
a $75,$75,$78

# qhasm: z3_1619 &= redcoeffmask
# asm 1: and >z3_1619=vec128#71,<z3_1619=vec128#71,<redcoeffmask=vec128#37
# asm 2: and >z3_1619=$73,<z3_1619=$73,<redcoeffmask=$39
and $73,$73,$39

# qhasm: carry0 = select bytes from z3_03 by sel01
# asm 1: shufb >carry0=vec128#76,<z3_03=vec128#73,<z3_03=vec128#73,<sel01=vec128#31
# asm 2: shufb >carry0=$78,<z3_03=$75,<z3_03=$75,<sel01=$33
shufb $78,$75,$75,$33

# qhasm: carry1 = select bytes from z3_47 by sel01
# asm 1: shufb >carry1=vec128#77,<z3_47=vec128#72,<z3_47=vec128#72,<sel01=vec128#31
# asm 2: shufb >carry1=$79,<z3_47=$74,<z3_47=$74,<sel01=$33
shufb $79,$74,$74,$33

# qhasm: carry2 = select bytes from z3_811 by sel01
# asm 1: shufb >carry2=vec128#78,<z3_811=vec128#5,<z3_811=vec128#5,<sel01=vec128#31
# asm 2: shufb >carry2=$80,<z3_811=$7,<z3_811=$7,<sel01=$33
shufb $80,$7,$7,$33

# qhasm: carry3 = select bytes from z3_1215 by sel01
# asm 1: shufb >carry3=vec128#79,<z3_1215=vec128#70,<z3_1215=vec128#70,<sel01=vec128#31
# asm 2: shufb >carry3=$81,<z3_1215=$72,<z3_1215=$72,<sel01=$33
shufb $81,$72,$72,$33

# qhasm: uint32323232 carry0 >>= 13
# asm 1: rotmi >carry0=vec128#76,<carry0=vec128#76,-13
# asm 2: rotmi >carry0=$78,<carry0=$78,-13
rotmi $78,$78,-13

# qhasm: uint32323232 carry1 >>= 13
# asm 1: rotmi >carry1=vec128#77,<carry1=vec128#77,-13
# asm 2: rotmi >carry1=$79,<carry1=$79,-13
rotmi $79,$79,-13

# qhasm: uint32323232 carry2 >>= 13
# asm 1: rotmi >carry2=vec128#78,<carry2=vec128#78,-13
# asm 2: rotmi >carry2=$80,<carry2=$80,-13
rotmi $80,$80,-13

# qhasm: uint32323232 carry3 >>= 13
# asm 1: rotmi >carry3=vec128#79,<carry3=vec128#79,-13
# asm 2: rotmi >carry3=$81,<carry3=$81,-13
rotmi $81,$81,-13

# qhasm: int32323232 z3_03 += carry0
# asm 1: a >z3_03=vec128#73,<z3_03=vec128#73,<carry0=vec128#76
# asm 2: a >z3_03=$75,<z3_03=$75,<carry0=$78
a $75,$75,$78

# qhasm: int32323232 z3_47 += carry1
# asm 1: a >z3_47=vec128#72,<z3_47=vec128#72,<carry1=vec128#77
# asm 2: a >z3_47=$74,<z3_47=$74,<carry1=$79
a $74,$74,$79

# qhasm: int32323232 z3_811 += carry2
# asm 1: a >z3_811=vec128#5,<z3_811=vec128#5,<carry2=vec128#78
# asm 2: a >z3_811=$7,<z3_811=$7,<carry2=$80
a $7,$7,$80

# qhasm: int32323232 z3_1215 += carry3
# asm 1: a >z3_1215=vec128#70,<z3_1215=vec128#70,<carry3=vec128#79
# asm 2: a >z3_1215=$72,<z3_1215=$72,<carry3=$81
a $72,$72,$81

# qhasm: carry0 = select bytes from z3_03 by sel12
# asm 1: shufb >carry0=vec128#76,<z3_03=vec128#73,<z3_03=vec128#73,<sel12=vec128#32
# asm 2: shufb >carry0=$78,<z3_03=$75,<z3_03=$75,<sel12=$34
shufb $78,$75,$75,$34

# qhasm: carry1 = select bytes from z3_47 by sel12
# asm 1: shufb >carry1=vec128#77,<z3_47=vec128#72,<z3_47=vec128#72,<sel12=vec128#32
# asm 2: shufb >carry1=$79,<z3_47=$74,<z3_47=$74,<sel12=$34
shufb $79,$74,$74,$34

# qhasm: carry2 = select bytes from z3_811 by sel12
# asm 1: shufb >carry2=vec128#78,<z3_811=vec128#5,<z3_811=vec128#5,<sel12=vec128#32
# asm 2: shufb >carry2=$80,<z3_811=$7,<z3_811=$7,<sel12=$34
shufb $80,$7,$7,$34

# qhasm: carry3 = select bytes from z3_1215 by sel12
# asm 1: shufb >carry3=vec128#79,<z3_1215=vec128#70,<z3_1215=vec128#70,<sel12=vec128#32
# asm 2: shufb >carry3=$81,<z3_1215=$72,<z3_1215=$72,<sel12=$34
shufb $81,$72,$72,$34

# qhasm: uint32323232 carry0 >>= 13
# asm 1: rotmi >carry0=vec128#76,<carry0=vec128#76,-13
# asm 2: rotmi >carry0=$78,<carry0=$78,-13
rotmi $78,$78,-13

# qhasm: uint32323232 carry1 >>= 13
# asm 1: rotmi >carry1=vec128#77,<carry1=vec128#77,-13
# asm 2: rotmi >carry1=$79,<carry1=$79,-13
rotmi $79,$79,-13

# qhasm: uint32323232 carry2 >>= 13
# asm 1: rotmi >carry2=vec128#78,<carry2=vec128#78,-13
# asm 2: rotmi >carry2=$80,<carry2=$80,-13
rotmi $80,$80,-13

# qhasm: uint32323232 carry3 >>= 13
# asm 1: rotmi >carry3=vec128#79,<carry3=vec128#79,-13
# asm 2: rotmi >carry3=$81,<carry3=$81,-13
rotmi $81,$81,-13

# qhasm: int32323232 z3_03 += carry0
# asm 1: a >z3_03=vec128#73,<z3_03=vec128#73,<carry0=vec128#76
# asm 2: a >z3_03=$75,<z3_03=$75,<carry0=$78
a $75,$75,$78

# qhasm: int32323232 z3_47 += carry1
# asm 1: a >z3_47=vec128#72,<z3_47=vec128#72,<carry1=vec128#77
# asm 2: a >z3_47=$74,<z3_47=$74,<carry1=$79
a $74,$74,$79

# qhasm: int32323232 z3_811 += carry2
# asm 1: a >z3_811=vec128#5,<z3_811=vec128#5,<carry2=vec128#78
# asm 2: a >z3_811=$7,<z3_811=$7,<carry2=$80
a $7,$7,$80

# qhasm: int32323232 z3_1215 += carry3
# asm 1: a >z3_1215=vec128#70,<z3_1215=vec128#70,<carry3=vec128#79
# asm 2: a >z3_1215=$72,<z3_1215=$72,<carry3=$81
a $72,$72,$81

# qhasm: carry0 = select bytes from z3_03 by sel23
# asm 1: shufb >carry0=vec128#76,<z3_03=vec128#73,<z3_03=vec128#73,<sel23=vec128#33
# asm 2: shufb >carry0=$78,<z3_03=$75,<z3_03=$75,<sel23=$35
shufb $78,$75,$75,$35

# qhasm: carry1 = select bytes from z3_47 by sel23
# asm 1: shufb >carry1=vec128#77,<z3_47=vec128#72,<z3_47=vec128#72,<sel23=vec128#33
# asm 2: shufb >carry1=$79,<z3_47=$74,<z3_47=$74,<sel23=$35
shufb $79,$74,$74,$35

# qhasm: carry2 = select bytes from z3_811 by sel23
# asm 1: shufb >carry2=vec128#78,<z3_811=vec128#5,<z3_811=vec128#5,<sel23=vec128#33
# asm 2: shufb >carry2=$80,<z3_811=$7,<z3_811=$7,<sel23=$35
shufb $80,$7,$7,$35

# qhasm: carry3 = select bytes from z3_1215 by sel23
# asm 1: shufb >carry3=vec128#79,<z3_1215=vec128#70,<z3_1215=vec128#70,<sel23=vec128#33
# asm 2: shufb >carry3=$81,<z3_1215=$72,<z3_1215=$72,<sel23=$35
shufb $81,$72,$72,$35

# qhasm: uint32323232 carry0 >>= 13
# asm 1: rotmi >carry0=vec128#76,<carry0=vec128#76,-13
# asm 2: rotmi >carry0=$78,<carry0=$78,-13
rotmi $78,$78,-13

# qhasm: uint32323232 carry1 >>= 13
# asm 1: rotmi >carry1=vec128#77,<carry1=vec128#77,-13
# asm 2: rotmi >carry1=$79,<carry1=$79,-13
rotmi $79,$79,-13

# qhasm: uint32323232 carry2 >>= 13
# asm 1: rotmi >carry2=vec128#78,<carry2=vec128#78,-13
# asm 2: rotmi >carry2=$80,<carry2=$80,-13
rotmi $80,$80,-13

# qhasm: uint32323232 carry3 >>= 13
# asm 1: rotmi >carry3=vec128#79,<carry3=vec128#79,-13
# asm 2: rotmi >carry3=$81,<carry3=$81,-13
rotmi $81,$81,-13

# qhasm: int32323232 z3_03 += carry0
# asm 1: a >z3_03=vec128#73,<z3_03=vec128#73,<carry0=vec128#76
# asm 2: a >z3_03=$75,<z3_03=$75,<carry0=$78
a $75,$75,$78

# qhasm: int32323232 z3_47 += carry1
# asm 1: a >z3_47=vec128#72,<z3_47=vec128#72,<carry1=vec128#77
# asm 2: a >z3_47=$74,<z3_47=$74,<carry1=$79
a $74,$74,$79

# qhasm: int32323232 z3_811 += carry2
# asm 1: a >z3_811=vec128#76,<z3_811=vec128#5,<carry2=vec128#78
# asm 2: a >z3_811=$78,<z3_811=$7,<carry2=$80
a $78,$7,$80

# qhasm: int32323232 z3_1215 += carry3
# asm 1: a >z3_1215=vec128#70,<z3_1215=vec128#70,<carry3=vec128#79
# asm 2: a >z3_1215=$72,<z3_1215=$72,<carry3=$81
a $72,$72,$81

# qhasm: carry0 = select bytes from z3_03 by sel30
# asm 1: shufb >carry0=vec128#5,<z3_03=vec128#73,<z3_03=vec128#73,<sel30=vec128#34
# asm 2: shufb >carry0=$7,<z3_03=$75,<z3_03=$75,<sel30=$36
shufb $7,$75,$75,$36

# qhasm: carry1 = select bytes from z3_47 by sel30
# asm 1: shufb >carry1=vec128#77,<z3_47=vec128#72,<z3_47=vec128#72,<sel30=vec128#34
# asm 2: shufb >carry1=$79,<z3_47=$74,<z3_47=$74,<sel30=$36
shufb $79,$74,$74,$36

# qhasm: carry2 = select bytes from z3_811 by sel30
# asm 1: shufb >carry2=vec128#78,<z3_811=vec128#76,<z3_811=vec128#76,<sel30=vec128#34
# asm 2: shufb >carry2=$80,<z3_811=$78,<z3_811=$78,<sel30=$36
shufb $80,$78,$78,$36

# qhasm: carry3 = select bytes from z3_1215 by sel30
# asm 1: shufb >carry3=vec128#79,<z3_1215=vec128#70,<z3_1215=vec128#70,<sel30=vec128#34
# asm 2: shufb >carry3=$81,<z3_1215=$72,<z3_1215=$72,<sel30=$36
shufb $81,$72,$72,$36

# qhasm: uint32323232 carry0 >>= 12
# asm 1: rotmi >carry0=vec128#80,<carry0=vec128#5,-12
# asm 2: rotmi >carry0=$82,<carry0=$7,-12
rotmi $82,$7,-12

# qhasm: uint32323232 carry1 >>= 12
# asm 1: rotmi >carry1=vec128#77,<carry1=vec128#77,-12
# asm 2: rotmi >carry1=$79,<carry1=$79,-12
rotmi $79,$79,-12

# qhasm: uint32323232 carry2 >>= 12
# asm 1: rotmi >carry2=vec128#78,<carry2=vec128#78,-12
# asm 2: rotmi >carry2=$80,<carry2=$80,-12
rotmi $80,$80,-12

# qhasm: uint32323232 carry3 >>= 12
# asm 1: rotmi >carry3=vec128#79,<carry3=vec128#79,-12
# asm 2: rotmi >carry3=$81,<carry3=$81,-12
rotmi $81,$81,-12

# qhasm: z3_03 &= redcoeffmask
# asm 1: and >z3_03=vec128#5,<z3_03=vec128#73,<redcoeffmask=vec128#37
# asm 2: and >z3_03=$7,<z3_03=$75,<redcoeffmask=$39
and $7,$75,$39

# qhasm: z3_47 &= redcoeffmask
# asm 1: and >z3_47=vec128#72,<z3_47=vec128#72,<redcoeffmask=vec128#37
# asm 2: and >z3_47=$74,<z3_47=$74,<redcoeffmask=$39
and $74,$74,$39

# qhasm: z3_811 &= redcoeffmask
# asm 1: and >z3_811=vec128#73,<z3_811=vec128#76,<redcoeffmask=vec128#37
# asm 2: and >z3_811=$75,<z3_811=$78,<redcoeffmask=$39
and $75,$78,$39

# qhasm: z3_1215 &= redcoeffmask
# asm 1: and >z3_1215=vec128#70,<z3_1215=vec128#70,<redcoeffmask=vec128#37
# asm 2: and >z3_1215=$72,<z3_1215=$72,<redcoeffmask=$39
and $72,$72,$39

# qhasm: int32323232 z3_47 += carry0
# asm 1: a >z3_47=vec128#72,<z3_47=vec128#72,<carry0=vec128#80
# asm 2: a >z3_47=$74,<z3_47=$74,<carry0=$82
a $74,$74,$82

# qhasm: int32323232 z3_811 += carry1
# asm 1: a >z3_811=vec128#73,<z3_811=vec128#73,<carry1=vec128#77
# asm 2: a >z3_811=$75,<z3_811=$75,<carry1=$79
a $75,$75,$79

# qhasm: int32323232 z3_1215 += carry2
# asm 1: a >z3_1215=vec128#70,<z3_1215=vec128#70,<carry2=vec128#78
# asm 2: a >z3_1215=$72,<z3_1215=$72,<carry2=$80
a $72,$72,$80

# qhasm: int32323232 z3_1619 += carry3
# asm 1: a >z3_1619=vec128#71,<z3_1619=vec128#71,<carry3=vec128#79
# asm 2: a >z3_1619=$73,<z3_1619=$73,<carry3=$81
a $73,$73,$81

# qhasm: carry1 = select bytes from z3_47 by sel01
# asm 1: shufb >carry1=vec128#76,<z3_47=vec128#72,<z3_47=vec128#72,<sel01=vec128#31
# asm 2: shufb >carry1=$78,<z3_47=$74,<z3_47=$74,<sel01=$33
shufb $78,$74,$74,$33

# qhasm: carry2 = select bytes from z3_811 by sel01
# asm 1: shufb >carry2=vec128#77,<z3_811=vec128#73,<z3_811=vec128#73,<sel01=vec128#31
# asm 2: shufb >carry2=$79,<z3_811=$75,<z3_811=$75,<sel01=$33
shufb $79,$75,$75,$33

# qhasm: carry3 = select bytes from z3_1215 by sel01
# asm 1: shufb >carry3=vec128#78,<z3_1215=vec128#70,<z3_1215=vec128#70,<sel01=vec128#31
# asm 2: shufb >carry3=$80,<z3_1215=$72,<z3_1215=$72,<sel01=$33
shufb $80,$72,$72,$33

# qhasm: carry4 = select bytes from z3_1619 by sel01
# asm 1: shufb >carry4=vec128#79,<z3_1619=vec128#71,<z3_1619=vec128#71,<sel01=vec128#31
# asm 2: shufb >carry4=$81,<z3_1619=$73,<z3_1619=$73,<sel01=$33
shufb $81,$73,$73,$33

# qhasm: uint32323232 carry1 >>= 13
# asm 1: rotmi >carry1=vec128#76,<carry1=vec128#76,-13
# asm 2: rotmi >carry1=$78,<carry1=$78,-13
rotmi $78,$78,-13

# qhasm: uint32323232 carry2 >>= 13
# asm 1: rotmi >carry2=vec128#77,<carry2=vec128#77,-13
# asm 2: rotmi >carry2=$79,<carry2=$79,-13
rotmi $79,$79,-13

# qhasm: uint32323232 carry3 >>= 13
# asm 1: rotmi >carry3=vec128#78,<carry3=vec128#78,-13
# asm 2: rotmi >carry3=$80,<carry3=$80,-13
rotmi $80,$80,-13

# qhasm: uint32323232 carry4 >>= 13
# asm 1: rotmi >carry4=vec128#79,<carry4=vec128#79,-13
# asm 2: rotmi >carry4=$81,<carry4=$81,-13
rotmi $81,$81,-13

# qhasm: int32323232 z3_47 += carry1
# asm 1: a >z3_47=vec128#72,<z3_47=vec128#72,<carry1=vec128#76
# asm 2: a >z3_47=$74,<z3_47=$74,<carry1=$78
a $74,$74,$78

# qhasm: int32323232 z3_811 += carry2
# asm 1: a >z3_811=vec128#73,<z3_811=vec128#73,<carry2=vec128#77
# asm 2: a >z3_811=$75,<z3_811=$75,<carry2=$79
a $75,$75,$79

# qhasm: int32323232 z3_1215 += carry3
# asm 1: a >z3_1215=vec128#70,<z3_1215=vec128#70,<carry3=vec128#78
# asm 2: a >z3_1215=$72,<z3_1215=$72,<carry3=$80
a $72,$72,$80

# qhasm: int32323232 z3_1619 += carry4
# asm 1: a >z3_1619=vec128#71,<z3_1619=vec128#71,<carry4=vec128#79
# asm 2: a >z3_1619=$73,<z3_1619=$73,<carry4=$81
a $73,$73,$81

# qhasm: carry1 = select bytes from z3_47 by sel12
# asm 1: shufb >carry1=vec128#76,<z3_47=vec128#72,<z3_47=vec128#72,<sel12=vec128#32
# asm 2: shufb >carry1=$78,<z3_47=$74,<z3_47=$74,<sel12=$34
shufb $78,$74,$74,$34

# qhasm: carry2 = select bytes from z3_811 by sel12
# asm 1: shufb >carry2=vec128#77,<z3_811=vec128#73,<z3_811=vec128#73,<sel12=vec128#32
# asm 2: shufb >carry2=$79,<z3_811=$75,<z3_811=$75,<sel12=$34
shufb $79,$75,$75,$34

# qhasm: carry3 = select bytes from z3_1215 by sel12
# asm 1: shufb >carry3=vec128#78,<z3_1215=vec128#70,<z3_1215=vec128#70,<sel12=vec128#32
# asm 2: shufb >carry3=$80,<z3_1215=$72,<z3_1215=$72,<sel12=$34
shufb $80,$72,$72,$34

# qhasm: carry4 = select bytes from z3_1619 by sel12
# asm 1: shufb >carry4=vec128#79,<z3_1619=vec128#71,<z3_1619=vec128#71,<sel12=vec128#32
# asm 2: shufb >carry4=$81,<z3_1619=$73,<z3_1619=$73,<sel12=$34
shufb $81,$73,$73,$34

# qhasm: uint32323232 carry1 >>= 13
# asm 1: rotmi >carry1=vec128#76,<carry1=vec128#76,-13
# asm 2: rotmi >carry1=$78,<carry1=$78,-13
rotmi $78,$78,-13

# qhasm: uint32323232 carry2 >>= 13
# asm 1: rotmi >carry2=vec128#77,<carry2=vec128#77,-13
# asm 2: rotmi >carry2=$79,<carry2=$79,-13
rotmi $79,$79,-13

# qhasm: uint32323232 carry3 >>= 13
# asm 1: rotmi >carry3=vec128#78,<carry3=vec128#78,-13
# asm 2: rotmi >carry3=$80,<carry3=$80,-13
rotmi $80,$80,-13

# qhasm: uint32323232 carry4 >>= 13
# asm 1: rotmi >carry4=vec128#79,<carry4=vec128#79,-13
# asm 2: rotmi >carry4=$81,<carry4=$81,-13
rotmi $81,$81,-13

# qhasm: int32323232 z3_47 += carry1
# asm 1: a >z3_47=vec128#72,<z3_47=vec128#72,<carry1=vec128#76
# asm 2: a >z3_47=$74,<z3_47=$74,<carry1=$78
a $74,$74,$78

# qhasm: int32323232 z3_811 += carry2
# asm 1: a >z3_811=vec128#73,<z3_811=vec128#73,<carry2=vec128#77
# asm 2: a >z3_811=$75,<z3_811=$75,<carry2=$79
a $75,$75,$79

# qhasm: int32323232 z3_1215 += carry3
# asm 1: a >z3_1215=vec128#70,<z3_1215=vec128#70,<carry3=vec128#78
# asm 2: a >z3_1215=$72,<z3_1215=$72,<carry3=$80
a $72,$72,$80

# qhasm: int32323232 z3_1619 += carry4
# asm 1: a >z3_1619=vec128#71,<z3_1619=vec128#71,<carry4=vec128#79
# asm 2: a >z3_1619=$73,<z3_1619=$73,<carry4=$81
a $73,$73,$81

# qhasm: carry1 = select bytes from z3_47 by sel23
# asm 1: shufb >carry1=vec128#76,<z3_47=vec128#72,<z3_47=vec128#72,<sel23=vec128#33
# asm 2: shufb >carry1=$78,<z3_47=$74,<z3_47=$74,<sel23=$35
shufb $78,$74,$74,$35

# qhasm: carry2 = select bytes from z3_811 by sel23
# asm 1: shufb >carry2=vec128#77,<z3_811=vec128#73,<z3_811=vec128#73,<sel23=vec128#33
# asm 2: shufb >carry2=$79,<z3_811=$75,<z3_811=$75,<sel23=$35
shufb $79,$75,$75,$35

# qhasm: carry3 = select bytes from z3_1215 by sel23
# asm 1: shufb >carry3=vec128#78,<z3_1215=vec128#70,<z3_1215=vec128#70,<sel23=vec128#33
# asm 2: shufb >carry3=$80,<z3_1215=$72,<z3_1215=$72,<sel23=$35
shufb $80,$72,$72,$35

# qhasm: carry4 = select bytes from z3_1619 by sel23
# asm 1: shufb >carry4=vec128#79,<z3_1619=vec128#71,<z3_1619=vec128#71,<sel23=vec128#33
# asm 2: shufb >carry4=$81,<z3_1619=$73,<z3_1619=$73,<sel23=$35
shufb $81,$73,$73,$35

# qhasm: uint32323232 carry1 >>= 13
# asm 1: rotmi >carry1=vec128#76,<carry1=vec128#76,-13
# asm 2: rotmi >carry1=$78,<carry1=$78,-13
rotmi $78,$78,-13

# qhasm: uint32323232 carry2 >>= 13
# asm 1: rotmi >carry2=vec128#77,<carry2=vec128#77,-13
# asm 2: rotmi >carry2=$79,<carry2=$79,-13
rotmi $79,$79,-13

# qhasm: uint32323232 carry3 >>= 13
# asm 1: rotmi >carry3=vec128#78,<carry3=vec128#78,-13
# asm 2: rotmi >carry3=$80,<carry3=$80,-13
rotmi $80,$80,-13

# qhasm: uint32323232 carry4 >>= 13
# asm 1: rotmi >carry4=vec128#79,<carry4=vec128#79,-13
# asm 2: rotmi >carry4=$81,<carry4=$81,-13
rotmi $81,$81,-13

# qhasm: int32323232 z3_47 += carry1
# asm 1: a >z3_47=vec128#72,<z3_47=vec128#72,<carry1=vec128#76
# asm 2: a >z3_47=$74,<z3_47=$74,<carry1=$78
a $74,$74,$78

# qhasm: int32323232 z3_811 += carry2
# asm 1: a >z3_811=vec128#73,<z3_811=vec128#73,<carry2=vec128#77
# asm 2: a >z3_811=$75,<z3_811=$75,<carry2=$79
a $75,$75,$79

# qhasm: int32323232 z3_1215 += carry3
# asm 1: a >z3_1215=vec128#76,<z3_1215=vec128#70,<carry3=vec128#78
# asm 2: a >z3_1215=$78,<z3_1215=$72,<carry3=$80
a $78,$72,$80

# qhasm: int32323232 z3_1619 += carry4
# asm 1: a >z3_1619=vec128#77,<z3_1619=vec128#71,<carry4=vec128#79
# asm 2: a >z3_1619=$79,<z3_1619=$73,<carry4=$81
a $79,$73,$81

# qhasm: z3_47 &= redcoeffmaskveryend
# asm 1: and >z3_47=vec128#70,<z3_47=vec128#72,<redcoeffmaskveryend=vec128#39
# asm 2: and >z3_47=$72,<z3_47=$74,<redcoeffmaskveryend=$41
and $72,$74,$41

# qhasm: z3_811 &= redcoeffmaskveryend
# asm 1: and >z3_811=vec128#71,<z3_811=vec128#73,<redcoeffmaskveryend=vec128#39
# asm 2: and >z3_811=$73,<z3_811=$75,<redcoeffmaskveryend=$41
and $73,$75,$41

# qhasm: z3_1215 &= redcoeffmaskveryend
# asm 1: and >z3_1215=vec128#72,<z3_1215=vec128#76,<redcoeffmaskveryend=vec128#39
# asm 2: and >z3_1215=$74,<z3_1215=$78,<redcoeffmaskveryend=$41
and $74,$78,$41

# qhasm: z3_1619 &= redcoeffmaskveryend
# asm 1: and >z3_1619=vec128#73,<z3_1619=vec128#77,<redcoeffmaskveryend=vec128#39
# asm 2: and >z3_1619=$75,<z3_1619=$79,<redcoeffmaskveryend=$41
and $75,$79,$41

# qhasm: uint32323232 check = loopmask[0] | loopmask[1] | loopmask[2] | loopmask[3]
# asm 1: orx >check=vec128#76,<loopmask=vec128#6
# asm 2: orx >check=$78,<loopmask=$8
orx $78,$8

# qhasm: goto loop if (check & 0xffffffff)
# asm 1: brnz <check=vec128#76,._loop
# asm 2: brnz <check=$78,._loop
brnz $78,._loop

# qhasm: goto end if (done & 0xffff)
# asm 1: brhnz <done=vec128#49,._end
# asm 2: brhnz <done=$51,._end
brhnz $51,._end

# qhasm: prevextbit_stack = prevextbit
# asm 1: stqd <prevextbit=vec128#7,[32+>prevextbit_stack=stack128#49]($sp)
# asm 2: stqd <prevextbit=$9,[32+>prevextbit_stack=768]($sp)
stqd $9,[32+768]($sp)

# qhasm: sk = *(vec128 *) ((skp + 0) & ~15)
# asm 1: lqd >sk=vec128#7,0(<skp=vec128#2)
# asm 2: lqd >sk=$9,0(<skp=$4)
lqd $9,0($4)

# qhasm: uint32323232 loopmask = 1
# asm 1: ila >loopmask=vec128#6,1
# asm 2: ila >loopmask=$8,1
ila $8,1

# qhasm: loopmask <<= (8 * 15)
# asm 1: shlqbyi >loopmask=vec128#6,<loopmask=vec128#6,15
# asm 2: shlqbyi >loopmask=$8,<loopmask=$8,15
shlqbyi $8,$8,15

# qhasm: loopmask <<= (7 % 8)
# asm 1: shlqbii >loopmask=vec128#6,<loopmask=vec128#6,7
# asm 2: shlqbii >loopmask=$8,<loopmask=$8,7
shlqbii $8,$8,7

# qhasm: sk = select bytes from sk by swapendian
# asm 1: shufb >sk=vec128#50,<sk=vec128#7,<sk=vec128#7,<swapendian=vec128#8
# asm 2: shufb >sk=$52,<sk=$9,<sk=$9,<swapendian=$10
shufb $52,$9,$9,$10

# qhasm: prevextbit = prevextbit_stack
# asm 1: lqd >prevextbit=vec128#7,[32+<prevextbit_stack=stack128#49]($sp)
# asm 2: lqd >prevextbit=$9,[32+<prevextbit_stack=768]($sp)
lqd $9,[32+768]($sp)

# qhasm: int32323232 done = 1
# asm 1: il >done=vec128#49,1
# asm 2: il >done=$51,1
il $51,1

# qhasm: goto loop
br ._loop

# qhasm: end:
._end:

# qhasm: flip = extbit
# asm 1: ai >flip=vec128#2,<extbit=vec128#75,0
# asm 2: ai >flip=$4,<extbit=$77,0
ai $4,$77,0

# qhasm: nflip = ~(flip | zero)
# asm 1: nor >nflip=vec128#3,<flip=vec128#2,<zero=vec128#4
# asm 2: nor >nflip=$5,<flip=$4,<zero=$6
nor $5,$4,$6

# qhasm: tmp0 = x2_03   & nflip
# asm 1: and >tmp0=vec128#4,<x2_03=vec128#55,<nflip=vec128#3
# asm 2: and >tmp0=$6,<x2_03=$57,<nflip=$5
and $6,$57,$5

# qhasm: tmp1 = x3_03   & flip
# asm 1: and >tmp1=vec128#6,<x3_03=vec128#65,<flip=vec128#2
# asm 2: and >tmp1=$8,<x3_03=$67,<flip=$4
and $8,$67,$4

# qhasm: tmp2 = x2_03   & flip
# asm 1: and >tmp2=vec128#7,<x2_03=vec128#55,<flip=vec128#2
# asm 2: and >tmp2=$9,<x2_03=$57,<flip=$4
and $9,$57,$4

# qhasm: tmp3 = x3_03   & nflip
# asm 1: and >tmp3=vec128#8,<x3_03=vec128#65,<nflip=vec128#3
# asm 2: and >tmp3=$10,<x3_03=$67,<nflip=$5
and $10,$67,$5

# qhasm: x2_03 = tmp0 ^ tmp1
# asm 1: xor >x2_03=vec128#4,<tmp0=vec128#4,<tmp1=vec128#6
# asm 2: xor >x2_03=$6,<tmp0=$6,<tmp1=$8
xor $6,$6,$8

# qhasm: x3_03 = tmp2 ^ tmp3
# asm 1: xor >x3_03=vec128#6,<tmp2=vec128#7,<tmp3=vec128#8
# asm 2: xor >x3_03=$8,<tmp2=$9,<tmp3=$10
xor $8,$9,$10

# qhasm: tmp0 = x2_47   & nflip
# asm 1: and >tmp0=vec128#7,<x2_47=vec128#56,<nflip=vec128#3
# asm 2: and >tmp0=$9,<x2_47=$58,<nflip=$5
and $9,$58,$5

# qhasm: tmp1 = x3_47   & flip
# asm 1: and >tmp1=vec128#8,<x3_47=vec128#66,<flip=vec128#2
# asm 2: and >tmp1=$10,<x3_47=$68,<flip=$4
and $10,$68,$4

# qhasm: tmp2 = x2_47   & flip
# asm 1: and >tmp2=vec128#9,<x2_47=vec128#56,<flip=vec128#2
# asm 2: and >tmp2=$11,<x2_47=$58,<flip=$4
and $11,$58,$4

# qhasm: tmp3 = x3_47   & nflip
# asm 1: and >tmp3=vec128#10,<x3_47=vec128#66,<nflip=vec128#3
# asm 2: and >tmp3=$12,<x3_47=$68,<nflip=$5
and $12,$68,$5

# qhasm: x2_47 = tmp0 ^ tmp1
# asm 1: xor >x2_47=vec128#7,<tmp0=vec128#7,<tmp1=vec128#8
# asm 2: xor >x2_47=$9,<tmp0=$9,<tmp1=$10
xor $9,$9,$10

# qhasm: x3_47 = tmp2 ^ tmp3
# asm 1: xor >x3_47=vec128#8,<tmp2=vec128#9,<tmp3=vec128#10
# asm 2: xor >x3_47=$10,<tmp2=$11,<tmp3=$12
xor $10,$11,$12

# qhasm: tmp0 = x2_811  & nflip
# asm 1: and >tmp0=vec128#9,<x2_811=vec128#57,<nflip=vec128#3
# asm 2: and >tmp0=$11,<x2_811=$59,<nflip=$5
and $11,$59,$5

# qhasm: tmp1 = x3_811  & flip
# asm 1: and >tmp1=vec128#10,<x3_811=vec128#67,<flip=vec128#2
# asm 2: and >tmp1=$12,<x3_811=$69,<flip=$4
and $12,$69,$4

# qhasm: tmp2 = x2_811  & flip
# asm 1: and >tmp2=vec128#11,<x2_811=vec128#57,<flip=vec128#2
# asm 2: and >tmp2=$13,<x2_811=$59,<flip=$4
and $13,$59,$4

# qhasm: tmp3 = x3_811  & nflip
# asm 1: and >tmp3=vec128#12,<x3_811=vec128#67,<nflip=vec128#3
# asm 2: and >tmp3=$14,<x3_811=$69,<nflip=$5
and $14,$69,$5

# qhasm: x2_811 = tmp0 ^ tmp1
# asm 1: xor >x2_811=vec128#9,<tmp0=vec128#9,<tmp1=vec128#10
# asm 2: xor >x2_811=$11,<tmp0=$11,<tmp1=$12
xor $11,$11,$12

# qhasm: x3_811 = tmp2 ^ tmp3
# asm 1: xor >x3_811=vec128#10,<tmp2=vec128#11,<tmp3=vec128#12
# asm 2: xor >x3_811=$12,<tmp2=$13,<tmp3=$14
xor $12,$13,$14

# qhasm: tmp0 = x2_1215 & nflip
# asm 1: and >tmp0=vec128#11,<x2_1215=vec128#58,<nflip=vec128#3
# asm 2: and >tmp0=$13,<x2_1215=$60,<nflip=$5
and $13,$60,$5

# qhasm: tmp1 = x3_1215 & flip
# asm 1: and >tmp1=vec128#12,<x3_1215=vec128#68,<flip=vec128#2
# asm 2: and >tmp1=$14,<x3_1215=$70,<flip=$4
and $14,$70,$4

# qhasm: tmp2 = x2_1215 & flip
# asm 1: and >tmp2=vec128#13,<x2_1215=vec128#58,<flip=vec128#2
# asm 2: and >tmp2=$15,<x2_1215=$60,<flip=$4
and $15,$60,$4

# qhasm: tmp3 = x3_1215 & nflip
# asm 1: and >tmp3=vec128#14,<x3_1215=vec128#68,<nflip=vec128#3
# asm 2: and >tmp3=$16,<x3_1215=$70,<nflip=$5
and $16,$70,$5

# qhasm: x2_1215 = tmp0 ^ tmp1
# asm 1: xor >x2_1215=vec128#11,<tmp0=vec128#11,<tmp1=vec128#12
# asm 2: xor >x2_1215=$13,<tmp0=$13,<tmp1=$14
xor $13,$13,$14

# qhasm: x3_1215 = tmp2 ^ tmp3
# asm 1: xor >x3_1215=vec128#12,<tmp2=vec128#13,<tmp3=vec128#14
# asm 2: xor >x3_1215=$14,<tmp2=$15,<tmp3=$16
xor $14,$15,$16

# qhasm: tmp0 = x2_1619 & nflip
# asm 1: and >tmp0=vec128#13,<x2_1619=vec128#59,<nflip=vec128#3
# asm 2: and >tmp0=$15,<x2_1619=$61,<nflip=$5
and $15,$61,$5

# qhasm: tmp1 = x3_1619 & flip
# asm 1: and >tmp1=vec128#14,<x3_1619=vec128#69,<flip=vec128#2
# asm 2: and >tmp1=$16,<x3_1619=$71,<flip=$4
and $16,$71,$4

# qhasm: tmp2 = x2_1619 & flip
# asm 1: and >tmp2=vec128#15,<x2_1619=vec128#59,<flip=vec128#2
# asm 2: and >tmp2=$17,<x2_1619=$61,<flip=$4
and $17,$61,$4

# qhasm: tmp3 = x3_1619 & nflip
# asm 1: and >tmp3=vec128#16,<x3_1619=vec128#69,<nflip=vec128#3
# asm 2: and >tmp3=$18,<x3_1619=$71,<nflip=$5
and $18,$71,$5

# qhasm: x2_1619 = tmp0 ^ tmp1
# asm 1: xor >x2_1619=vec128#13,<tmp0=vec128#13,<tmp1=vec128#14
# asm 2: xor >x2_1619=$15,<tmp0=$15,<tmp1=$16
xor $15,$15,$16

# qhasm: x3_1619 = tmp2 ^ tmp3
# asm 1: xor >x3_1619=vec128#14,<tmp2=vec128#15,<tmp3=vec128#16
# asm 2: xor >x3_1619=$16,<tmp2=$17,<tmp3=$18
xor $16,$17,$18

# qhasm: tmp0 = z2_03   & nflip
# asm 1: and >tmp0=vec128#15,<z2_03=vec128#60,<nflip=vec128#3
# asm 2: and >tmp0=$17,<z2_03=$62,<nflip=$5
and $17,$62,$5

# qhasm: tmp1 = z3_03   & flip
# asm 1: and >tmp1=vec128#16,<z3_03=vec128#5,<flip=vec128#2
# asm 2: and >tmp1=$18,<z3_03=$7,<flip=$4
and $18,$7,$4

# qhasm: tmp2 = z2_03   & flip
# asm 1: and >tmp2=vec128#17,<z2_03=vec128#60,<flip=vec128#2
# asm 2: and >tmp2=$19,<z2_03=$62,<flip=$4
and $19,$62,$4

# qhasm: tmp3 = z3_03   & nflip
# asm 1: and >tmp3=vec128#5,<z3_03=vec128#5,<nflip=vec128#3
# asm 2: and >tmp3=$7,<z3_03=$7,<nflip=$5
and $7,$7,$5

# qhasm: z2_03 = tmp0 ^ tmp1
# asm 1: xor >z2_03=vec128#15,<tmp0=vec128#15,<tmp1=vec128#16
# asm 2: xor >z2_03=$17,<tmp0=$17,<tmp1=$18
xor $17,$17,$18

# qhasm: z3_03 = tmp2 ^ tmp3
# asm 1: xor >z3_03=vec128#5,<tmp2=vec128#17,<tmp3=vec128#5
# asm 2: xor >z3_03=$7,<tmp2=$19,<tmp3=$7
xor $7,$19,$7

# qhasm: tmp0 = z2_47   & nflip
# asm 1: and >tmp0=vec128#16,<z2_47=vec128#61,<nflip=vec128#3
# asm 2: and >tmp0=$18,<z2_47=$63,<nflip=$5
and $18,$63,$5

# qhasm: tmp1 = z3_47   & flip
# asm 1: and >tmp1=vec128#17,<z3_47=vec128#70,<flip=vec128#2
# asm 2: and >tmp1=$19,<z3_47=$72,<flip=$4
and $19,$72,$4

# qhasm: tmp2 = z2_47   & flip
# asm 1: and >tmp2=vec128#18,<z2_47=vec128#61,<flip=vec128#2
# asm 2: and >tmp2=$20,<z2_47=$63,<flip=$4
and $20,$63,$4

# qhasm: tmp3 = z3_47   & nflip
# asm 1: and >tmp3=vec128#19,<z3_47=vec128#70,<nflip=vec128#3
# asm 2: and >tmp3=$21,<z3_47=$72,<nflip=$5
and $21,$72,$5

# qhasm: z2_47 = tmp0 ^ tmp1
# asm 1: xor >z2_47=vec128#16,<tmp0=vec128#16,<tmp1=vec128#17
# asm 2: xor >z2_47=$18,<tmp0=$18,<tmp1=$19
xor $18,$18,$19

# qhasm: z3_47 = tmp2 ^ tmp3
# asm 1: xor >z3_47=vec128#17,<tmp2=vec128#18,<tmp3=vec128#19
# asm 2: xor >z3_47=$19,<tmp2=$20,<tmp3=$21
xor $19,$20,$21

# qhasm: tmp0 = z2_811  & nflip
# asm 1: and >tmp0=vec128#18,<z2_811=vec128#62,<nflip=vec128#3
# asm 2: and >tmp0=$20,<z2_811=$64,<nflip=$5
and $20,$64,$5

# qhasm: tmp1 = z3_811  & flip
# asm 1: and >tmp1=vec128#19,<z3_811=vec128#71,<flip=vec128#2
# asm 2: and >tmp1=$21,<z3_811=$73,<flip=$4
and $21,$73,$4

# qhasm: tmp2 = z2_811  & flip
# asm 1: and >tmp2=vec128#20,<z2_811=vec128#62,<flip=vec128#2
# asm 2: and >tmp2=$22,<z2_811=$64,<flip=$4
and $22,$64,$4

# qhasm: tmp3 = z3_811  & nflip
# asm 1: and >tmp3=vec128#21,<z3_811=vec128#71,<nflip=vec128#3
# asm 2: and >tmp3=$23,<z3_811=$73,<nflip=$5
and $23,$73,$5

# qhasm: z2_811 = tmp0 ^ tmp1
# asm 1: xor >z2_811=vec128#18,<tmp0=vec128#18,<tmp1=vec128#19
# asm 2: xor >z2_811=$20,<tmp0=$20,<tmp1=$21
xor $20,$20,$21

# qhasm: z3_811 = tmp2 ^ tmp3
# asm 1: xor >z3_811=vec128#19,<tmp2=vec128#20,<tmp3=vec128#21
# asm 2: xor >z3_811=$21,<tmp2=$22,<tmp3=$23
xor $21,$22,$23

# qhasm: tmp0 = z2_1215 & nflip
# asm 1: and >tmp0=vec128#20,<z2_1215=vec128#63,<nflip=vec128#3
# asm 2: and >tmp0=$22,<z2_1215=$65,<nflip=$5
and $22,$65,$5

# qhasm: tmp1 = z3_1215 & flip
# asm 1: and >tmp1=vec128#21,<z3_1215=vec128#72,<flip=vec128#2
# asm 2: and >tmp1=$23,<z3_1215=$74,<flip=$4
and $23,$74,$4

# qhasm: tmp2 = z2_1215 & flip
# asm 1: and >tmp2=vec128#22,<z2_1215=vec128#63,<flip=vec128#2
# asm 2: and >tmp2=$24,<z2_1215=$65,<flip=$4
and $24,$65,$4

# qhasm: tmp3 = z3_1215 & nflip
# asm 1: and >tmp3=vec128#23,<z3_1215=vec128#72,<nflip=vec128#3
# asm 2: and >tmp3=$25,<z3_1215=$74,<nflip=$5
and $25,$74,$5

# qhasm: z2_1215 = tmp0 ^ tmp1
# asm 1: xor >z2_1215=vec128#20,<tmp0=vec128#20,<tmp1=vec128#21
# asm 2: xor >z2_1215=$22,<tmp0=$22,<tmp1=$23
xor $22,$22,$23

# qhasm: z3_1215 = tmp2 ^ tmp3
# asm 1: xor >z3_1215=vec128#21,<tmp2=vec128#22,<tmp3=vec128#23
# asm 2: xor >z3_1215=$23,<tmp2=$24,<tmp3=$25
xor $23,$24,$25

# qhasm: tmp0 = z2_1619 & nflip
# asm 1: and >tmp0=vec128#22,<z2_1619=vec128#64,<nflip=vec128#3
# asm 2: and >tmp0=$24,<z2_1619=$66,<nflip=$5
and $24,$66,$5

# qhasm: tmp1 = z3_1619 & flip
# asm 1: and >tmp1=vec128#23,<z3_1619=vec128#73,<flip=vec128#2
# asm 2: and >tmp1=$25,<z3_1619=$75,<flip=$4
and $25,$75,$4

# qhasm: tmp2 = z2_1619 & flip
# asm 1: and >tmp2=vec128#2,<z2_1619=vec128#64,<flip=vec128#2
# asm 2: and >tmp2=$4,<z2_1619=$66,<flip=$4
and $4,$66,$4

# qhasm: tmp3 = z3_1619 & nflip
# asm 1: and >tmp3=vec128#3,<z3_1619=vec128#73,<nflip=vec128#3
# asm 2: and >tmp3=$5,<z3_1619=$75,<nflip=$5
and $5,$75,$5

# qhasm: z2_1619 = tmp0 ^ tmp1
# asm 1: xor >z2_1619=vec128#22,<tmp0=vec128#22,<tmp1=vec128#23
# asm 2: xor >z2_1619=$24,<tmp0=$24,<tmp1=$25
xor $24,$24,$25

# qhasm: z3_1619 = tmp2 ^ tmp3
# asm 1: xor >z3_1619=vec128#2,<tmp2=vec128#2,<tmp3=vec128#3
# asm 2: xor >z3_1619=$4,<tmp2=$4,<tmp3=$5
xor $4,$4,$5

# qhasm: *(vec128 *) ((retp + 0) & ~15) = x2_03
# asm 1: stqd <x2_03=vec128#4,0(<retp=vec128#1)
# asm 2: stqd <x2_03=$6,0(<retp=$3)
stqd $6,0($3)

# qhasm: *(vec128 *) ((retp + 16) & ~15) = x2_47
# asm 1: stqd <x2_47=vec128#7,16(<retp=vec128#1)
# asm 2: stqd <x2_47=$9,16(<retp=$3)
stqd $9,16($3)

# qhasm: *(vec128 *) ((retp + 32) & ~15) = x2_811
# asm 1: stqd <x2_811=vec128#9,32(<retp=vec128#1)
# asm 2: stqd <x2_811=$11,32(<retp=$3)
stqd $11,32($3)

# qhasm: *(vec128 *) ((retp + 48) & ~15) = x2_1215
# asm 1: stqd <x2_1215=vec128#11,48(<retp=vec128#1)
# asm 2: stqd <x2_1215=$13,48(<retp=$3)
stqd $13,48($3)

# qhasm: *(vec128 *) ((retp + 64) & ~15) = x2_1619
# asm 1: stqd <x2_1619=vec128#13,64(<retp=vec128#1)
# asm 2: stqd <x2_1619=$15,64(<retp=$3)
stqd $15,64($3)

# qhasm: *(vec128 *) ((retp + 80) & ~15) = z2_03
# asm 1: stqd <z2_03=vec128#15,80(<retp=vec128#1)
# asm 2: stqd <z2_03=$17,80(<retp=$3)
stqd $17,80($3)

# qhasm: *(vec128 *) ((retp + 96) & ~15) = z2_47
# asm 1: stqd <z2_47=vec128#16,96(<retp=vec128#1)
# asm 2: stqd <z2_47=$18,96(<retp=$3)
stqd $18,96($3)

# qhasm: *(vec128 *) ((retp + 112) & ~15) = z2_811
# asm 1: stqd <z2_811=vec128#18,112(<retp=vec128#1)
# asm 2: stqd <z2_811=$20,112(<retp=$3)
stqd $20,112($3)

# qhasm: *(vec128 *) ((retp + 128) & ~15) = z2_1215
# asm 1: stqd <z2_1215=vec128#20,128(<retp=vec128#1)
# asm 2: stqd <z2_1215=$22,128(<retp=$3)
stqd $22,128($3)

# qhasm: *(vec128 *) ((retp + 144) & ~15) = z2_1619
# asm 1: stqd <z2_1619=vec128#22,144(<retp=vec128#1)
# asm 2: stqd <z2_1619=$24,144(<retp=$3)
stqd $24,144($3)

# qhasm: call0 = call0_stack
# asm 1: lqd >call0=vec128#78,[32+<call0_stack=stack128#1]($sp)
# asm 2: lqd >call0=$80,[32+<call0_stack=0]($sp)
lqd $80,[32+0]($sp)

# qhasm: call1 = call1_stack
# asm 1: lqd >call1=vec128#79,[32+<call1_stack=stack128#2]($sp)
# asm 2: lqd >call1=$81,[32+<call1_stack=16]($sp)
lqd $81,[32+16]($sp)

# qhasm: call2 = call2_stack
# asm 1: lqd >call2=vec128#80,[32+<call2_stack=stack128#3]($sp)
# asm 2: lqd >call2=$82,[32+<call2_stack=32]($sp)
lqd $82,[32+32]($sp)

# qhasm: call3 = call3_stack
# asm 1: lqd >call3=vec128#81,[32+<call3_stack=stack128#4]($sp)
# asm 2: lqd >call3=$83,[32+<call3_stack=48]($sp)
lqd $83,[32+48]($sp)

# qhasm: call4 = call4_stack
# asm 1: lqd >call4=vec128#82,[32+<call4_stack=stack128#5]($sp)
# asm 2: lqd >call4=$84,[32+<call4_stack=64]($sp)
lqd $84,[32+64]($sp)

# qhasm: call5 = call5_stack
# asm 1: lqd >call5=vec128#83,[32+<call5_stack=stack128#6]($sp)
# asm 2: lqd >call5=$85,[32+<call5_stack=80]($sp)
lqd $85,[32+80]($sp)

# qhasm: call6 = call6_stack
# asm 1: lqd >call6=vec128#84,[32+<call6_stack=stack128#7]($sp)
# asm 2: lqd >call6=$86,[32+<call6_stack=96]($sp)
lqd $86,[32+96]($sp)

# qhasm: call7 = call7_stack
# asm 1: lqd >call7=vec128#85,[32+<call7_stack=stack128#8]($sp)
# asm 2: lqd >call7=$87,[32+<call7_stack=112]($sp)
lqd $87,[32+112]($sp)

# qhasm: call8 = call8_stack
# asm 1: lqd >call8=vec128#86,[32+<call8_stack=stack128#9]($sp)
# asm 2: lqd >call8=$88,[32+<call8_stack=128]($sp)
lqd $88,[32+128]($sp)

# qhasm: call9 = call9_stack
# asm 1: lqd >call9=vec128#87,[32+<call9_stack=stack128#10]($sp)
# asm 2: lqd >call9=$89,[32+<call9_stack=144]($sp)
lqd $89,[32+144]($sp)

# qhasm: call10 = call10_stack
# asm 1: lqd >call10=vec128#88,[32+<call10_stack=stack128#11]($sp)
# asm 2: lqd >call10=$90,[32+<call10_stack=160]($sp)
lqd $90,[32+160]($sp)

# qhasm: call11 = call11_stack
# asm 1: lqd >call11=vec128#89,[32+<call11_stack=stack128#12]($sp)
# asm 2: lqd >call11=$91,[32+<call11_stack=176]($sp)
lqd $91,[32+176]($sp)

# qhasm: call12 = call12_stack
# asm 1: lqd >call12=vec128#90,[32+<call12_stack=stack128#13]($sp)
# asm 2: lqd >call12=$92,[32+<call12_stack=192]($sp)
lqd $92,[32+192]($sp)

# qhasm: call13 = call13_stack
# asm 1: lqd >call13=vec128#91,[32+<call13_stack=stack128#14]($sp)
# asm 2: lqd >call13=$93,[32+<call13_stack=208]($sp)
lqd $93,[32+208]($sp)

# qhasm: call14 = call14_stack
# asm 1: lqd >call14=vec128#92,[32+<call14_stack=stack128#15]($sp)
# asm 2: lqd >call14=$94,[32+<call14_stack=224]($sp)
lqd $94,[32+224]($sp)

# qhasm: call15 = call15_stack
# asm 1: lqd >call15=vec128#93,[32+<call15_stack=stack128#16]($sp)
# asm 2: lqd >call15=$95,[32+<call15_stack=240]($sp)
lqd $95,[32+240]($sp)

# qhasm: call16 = call16_stack
# asm 1: lqd >call16=vec128#94,[32+<call16_stack=stack128#17]($sp)
# asm 2: lqd >call16=$96,[32+<call16_stack=256]($sp)
lqd $96,[32+256]($sp)

# qhasm: call17 = call17_stack
# asm 1: lqd >call17=vec128#95,[32+<call17_stack=stack128#18]($sp)
# asm 2: lqd >call17=$97,[32+<call17_stack=272]($sp)
lqd $97,[32+272]($sp)

# qhasm: call18 = call18_stack
# asm 1: lqd >call18=vec128#96,[32+<call18_stack=stack128#19]($sp)
# asm 2: lqd >call18=$98,[32+<call18_stack=288]($sp)
lqd $98,[32+288]($sp)

# qhasm: call19 = call19_stack
# asm 1: lqd >call19=vec128#97,[32+<call19_stack=stack128#20]($sp)
# asm 2: lqd >call19=$99,[32+<call19_stack=304]($sp)
lqd $99,[32+304]($sp)

# qhasm: call20 = call20_stack
# asm 1: lqd >call20=vec128#98,[32+<call20_stack=stack128#21]($sp)
# asm 2: lqd >call20=$100,[32+<call20_stack=320]($sp)
lqd $100,[32+320]($sp)

# qhasm: call21 = call21_stack
# asm 1: lqd >call21=vec128#99,[32+<call21_stack=stack128#22]($sp)
# asm 2: lqd >call21=$101,[32+<call21_stack=336]($sp)
lqd $101,[32+336]($sp)

# qhasm: call22 = call22_stack
# asm 1: lqd >call22=vec128#100,[32+<call22_stack=stack128#23]($sp)
# asm 2: lqd >call22=$102,[32+<call22_stack=352]($sp)
lqd $102,[32+352]($sp)

# qhasm: call23 = call23_stack
# asm 1: lqd >call23=vec128#101,[32+<call23_stack=stack128#24]($sp)
# asm 2: lqd >call23=$103,[32+<call23_stack=368]($sp)
lqd $103,[32+368]($sp)

# qhasm: call24 = call24_stack
# asm 1: lqd >call24=vec128#102,[32+<call24_stack=stack128#25]($sp)
# asm 2: lqd >call24=$104,[32+<call24_stack=384]($sp)
lqd $104,[32+384]($sp)

# qhasm: call25 = call25_stack
# asm 1: lqd >call25=vec128#103,[32+<call25_stack=stack128#26]($sp)
# asm 2: lqd >call25=$105,[32+<call25_stack=400]($sp)
lqd $105,[32+400]($sp)

# qhasm: call26 = call26_stack
# asm 1: lqd >call26=vec128#104,[32+<call26_stack=stack128#27]($sp)
# asm 2: lqd >call26=$106,[32+<call26_stack=416]($sp)
lqd $106,[32+416]($sp)

# qhasm: call27 = call27_stack
# asm 1: lqd >call27=vec128#105,[32+<call27_stack=stack128#28]($sp)
# asm 2: lqd >call27=$107,[32+<call27_stack=432]($sp)
lqd $107,[32+432]($sp)

# qhasm: call28 = call28_stack
# asm 1: lqd >call28=vec128#106,[32+<call28_stack=stack128#29]($sp)
# asm 2: lqd >call28=$108,[32+<call28_stack=448]($sp)
lqd $108,[32+448]($sp)

# qhasm: call29 = call29_stack
# asm 1: lqd >call29=vec128#107,[32+<call29_stack=stack128#30]($sp)
# asm 2: lqd >call29=$109,[32+<call29_stack=464]($sp)
lqd $109,[32+464]($sp)

# qhasm: call30 = call30_stack
# asm 1: lqd >call30=vec128#108,[32+<call30_stack=stack128#31]($sp)
# asm 2: lqd >call30=$110,[32+<call30_stack=480]($sp)
lqd $110,[32+480]($sp)

# qhasm: call31 = call31_stack
# asm 1: lqd >call31=vec128#109,[32+<call31_stack=stack128#32]($sp)
# asm 2: lqd >call31=$111,[32+<call31_stack=496]($sp)
lqd $111,[32+496]($sp)

# qhasm: call32 = call32_stack
# asm 1: lqd >call32=vec128#110,[32+<call32_stack=stack128#33]($sp)
# asm 2: lqd >call32=$112,[32+<call32_stack=512]($sp)
lqd $112,[32+512]($sp)

# qhasm: call33 = call33_stack
# asm 1: lqd >call33=vec128#111,[32+<call33_stack=stack128#34]($sp)
# asm 2: lqd >call33=$113,[32+<call33_stack=528]($sp)
lqd $113,[32+528]($sp)

# qhasm: call34 = call34_stack
# asm 1: lqd >call34=vec128#112,[32+<call34_stack=stack128#35]($sp)
# asm 2: lqd >call34=$114,[32+<call34_stack=544]($sp)
lqd $114,[32+544]($sp)

# qhasm: call35 = call35_stack
# asm 1: lqd >call35=vec128#113,[32+<call35_stack=stack128#36]($sp)
# asm 2: lqd >call35=$115,[32+<call35_stack=560]($sp)
lqd $115,[32+560]($sp)

# qhasm: call36 = call36_stack
# asm 1: lqd >call36=vec128#114,[32+<call36_stack=stack128#37]($sp)
# asm 2: lqd >call36=$116,[32+<call36_stack=576]($sp)
lqd $116,[32+576]($sp)

# qhasm: call37 = call37_stack
# asm 1: lqd >call37=vec128#115,[32+<call37_stack=stack128#38]($sp)
# asm 2: lqd >call37=$117,[32+<call37_stack=592]($sp)
lqd $117,[32+592]($sp)

# qhasm: call38 = call38_stack
# asm 1: lqd >call38=vec128#116,[32+<call38_stack=stack128#39]($sp)
# asm 2: lqd >call38=$118,[32+<call38_stack=608]($sp)
lqd $118,[32+608]($sp)

# qhasm: call39 = call39_stack
# asm 1: lqd >call39=vec128#117,[32+<call39_stack=stack128#40]($sp)
# asm 2: lqd >call39=$119,[32+<call39_stack=624]($sp)
lqd $119,[32+624]($sp)

# qhasm: call40 = call40_stack
# asm 1: lqd >call40=vec128#118,[32+<call40_stack=stack128#41]($sp)
# asm 2: lqd >call40=$120,[32+<call40_stack=640]($sp)
lqd $120,[32+640]($sp)

# qhasm: call41 = call41_stack
# asm 1: lqd >call41=vec128#119,[32+<call41_stack=stack128#42]($sp)
# asm 2: lqd >call41=$121,[32+<call41_stack=656]($sp)
lqd $121,[32+656]($sp)

# qhasm: call42 = call42_stack
# asm 1: lqd >call42=vec128#120,[32+<call42_stack=stack128#43]($sp)
# asm 2: lqd >call42=$122,[32+<call42_stack=672]($sp)
lqd $122,[32+672]($sp)

# qhasm: call43 = call43_stack
# asm 1: lqd >call43=vec128#121,[32+<call43_stack=stack128#44]($sp)
# asm 2: lqd >call43=$123,[32+<call43_stack=688]($sp)
lqd $123,[32+688]($sp)

# qhasm: call44 = call44_stack
# asm 1: lqd >call44=vec128#122,[32+<call44_stack=stack128#45]($sp)
# asm 2: lqd >call44=$124,[32+<call44_stack=704]($sp)
lqd $124,[32+704]($sp)

# qhasm: call45 = call45_stack
# asm 1: lqd >call45=vec128#123,[32+<call45_stack=stack128#46]($sp)
# asm 2: lqd >call45=$125,[32+<call45_stack=720]($sp)
lqd $125,[32+720]($sp)

# qhasm: call46 = call46_stack
# asm 1: lqd >call46=vec128#124,[32+<call46_stack=stack128#47]($sp)
# asm 2: lqd >call46=$126,[32+<call46_stack=736]($sp)
lqd $126,[32+736]($sp)

# qhasm: call47 = call47_stack
# asm 1: lqd >call47=vec128#125,[32+<call47_stack=stack128#48]($sp)
# asm 2: lqd >call47=$127,[32+<call47_stack=752]($sp)
lqd $127,[32+752]($sp)

# qhasm: leave
ai $sp,$sp,511
ai $sp,$sp,305
bi $lr
